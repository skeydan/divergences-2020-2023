[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "divergences",
    "section": "",
    "text": "AI ethics\n\n\nAI and Society\n\n\n\n\nOften, AI researchers and engineers think of themselves as neutral and “objective”, operating in a framework of strict formalization. Fairness and absence of bias, however, are social constructs; there is no objectivity, no LaTeX-typesettable remedies, no algorithmic way out. AI models are developed based on a history and deployed in a context. In AI as in data science, the very absence of action can be of political significance.\n\n\n\n\n\n\nSep 22, 2020\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPrivacy\n\n\nSocial Credit\n\n\n\n\nWe live in a world of ever-diminishing privacy and ever-increasing surveillance - and this is a statement not just about openly-authoritarian regimes. Yet, we seem not to care that much, at least not until, for whatever reasons, we are personally affected by some negative consequence. This post wants to help increase awareness, casting a spotlight on recent history and also, letting words speak for themselves: Because nothing, to me, is less revealing than the “visions” that underly the actions.\n\n\n\n\n\n\nFeb 27, 2020\n\n\nSigrid Keydana\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I work at RStudio, PBC, where I write for the RStudio AI blog.\nOpinions are exclusively mine."
  },
  {
    "objectID": "posts/hard_problem_privacy/index.html",
    "href": "posts/hard_problem_privacy/index.html",
    "title": "The hard problem of privacy",
    "section": "",
    "text": "Of course, it depends on what is understood by privacy, or conversely, by surveillance. Last summer, Rachel Thomas, widely known as co-founder of fast.ai and founding director of the USF Center for Applied Data Ethics, wrote about 8 Things You Need to Know about Surveillance. Everything she writes is highly important and deserves being thought about every single day. However, in a well-meaning community there should be little controversy about how justified these points are. My starting point here is different, and it, like the whole of this post, is highly subjective.\n\nSoul mates\nRecently, I listened to an episode of Lex Fridman’s AI Podcast. Depending on one’s interest, this podcast has the most interesting guest list one could imagine: Donald Knuth, Daniel Kahneman, Gilbert Strang, Leonard Susskind, Yoshua Bengio … to name just a few. But the podcast is nice to listen to not just because of the guest list. The interviewer himself impresses by his modest, warm and deeply human ways.\nOn that backgound, I was immensely surprised when in the episode with Cristos Goodrow, VP of Engineering at Google and head of Search and Discovery at YouTube, Fridman not just highly extolled Youtube’s recommendation algorithm, but also expressed a “dream” that Youtube, besides similar-as-per-the-recommendation-algorithm videos, also suggested similar-as-per-the-algorithm users (min. 30/31):\n\nI know I’m gonna ask for things that are impossible but I would love to cluster human beings, I would love to know who has similar trajectories as me …” .\n\nThe algorithm as a helper to find friends, or people like oneself, one would never get to meet otherwise. The most human dream ever, finding soul mates, powered by Youtube.\nIncapable to get over my surprise, I started thinking whether I should write this post.\n\n\nWhy aren’t we more scared?\nIt’s not the most fruitful question, but helpless inability to understand probably starts like this, asking: Why aren’t we more scared?\nFor starters, we’re used to getting stuff recommended all the time. On Amazon, I may have bought some books that were purchased by similar users. (On the other hand, the overall quality of recommendations is not like I would start to feel threatened by the magic.) So, advantages jump to the eye, while disadvantages are not immediately visible.\nSecondly, traditionally, in many cultures, we are accustomed to associate surveillance with the government, not private companies, just simply because historically, it was in fact the government, and the government only, that had the power to surveil and oppress. This is the classic picture of totalitarianism.\nHowever, as described by Shoshana Zuboff in her lengthy-ish, not-without-redundancies, but immensely important book The age of surveillance capitalism, the modern reality – at least in the US and Europe – is less direct and less easy to see through. (Evidently, the story is different in a country like China, where a social credit system has already been established.)\n\n\nSurveillance capitalism in a nutshell\nZuboff’s book recounts, in great detail and citing a lot of sources public and private, how we ended up living in a world where each of our online actions is tracked by numerous “interested parties”, where our phones send who-knows-what to whichever distant beneficiary, and where more and more of our “real-world”, physical activity is recorded by agents we may not even know of.\nIn short, the mechanism she describes is this: Companies big and small, the so-called “surveillance capitalists”, have their business based on gathering, extracting, and processing information – more than is needed to improve a product or a service. Instead, this “behavioral surplus” – in processed form – is sold to customers, which may be everything from other businesses over banks and insurance companies to intelligence agencies and governments.\nWith such a business model, of course a company is interested in optimizing its ML algorithms. The better the algorithms, the better the results that are sold to customers, and the better those results, the more that company is willing to pay. Online ads may look like a triviality, but they are in fact a great illustration of the principle: If our customer pays per click, and our business is “full-service” – “don’t worry about anything, we’ll place your ad for you”, then it’s in my own best interest to show ads to the most susceptible users, in the most favorable circumstances. Of course we’ll get creative, thinking about how we can best determine those users and moments.\nAs already hinted at above, with private companies in possession of data and inferences not publicly available, governments, intelligence services, and the military re-enter the picture. Zuboff describes how intelligence agencies craved information that was hard to obtain legally, but could be provided by companies operating in a lawless zone, always ahead of legislation. 9/11 was just a door opener; from then on, massive interlacing ensued between politics and companies in the information business, as seen for example, but not only, in the 2012 and 2016 US elections.\nZuboff’s book is full of interesting details, but this post is not meant to be a “best of”. Rather, I’m wondering how the extent, as well as the importance of, what’s going on could be made more salient to ML people – not just to those to whom doubt and skepticism come naturally, but to well-meaning idealists as well.\n\n\nWhy we should care\nTrying to make this more salient, I can think of three things:\n\nConcisely stating a few facts that, important though they are in Zuboff’s argumentation, deserve being pointed out even outside the concrete contexts in which they appear in the book;\nCalling attention, in a few lines instead of a time-consuming narrative, to what has already happened, in the “real world”, in the very short time span since those new dynamics took off; and (the most impressive to me, personally)\nRelating, by citation, the actual “utopias” behind the reality.\n\n\n\n(1) A few things to consider\nA common way to think about this is: We’re using a free service (like a search engine), we have to pay somehow, so we’re paying with our data. But – data is collected and processed by products we buy, as well, be it phones, wearables or household appliances.\nAlso, it is not like we get to decide whether and if so, how much privacy we’re willing to give up in exchange for a service. As often noted, no-one has time to read through all the “privacy policies” we’re bombarded with; and sometimes we’re just told “you can disable this; but then feature x [[[some vital feature, probably the one we bought it for]]] can’t be guaranteed to be working anymore”.\n\n\n(2) Not just online\nEven to people who spend most of their day online, being tracked on the web may still seem like a minor inconvenience. Of course, through our smartphones, we carry the web with us. Evidently, depending on what apps one has installed, one may receive friendly “invitations” to, say, pay a visit to McDonald’s just as one passes there on one’s lunchtime run. (Or maybe not, depending on whether the type of ongoing activity is already being taken into account.)\nSure, such notifications disrupt the flow of whatever one’s doing. But still: For some of us, it’s easy to think that who cares? I don’t have those kinds of apps installed anyway. Put more generally: Yes, all this online tracking is annoying. But as long as all that happens is me seeing more “targeted” ads, why bother?\nThe recent years have seen a consistent and speedy extension of tracking in the real world.\nThere was Google Street View, filming the outdoors, a space once thought of as ephemeral, and subsequent attempts, by various actors, to map public interiors.\nThere was Google glass, allowing to video- and audiotape people without their knowledge.\nThen, there are all kinds of wearables, also able to record not just the wearer’s, but bystanders’ data. The wearers themselves must be aware that some data is collected (why else buy the thing?), but don’t necessarily know how it’s being made use of, and what other data may be collected.\nThen, there are all kinds of IoT devices: TVs, vacuum cleaners, thermostats… Again, they are bought for a reason; but no-one can expect a TV to record what you’re saying, or a vacuum cleaner to transmit back your floor plan.\nThen – and this makes for a perfect transition to the last section, “utopias” – there is smart city: not a utopia, but actual reality, already being implemented in Toronto. Here is an excerpt from an interview with CEO Dan Doctoroff:\n\nIn effect, what we’re doing is replicating the digital experience in physical space … So ubiquitous connectivity; incredible computing power including artificial intelligence and machine learning; the ability to display data; sensing, including cameras and location data as well as other specialized sensors… We fund it all… through a very novel advertising model… We can actually then target ads to people in proximity, and then obviously over time track them through things like beacons and location services as well as their browsing activity.\n\nFrom these publicly made statements, it is hard to doubt what The Globe and Mail reported re. a leaked document, the yellow book, containing detailed outlines about how this system would be implemented:\n\nEarly on, the company notes that a Sidewalk neighbourhood would collect real-time position data “for all entities” – including people. The company would also collect a “historical record of where things have been” and “about where they are going.” Furthermore, unique data identifiers would be generated for “every person, business or object registered in the district,” helping devices communicate with each other.\n\nThe newspaper continues,\n\nThe document also describes reputation tools that would lead to a “new currency for community co-operation,” effectively establishing a social credit system. Sidewalk could use these tools to “hold people or businesses accountable” while rewarding good behaviour, such as by rewarding a business’s good customer service with an easier or cheaper renewal process on its licence.\n\nIf, to people in “Western cultures”, China, with its open endorsement and implementation of a social credit system, tended to seem “far, far away” – now nothing seems as it did before.\nAt this point, it’s time to switch gears and look at the world view(s) behind those recent developments.\n\n\n(3) Utopias\nHere is part of the vision of Mark Zuckerberg. Zuboff, aggregating information from several Zuckerberg-quoting, publicly available sources, writes (p.402)\n\nPredictive models would enable the corporation to “tell you to what bar to go to” when you arrive in a strange city. The vision is detailed: when you arrive at the bar, the bartender has your favorite drink waiting, and you’re able to look around the room and identify people just like you.\n\nExtending this to life overall, here we have a vision of total boredom; no exploration, no curiosity, no surprise, no learning. How probable is it that evolutionarily, humans are prepared to live in a world without uncertainty?\nHow about regulation? Here is Larry Page on laws and experimentation:\n\nIf you look at the different kinds of laws we make, they’re very old. The laws when we went public we’re 50 years old. A law can’t be right if it’s 50 years old, like it’s before the Internet, that’s a pretty major change, in how you may go public.[…]\n\n\nThe other thing in my mind is we also haven’t built mechanisms to allow experimentation. There’s many, many exciting and important things you could do that you just can’t do because they’re illegal, or they’re not allowed by regulation, and that makes sense, we don’t want the world to change too fast. Maybe we should set aside a small part of the world …[…]\n\n\nI think as technologists we should have some safe places where we can try out some new things and figure out what is the effect on society, what’s the effect on people, without having to deploy kind of into the normal world. […]\n\nOr Zuckerberg again, on the ancient social norm of privacy:\n\n“People have really gotten comfortable not only sharing more information and different kinds, but more openly and with more people,” he said. “That social norm is just something that has evolved over time.”\n\nIn this light, is a social credit system – perhaps packaged differently – really so far away, in Western cultures? Sometimes, a scenario sounds so silly that it’s hard to see it as a real threat; but that may be just us being naive. Here is another vision, from Rana el Kaliouby, CEO at Affectiva:\n\nWe have been in conversations with a company in that space. It is an advertising-rewards company, and its business is based on positive moments. So if you set a goal to run three miles and you run three miles, that’s a moment. Or if you set the alarm for six o’clock and you actually do get up, that’s a moment. And they monetize these moments. They sell them. Like Kleenex can send you a coupon – I don’t know – when you get over a sad moment. Right now, this company is making assumptions about what those moments are. And we’re like, ‘Guess what? We can capture them.’\n\nIf that sounds like it needn’t be taken seriously, here is a quote from Social Physics, a highly acclaimed opus by Alex Pentland, director of MIT Media Lab:\n\nThe social physics approach to getting everyone to cooperate is to use social network incentives rather than to use individual market incentives or to provide additional information. That is, we focus on changing the connections between people rather than focussing on getting people individually to change their behavior. The logic here is clear: Since exchanges between people are of enormous value to the participants, we can leverage those exchanges to generate social pressure for change.[…]\n\n\nSocial network incentives act by generating social pressure around the problem of finding cooperative behaviors, and so people experiment with new behaviors to find ones that are better.\n\nAt this point, I feel like nothing has to be added to those visions. But let’s get back to the beginning, Lex Fridman’s fantasy of clustering users.\n\n\nExpressing oneself\nZuboff quotes Zuckerberg saying, “humans have such a deep need to express themselves” (p. 403). Evidently, this is what motivates Fridman’s wish, and yes, it’s a, or the, profoundly human thing.\nBut, doesn’t expressing oneself, if seen as sharing, involve a free decision when, what, and with whom to share?\nAnd what does it do to an experience if it is not just “there”, a thing valuable in itself, but a means for others, to control and profit?\nThanks for reading!"
  },
  {
    "objectID": "posts/ai_ethics_optimization_problem/index.html",
    "href": "posts/ai_ethics_optimization_problem/index.html",
    "title": "AI ethics is not an optimization problem",
    "section": "",
    "text": "But we are doing a lot to improve fairness and remove bias, aren’t we?\nSearch for “algorithmic fairness”, “deep learning fairness” or something similar, and you’ll find lots of papers, tools, guidelines … more than you have time to read. And bias: Haven’t we all heard about image recognition models failing on black people, women, and black women in particular; about machine translation incorporating gender stereotypes; about search engine results reflecting racism and discrimination? Given enough media attention, the respective algorithms get “fixed”; what’s more, we may also safely assume that overall, researchers developing new models will probe for these exact kinds of failures. So as a community, we do care about bias, don’t we?\nRe: fairness. It’s true that there are many attempts to increase algorithmic fairness. But as Ben Green, who I’ll cite a lot in this text, points out, most of this work does so via rigid formalization, as if fairness — its conflicting definitions notwithstanding — were an objective quantity, a metric that could be optimized just like the metrics we usually optimize in machine learning.\nAssume we were willing to stay in the formalist frame. Even then there is no satisfying solution to this optimization problem. Take the perspective of group fairness, where a fair algorithm is one that results in equal outcomes between groups. Chouldechova then shows that when an algorithm achieves predictive parity (a.k.a. precision), but prevalence differs between groups, it is not possible to also have both equal false positive and equal false negative rates.\nThat said, here I mainly want to focus on why pure formalism is not enough.\nWhat with bias? That datasets can be (or rather: are) biased is hardly something anyone working in AI would object to. What amount of bias is being admitted to in other components/stages of the machine learning process varies between people.\nHarini and Guttag map sources of bias to the model development and deployment process. Forms of bias they distinguish include representation (think: dataset), measurement (think: metrics3), aggregation (think: model4), and evaluation (similar to the two preceding ones, but at test time) bias. The paper is written in a technical style; in fact, there is even a diagram where the authors attempt to formalize the process in a mathematical way. (Personally, I find it hard to see what value is added by this diagram; its existence can probably best be explained by [perceived] requirements of the genre, namely, research paper.)\nNow, so far I’ve left out the remaining two sources of bias they name. Mapped to the end of the process is deployment bias, when a system “is used or interpreted in inappropriate ways”. That raises a question. Is this about the end of a process, or is what happens now a completely different process (or: processes)? For an in-house data scientist, it may well be the end of a process; for a scientist, or for a consultant, it is not. Once a scientist has developed and published a model, they have no control over who uses it and how. From the collection of humankind’s empirical truths: Once a technology exists, and it is useful, it will be used.\nThings get further out of control at the other side of the timeline. Here we have historical bias:\n\nHistorical bias arises when there is a misalignment between the world as it is and the values or objectives to be encoded and propagated in a model. It is a normative concern with the state of the world, and exists even given perfect sampling and feature selection.\n\nI’ll get back to why this is called “historical bias” later; the definition is a lot broader though. Now definitely we’re beyond the realm of formalization: We’re entering the realm of normativity, of ways of viewing the world, of ethics.\nPlease don’t get me wrong. I’m not criticizing the paper; in fact, it provides a useful categorization that may be used as a “check list” by practitioners. But given the formalist style, a reader is likely to focus on the readily-formalizable parts. It is then easy to dismiss the two others as not really being relevant to one’s work, and certainly not something one could exert influence on. (I’ll get back to that later.)\nOne little aside before we leave formalism: I do believe that a lot of the formalist work on AI ethics is done in good intent; however, work in this area also benefits organizations that undertake it. Citing Tom Slee,\n\nStandards set public benchmarks and provide protection from future accusations. Auditable criteria incorporated into product development and release processes can confirm compliance. There are also financial incentives to adopt a technical approach: standards that demand expertise and investment create barriers to entry by smaller firms, just as risk management regulations create barriers to entry in the financial and healthcare industries.\n\n\n\nNot everything in life is an optimization problem\nStephen Boyd, who teaches convex optimization at Stanford, is said to often start theintroductory lecture with the phrase “everything is an optimization problem”. It sounds intriguing at first; certainly a lot of things in my life can be thought of like that. It may become awkward once you start to compare, say, time spent with loved ones and time spent working; it becomes completely unfeasible when comparing across people.\nWe saw that even under formalist thinking, there is no impartial way to optimize for fairness. But it’s not just about choosing between different types of fairness. How do you weigh fairness against stakeholder interests? No algorithm will be deployed that does not serve an organization’s purpose.\nThere is thus an intimate link between metrics, objectives and power.\n\n\nMetrics and power\nEven though terminologically, in deep learning, we distinguish between optimization and metrics, the metrics really are what we are optimizing for: Goodhart’s law — When a measure becomes a target, it ceases to be a good measure5— does not seem to apply. Still, they deserve the same questioning and inspection as metrics in other contexts.\nIn AI as elsewhere, optimization objectives are proxies; they “stand in” for the thing we’re really interested in. That proxying process could fail in many ways, but failure is not the only applicable category to think in here.\nFor one, objectives are chosen according to the dominant paradigm. Dotan and Milli show how a technology’s perceived success feeds back into the criteria used to evaluate other technologies (as well as future instances of itself). Imagine a world where models were ranked not just for classification accuracy, but also for, say, climate friendliness, robustness to adversarial attacks, or reliable quantification of uncertainty.\nObjectives thus do not emerge out of nothing. That they reflect paradigms may still sound abstract; that they serve existing power structures less so. Power structures are complex; they reflect more than just who “has the say”. As pointed out in various “classics” of critical race theory, intersectionalist feminism and related areas6, we should ask ourselves: Who profits?\nThis is an all but simple question, comparable in difficulty, it seems to me, to the request that we question the unspoken premises that underlie our theories. The main point about such premises is that we aren’t aware of them: If we were, we could have stated them explicitly. Similarly, if I noticed I was profiting from someone, I would — hopefully — take some action. Hard as the task may be, though, we have to do our best.\nIf it’s hard to see how one is privileged, can’t we just be neutral? Objective? Isn’t this getting too political?\n\n\nThere is no objectivity, and all is politics\nTo some people, the assertion that objectivity cannot exist is too self-evident to require much dwelling on. Are numbers objective? Maybe in some formal way; but once I use them in communication, they convey a message, independently of my intention. \nLet’s say I want to convey the fact, taken from the IPCC website, that between 2030 and 2052, global warming is likely to reach 1.5°C. Let’s also assume that I look up the pre-industrial average for the place where I happen to live (15°C, say), and that I want to show off my impressive meteorological knowledge. Thus I say, “… so here, guys, temperature will rise from 288.15 Kelvin to 289.65 Kelvin, on average”. This surely is an objective statement. But what if the people I’m talking to don’t know that — even though the absolute values, when expressed in Kelvin, are so much higher than when expressed in degrees Celsius — the relative differences are the same? They might get the impression, totally unintended, that not much warming is going to happen.\nIf even numbers, once used in communication, lose their objectivity, this must hold even more for anything that involves more design choices: visualizations, APIs, and, of course, written text. For visualizations, this is nicely illustrated in d’Ignazio and Klein’s Data Feminism.\nThat book is also an exemplary exercise in what appears like the only way to “deal with” the fact that objectivity is impossible: Trying to be as clear as possible about where we stand, who we are, what are the circumstances that have influenced our way of thinking. Of course, like with the unspoken premises and assumptions discussed above, this is not easy; in fact, it’s impossible to do in perfection. But one can try.\nIn fact, the above “Trying to be as clear as possible …” is deliberately ambiguous. It refers to two things: For one, to me striving to analyze how I’m privileged, and secondly, to me giving information to others. The first alone is laudable but necessarily limited; the second, as exercised by d’Ignazio and Klein, opens the door not just for better mutual understanding, but also, for feedback and learning. The person I’m talking to might lead me to insights I wouldn’t have gotten otherwise.\nPutting things slightly differently, there is no objectivity because there’s always a context. Focus on metrics and formalization detract from that context. Green relates an interesting parallel from American law history. Until the early twentieth century, US law was dominated by a formalist ethos. Ideally, all rules should be traceable to a small number of universal principles, derived from natural rights. Autonomy being such a right, in a famous 1905 case, the U.S. Supreme Court concluded that a law limiting the working hours of employees represented “unreasonable, unnecessary and arbitrary interference with the right and liberty of the individual to contract”. However,\n\nIn his dissent, Justice Oliver Wendell Holmes argued that the Court failed to consider the context of the case, noting, “General propositions do not decide concrete cases”.\n\nThus, the context matters. Autonomy is good; but it can’t be used as an excuse for exploitation.\nThe same context dependence holds in AI. It is always developed and deployed in a context, — a context shaped by history. History determines what datasets we work with; what we optimize for; who tells us what to optimize for. An daunting example is so-called “predictive policing”. Datasets used to train prediction algorithms incorporate a history of racial injustice: The very definition of “crime” they rely on wasshaped by racist and classist practice. The new method then perpetuates — more than that: exacerbates — the current system, creating a vicious cycle of positive feedback that makes it look like the algorithm was successful.\nSumming up: When there is no neutrality, everything is politics. Not acting is acting. Citing Green,\n\nBut efforts for reform are no more political than efforts to resist reform or even the choice simply to not act, both of which preserve existing systems.\n\n\n\nBut I’m just an engineer\nWhen machine learning people, or computer science people in general, are asked about their views on the societal impact of modern AI, an often-heard answer is: “But I’m just an engineer…”. This is completely understandable. Most of us are just tiny cogs in those big machines, wired together in a complex network, that run the world. We’re not in control; how could we be accountable?\nFor the AI scientist, though normally all but sitting in an “ivory tower”, it’s nevertheless the ML engineers who are responsible of how a model gets deployed, and to what consequences⁷. The ML engineer may delegate to the head of IT, who in turn had no choice but implement what was requested “by business”. And so on and so forth, ad infinitum.\nThat said, it is hard to come up with a moral imperative here. In the line of thinking exercised above: There is always a context. In some parts of the world, you have more choices than in others. Options vary based on race, gender, abledness, and more. Maybe you can just quit and get another job; maybe you can’t.\nThere is another — related — point though, on which I’d like to dwell a little longer.\n\n\nTechnology optimism\nSometimes, it’s not that the people working in AI are fully aware of, but don’t see how to counter, the harm that is being done in their field. On the contrary. They are convinced that they’re doing good. Just do a quick search for “AI for good”, and you’ll be presented with a great number of projects and initiatives. But how, actually, is decided what is good? Who decides? Who profits?\nBen Green, again, relates an instructive example:\n\nUSC’s Center for Artificial Intelligence in Society (CAIS) is emblematic of how computer science projects labeled as promoting “social good” can cause harm by wading into hotly contested political territory with a regressive perspective. One of the group’s projects involved deploying game theory and machine learning to predict and prevent behavior from “adversarial groups.” Although CAIS motivated the project by discussing “extremist organizations such as ISIS and Jabhat al-Nusra,” it quickly slipped into focusing on “criminal street gangs” [43]. In fact, the project’s only publication was a controversial paper that used neural networks to classify crimes in Los Angeles as gang-related [28, 41].\n\nPredictive policing, already mentioned above, can also be seen in this category. At first thought, isn’t it a good thing? Wouldn’t it be nice if we could make our world a bit more secure?\nPhillip Rogaway, who I’ll mention again in the concluding section, talks a lot about how technology optimism dominates among his students; he seems to be as intrigued by it as I am. Personally, I think that whether someone “intrinsically” tends to be a technology optimist or a pessimist is a persistent trait; it seems to be a matter of personality and socialization (or just call it fate: I don’t want to go into any nature-nurture debates here). That glass, is it half full or half empty?\nAll I could say to a hardcore technology optimist is that their utopia may be another person’s dystopia. Especially if that other person is poor, or black, or poor and a black woman … and so on.\nLet me just end this section with a citation from a blog post by Ali Alkhatib centered around the launch of the Stanford institute for human-centered artificial intelligence (HAI). Referring to the director’s accompanying blog post, he writes\n\nJames opens with a story of an office that senses you slouching, registers that you’re fatigued, intuits that your mood has shifted, and alters the ambiance accordingly to keep you alert throughout the day.\n\nYou can head to Alkhatib’s post (very worth reading) and read the continuation, expanding on the scenario. But for some people, this single sentence may already be enough.\n\n\nAnd now?\nAt some point, an author is expected to wrap up and present ideas for improvement. With most of the topics touched upon here, this is yet another intimidating task. I’ll give it a try anyway. The best synthesis I can come up with at the moment looks about like this.\nFirst, some approaches filed by Green under “formalist” can still make for a good start, or rather, can constitute a set of default measures, to be taken routinely. Most prominently, these include dataset and model documentation.\nBeyond the technical, I like the advice given in Baumer and Silberman’s When the Implication is Not to Design. If the consequences, especially on socially marginalized groups, of a technological approach are unforeseeable, think whether the problem can be solved in a “low-tech” way. By all means, do not start with the solution and then, go find a problem.\nIn some cases, even that may not be enough. With some goals, don’t look for alternative ways to achieve them. Sometimes the goal itself has to be questioned.\nThe same can be said for the other direction. With some technologies, there is no goal that could justify their application. This is because such a technology is certain to get misused. Facial recognition is one example.\nLastly, let me finish on a speculative note. Rogaway, already referred to above for his comments on technology optimism, calls on his colleagues, fellow cryptographers, to devise protocols in such a way that private communication stays private, that breaches are, in plain terms, impossible. While I personally can’t think see how to port the analogy to AI, maybe others will be able to, drawing inspiration from his text. Until then, changes in politics and legislation seem to be the only recourse.\n\n\n\n\n\n\nFootnotes\n\n\nFor brevity, I’ll be subsuming deep learning and other contemporary machine learning methods under AI, following common(-ish) usage.↩︎\n I can’t hope to express this better than Maciej Cegłowski did here, so I won’t elaborate on that topic any further.↩︎\nMore on that below. Metrics used in machine learning mostly are proxies for things we really care about; there are lots of ways this can go wrong.↩︎\nMeaning, a one-model-fits-all approach puts some groups at a disadvantage.↩︎\ncited after Thomas and Uminsky↩︎\nsee e.g., DataFeminism and RaceAfterTechnology.↩︎"
  }
]