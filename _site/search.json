[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "divergences",
    "section": "",
    "text": "Technical\n\n\nDeep Learning\n\n\n\n\nThis post introduces deep learning on graphs by mapping its central concept - message passing - to minimal usage patterns of PyTorch Geometric’s foundation-laying MessagePassing class. Exploring those patterns, we gain some basic, very concrete insights into how graph DL works.\n\n\n\n\n\n\nNov 22, 2022\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nBooks\n\n\nPhilosophy (sort of)\n\n\n\n\nMuch as we humans like to believe, consciousness is not a neocortex thing, a matter of analysis and meta-analysis. Instead – says Mark Solms, in his 2021 The Hidden Spring: A Journey to the Source of Consciousness – instead, consciousness is all about feeling. A claim that, if we take it seriously (and I don’t see why we shouldn’t) has far-ranging consequences.\n\n\n\n\n\n\nAug 1, 2022\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAI societal impact\n\n\n\n\nOften, AI researchers and engineers think of themselves as neutral and “objective”, operating in a framework of strict formalization. Fairness and absence of bias, however, are social constructs; there is no objectivity, no LaTeX-typesettable remedies, no algorithmic way out. AI models are developed based on a history and deployed in a context. In AI as in data science, the very absence of action can be of political significance.\n\n\n\n\n\n\nSep 22, 2020\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nPrivacy\n\n\nSocial Credit\n\n\n\n\nWe live in a world of ever-diminishing privacy and ever-increasing surveillance - and this is a statement not just about openly-authoritarian regimes. Yet, we seem not to care that much, at least not until, for whatever reasons, we are personally affected by some negative consequence. This post wants to help increase awareness, casting a spotlight on recent history and also, letting words speak for themselves: Because nothing, to me, is less revealing than the “visions” that underly the actions.\n\n\n\n\n\n\nFeb 27, 2020\n\n\nSigrid Keydana\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I work at Posit, PBC, where I write for the Posit AI blog.\nOpinions are exclusively mine."
  },
  {
    "objectID": "posts/hard_problem_privacy/index.html",
    "href": "posts/hard_problem_privacy/index.html",
    "title": "The hard problem of privacy",
    "section": "",
    "text": "Of course, it depends on what is understood by privacy, or conversely, by surveillance. Last summer, Rachel Thomas, widely known as co-founder of fast.ai and founding director of the USF Center for Applied Data Ethics, wrote about 8 Things You Need to Know about Surveillance. Everything she writes is highly important and deserves being thought about every single day. However, in a well-meaning community there should be little controversy about how justified these points are. My starting point here is different, and it, like the whole of this post, is highly subjective.\n\nSoul mates\nRecently, I listened to an episode of Lex Fridman’s AI Podcast. Depending on one’s interest, this podcast has the most interesting guest list one could imagine: Donald Knuth, Daniel Kahneman, Gilbert Strang, Leonard Susskind, Yoshua Bengio … to name just a few. But the podcast is nice to listen to not just because of the guest list. The interviewer himself impresses by his modest, warm and deeply human ways.\nOn that backgound, I was immensely surprised when in the episode with Cristos Goodrow, VP of Engineering at Google and head of Search and Discovery at YouTube, Fridman not just highly extolled Youtube’s recommendation algorithm, but also expressed a “dream” that Youtube, besides similar-as-per-the-recommendation-algorithm videos, also suggested similar-as-per-the-algorithm users (min. 30/31):\n\nI know I’m gonna ask for things that are impossible but I would love to cluster human beings, I would love to know who has similar trajectories as me …” .\n\nThe algorithm as a helper to find friends, or people like oneself, one would never get to meet otherwise. The most human dream ever, finding soul mates, powered by Youtube.\nIncapable to get over my surprise, I started thinking whether I should write this post.\n\n\nWhy aren’t we more scared?\nIt’s not the most fruitful question, but helpless inability to understand probably starts like this, asking: Why aren’t we more scared?\nFor starters, we’re used to getting stuff recommended all the time. On Amazon, I may have bought some books that were purchased by similar users. (On the other hand, the overall quality of recommendations is not like I would start to feel threatened by the magic.) So, advantages jump to the eye, while disadvantages are not immediately visible.\nSecondly, traditionally, in many cultures, we are accustomed to associate surveillance with the government, not private companies, just simply because historically, it was in fact the government, and the government only, that had the power to surveil and oppress. This is the classic picture of totalitarianism.\nHowever, as described by Shoshana Zuboff in her lengthy-ish, not-without-redundancies, but immensely important book The age of surveillance capitalism, the modern reality – at least in the US and Europe – is less direct and less easy to see through. (Evidently, the story is different in a country like China, where a social credit system has already been established.)\n\n\nSurveillance capitalism in a nutshell\nZuboff’s book recounts, in great detail and citing a lot of sources public and private, how we ended up living in a world where each of our online actions is tracked by numerous “interested parties”, where our phones send who-knows-what to whichever distant beneficiary, and where more and more of our “real-world”, physical activity is recorded by agents we may not even know of.\nIn short, the mechanism she describes is this: Companies big and small, the so-called “surveillance capitalists”, have their business based on gathering, extracting, and processing information – more than is needed to improve a product or a service. Instead, this “behavioral surplus” – in processed form – is sold to customers, which may be everything from other businesses over banks and insurance companies to intelligence agencies and governments.\nWith such a business model, of course a company is interested in optimizing its ML algorithms. The better the algorithms, the better the results that are sold to customers, and the better those results, the more that company is willing to pay. Online ads may look like a triviality, but they are in fact a great illustration of the principle: If our customer pays per click, and our business is “full-service” – “don’t worry about anything, we’ll place your ad for you”, then it’s in my own best interest to show ads to the most susceptible users, in the most favorable circumstances. Of course we’ll get creative, thinking about how we can best determine those users and moments.\nAs already hinted at above, with private companies in possession of data and inferences not publicly available, governments, intelligence services, and the military re-enter the picture. Zuboff describes how intelligence agencies craved information that was hard to obtain legally, but could be provided by companies operating in a lawless zone, always ahead of legislation. 9/11 was just a door opener; from then on, massive interlacing ensued between politics and companies in the information business, as seen for example, but not only, in the 2012 and 2016 US elections.\nZuboff’s book is full of interesting details, but this post is not meant to be a “best of”. Rather, I’m wondering how the extent, as well as the importance of, what’s going on could be made more salient to ML people – not just to those to whom doubt and skepticism come naturally, but to well-meaning idealists as well.\n\n\nWhy we should care\nTrying to make this more salient, I can think of three things:\n\nConcisely stating a few facts that, important though they are in Zuboff’s argumentation, deserve being pointed out even outside the concrete contexts in which they appear in the book;\nCalling attention, in a few lines instead of a time-consuming narrative, to what has already happened, in the “real world”, in the very short time span since those new dynamics took off; and (the most impressive to me, personally)\nRelating, by citation, the actual “utopias” behind the reality.\n\n\n\n(1) A few things to consider\nA common way to think about this is: We’re using a free service (like a search engine), we have to pay somehow, so we’re paying with our data. But – data is collected and processed by products we buy, as well, be it phones, wearables or household appliances.\nAlso, it is not like we get to decide whether and if so, how much privacy we’re willing to give up in exchange for a service. As often noted, no-one has time to read through all the “privacy policies” we’re bombarded with; and sometimes we’re just told “you can disable this; but then feature x [[[some vital feature, probably the one we bought it for]]] can’t be guaranteed to be working anymore”.\n\n\n(2) Not just online\nEven to people who spend most of their day online, being tracked on the web may still seem like a minor inconvenience. Of course, through our smartphones, we carry the web with us. Evidently, depending on what apps one has installed, one may receive friendly “invitations” to, say, pay a visit to McDonald’s just as one passes there on one’s lunchtime run. (Or maybe not, depending on whether the type of ongoing activity is already being taken into account.)\nSure, such notifications disrupt the flow of whatever one’s doing. But still: For some of us, it’s easy to think that who cares? I don’t have those kinds of apps installed anyway. Put more generally: Yes, all this online tracking is annoying. But as long as all that happens is me seeing more “targeted” ads, why bother?\nThe recent years have seen a consistent and speedy extension of tracking in the real world.\nThere was Google Street View, filming the outdoors, a space once thought of as ephemeral, and subsequent attempts, by various actors, to map public interiors.\nThere was Google glass, allowing to video- and audiotape people without their knowledge.\nThen, there are all kinds of wearables, also able to record not just the wearer’s, but bystanders’ data. The wearers themselves must be aware that some data is collected (why else buy the thing?), but don’t necessarily know how it’s being made use of, and what other data may be collected.\nThen, there are all kinds of IoT devices: TVs, vacuum cleaners, thermostats… Again, they are bought for a reason; but no-one can expect a TV to record what you’re saying, or a vacuum cleaner to transmit back your floor plan.\nThen – and this makes for a perfect transition to the last section, “utopias” – there is smart city: not a utopia, but actual reality, already being implemented in Toronto. Here is an excerpt from an interview with CEO Dan Doctoroff:\n\nIn effect, what we’re doing is replicating the digital experience in physical space … So ubiquitous connectivity; incredible computing power including artificial intelligence and machine learning; the ability to display data; sensing, including cameras and location data as well as other specialized sensors… We fund it all… through a very novel advertising model… We can actually then target ads to people in proximity, and then obviously over time track them through things like beacons and location services as well as their browsing activity.\n\nFrom these publicly made statements, it is hard to doubt what The Globe and Mail reported re. a leaked document, the yellow book, containing detailed outlines about how this system would be implemented:\n\nEarly on, the company notes that a Sidewalk neighbourhood would collect real-time position data “for all entities” – including people. The company would also collect a “historical record of where things have been” and “about where they are going.” Furthermore, unique data identifiers would be generated for “every person, business or object registered in the district,” helping devices communicate with each other.\n\nThe newspaper continues,\n\nThe document also describes reputation tools that would lead to a “new currency for community co-operation,” effectively establishing a social credit system. Sidewalk could use these tools to “hold people or businesses accountable” while rewarding good behaviour, such as by rewarding a business’s good customer service with an easier or cheaper renewal process on its licence.\n\nIf, to people in “Western cultures”, China, with its open endorsement and implementation of a social credit system, tended to seem “far, far away” – now nothing seems as it did before.\nAt this point, it’s time to switch gears and look at the world view(s) behind those recent developments.\n\n\n(3) Utopias\nHere is part of the vision of Mark Zuckerberg. Zuboff, aggregating information from several Zuckerberg-quoting, publicly available sources, writes (p.402)\n\nPredictive models would enable the corporation to “tell you to what bar to go to” when you arrive in a strange city. The vision is detailed: when you arrive at the bar, the bartender has your favorite drink waiting, and you’re able to look around the room and identify people just like you.\n\nExtending this to life overall, here we have a vision of total boredom; no exploration, no curiosity, no surprise, no learning. How probable is it that evolutionarily, humans are prepared to live in a world without uncertainty?\nHow about regulation? Here is Larry Page on laws and experimentation:\n\nIf you look at the different kinds of laws we make, they’re very old. The laws when we went public we’re 50 years old. A law can’t be right if it’s 50 years old, like it’s before the Internet, that’s a pretty major change, in how you may go public.[…]\n\n\nThe other thing in my mind is we also haven’t built mechanisms to allow experimentation. There’s many, many exciting and important things you could do that you just can’t do because they’re illegal, or they’re not allowed by regulation, and that makes sense, we don’t want the world to change too fast. Maybe we should set aside a small part of the world …[…]\n\n\nI think as technologists we should have some safe places where we can try out some new things and figure out what is the effect on society, what’s the effect on people, without having to deploy kind of into the normal world. […]\n\nOr Zuckerberg again, on the ancient social norm of privacy:\n\n“People have really gotten comfortable not only sharing more information and different kinds, but more openly and with more people,” he said. “That social norm is just something that has evolved over time.”\n\nIn this light, is a social credit system – perhaps packaged differently – really so far away, in Western cultures? Sometimes, a scenario sounds so silly that it’s hard to see it as a real threat; but that may be just us being naive. Here is another vision, from Rana el Kaliouby, CEO at Affectiva:\n\nWe have been in conversations with a company in that space. It is an advertising-rewards company, and its business is based on positive moments. So if you set a goal to run three miles and you run three miles, that’s a moment. Or if you set the alarm for six o’clock and you actually do get up, that’s a moment. And they monetize these moments. They sell them. Like Kleenex can send you a coupon – I don’t know – when you get over a sad moment. Right now, this company is making assumptions about what those moments are. And we’re like, ‘Guess what? We can capture them.’\n\nIf that sounds like it needn’t be taken seriously, here is a quote from Social Physics, a highly acclaimed opus by Alex Pentland, director of MIT Media Lab:\n\nThe social physics approach to getting everyone to cooperate is to use social network incentives rather than to use individual market incentives or to provide additional information. That is, we focus on changing the connections between people rather than focussing on getting people individually to change their behavior. The logic here is clear: Since exchanges between people are of enormous value to the participants, we can leverage those exchanges to generate social pressure for change.[…]\n\n\nSocial network incentives act by generating social pressure around the problem of finding cooperative behaviors, and so people experiment with new behaviors to find ones that are better.\n\nAt this point, I feel like nothing has to be added to those visions. But let’s get back to the beginning, Lex Fridman’s fantasy of clustering users.\n\n\nExpressing oneself\nZuboff quotes Zuckerberg saying, “humans have such a deep need to express themselves” (p. 403). Evidently, this is what motivates Fridman’s wish, and yes, it’s a, or the, profoundly human thing.\nBut, doesn’t expressing oneself, if seen as sharing, involve a free decision when, what, and with whom to share?\nAnd what does it do to an experience if it is not just “there”, a thing valuable in itself, but a means for others, to control and profit?\nThanks for reading!"
  },
  {
    "objectID": "posts/ai_ethics_optimization_problem/index.html",
    "href": "posts/ai_ethics_optimization_problem/index.html",
    "title": "AI ethics is not an optimization problem",
    "section": "",
    "text": "But we are doing a lot to improve fairness and remove bias, aren’t we?\nSearch for “algorithmic fairness”, “deep learning fairness” or something similar, and you’ll find lots of papers, tools, guidelines … more than you have time to read. And bias: Haven’t we all heard about image recognition models failing on black people, women, and black women in particular; about machine translation incorporating gender stereotypes; about search engine results reflecting racism and discrimination? Given enough media attention, the respective algorithms get “fixed”; what’s more, we may also safely assume that overall, researchers developing new models will probe for these exact kinds of failures. So as a community, we do care about bias, don’t we?\nRe: fairness. It’s true that there are many attempts to increase algorithmic fairness. But as Ben Green, who I’ll cite a lot in this text, points out, most of this work does so via rigid formalization, as if fairness — its conflicting definitions notwithstanding — were an objective quantity, a metric that could be optimized just like the metrics we usually optimize in machine learning.\nAssume we were willing to stay in the formalist frame. Even then there is no satisfying solution to this optimization problem. Take the perspective of group fairness, where a fair algorithm is one that results in equal outcomes between groups. Chouldechova then shows that when an algorithm achieves predictive parity (a.k.a. precision), but prevalence differs between groups, it is not possible to also have both equal false positive and equal false negative rates.\nThat said, here I mainly want to focus on why pure formalism is not enough.\nWhat with bias? That datasets can be (or rather: are) biased is hardly something anyone working in AI would object to. What amount of bias is being admitted to in other components/stages of the machine learning process varies between people.\nHarini and Guttag map sources of bias to the model development and deployment process. Forms of bias they distinguish include representation (think: dataset), measurement (think: metrics3), aggregation (think: model4), and evaluation (similar to the two preceding ones, but at test time) bias. The paper is written in a technical style; in fact, there is even a diagram where the authors attempt to formalize the process in a mathematical way. (Personally, I find it hard to see what value is added by this diagram; its existence can probably best be explained by [perceived] requirements of the genre, namely, research paper.)\nNow, so far I’ve left out the remaining two sources of bias they name. Mapped to the end of the process is deployment bias, when a system “is used or interpreted in inappropriate ways”. That raises a question. Is this about the end of a process, or is what happens now a completely different process (or: processes)? For an in-house data scientist, it may well be the end of a process; for a scientist, or for a consultant, it is not. Once a scientist has developed and published a model, they have no control over who uses it and how. From the collection of humankind’s empirical truths: Once a technology exists, and it is useful, it will be used.\nThings get further out of control at the other side of the timeline. Here we have historical bias:\n\nHistorical bias arises when there is a misalignment between the world as it is and the values or objectives to be encoded and propagated in a model. It is a normative concern with the state of the world, and exists even given perfect sampling and feature selection.\n\nI’ll get back to why this is called “historical bias” later; the definition is a lot broader though. Now definitely we’re beyond the realm of formalization: We’re entering the realm of normativity, of ways of viewing the world, of ethics.\nPlease don’t get me wrong. I’m not criticizing the paper; in fact, it provides a useful categorization that may be used as a “check list” by practitioners. But given the formalist style, a reader is likely to focus on the readily-formalizable parts. It is then easy to dismiss the two others as not really being relevant to one’s work, and certainly not something one could exert influence on. (I’ll get back to that later.)\nOne little aside before we leave formalism: I do believe that a lot of the formalist work on AI ethics is done in good intent; however, work in this area also benefits organizations that undertake it. Citing Tom Slee,\n\nStandards set public benchmarks and provide protection from future accusations. Auditable criteria incorporated into product development and release processes can confirm compliance. There are also financial incentives to adopt a technical approach: standards that demand expertise and investment create barriers to entry by smaller firms, just as risk management regulations create barriers to entry in the financial and healthcare industries.\n\n\n\nNot everything in life is an optimization problem\nStephen Boyd, who teaches convex optimization at Stanford, is said to often start theintroductory lecture with the phrase “everything is an optimization problem”. It sounds intriguing at first; certainly a lot of things in my life can be thought of like that. It may become awkward once you start to compare, say, time spent with loved ones and time spent working; it becomes completely unfeasible when comparing across people.\nWe saw that even under formalist thinking, there is no impartial way to optimize for fairness. But it’s not just about choosing between different types of fairness. How do you weigh fairness against stakeholder interests? No algorithm will be deployed that does not serve an organization’s purpose.\nThere is thus an intimate link between metrics, objectives and power.\n\n\nMetrics and power\nEven though terminologically, in deep learning, we distinguish between optimization and metrics, the metrics really are what we are optimizing for: Goodhart’s law — When a measure becomes a target, it ceases to be a good measure5— does not seem to apply. Still, they deserve the same questioning and inspection as metrics in other contexts.\nIn AI as elsewhere, optimization objectives are proxies; they “stand in” for the thing we’re really interested in. That proxying process could fail in many ways, but failure is not the only applicable category to think in here.\nFor one, objectives are chosen according to the dominant paradigm. Dotan and Milli show how a technology’s perceived success feeds back into the criteria used to evaluate other technologies (as well as future instances of itself). Imagine a world where models were ranked not just for classification accuracy, but also for, say, climate friendliness, robustness to adversarial attacks, or reliable quantification of uncertainty.\nObjectives thus do not emerge out of nothing. That they reflect paradigms may still sound abstract; that they serve existing power structures less so. Power structures are complex; they reflect more than just who “has the say”. As pointed out in various “classics” of critical race theory, intersectionalist feminism and related areas6, we should ask ourselves: Who profits?\nThis is an all but simple question, comparable in difficulty, it seems to me, to the request that we question the unspoken premises that underlie our theories. The main point about such premises is that we aren’t aware of them: If we were, we could have stated them explicitly. Similarly, if I noticed I was profiting from someone, I would — hopefully — take some action. Hard as the task may be, though, we have to do our best.\nIf it’s hard to see how one is privileged, can’t we just be neutral? Objective? Isn’t this getting too political?\n\n\nThere is no objectivity, and all is politics\nTo some people, the assertion that objectivity cannot exist is too self-evident to require much dwelling on. Are numbers objective? Maybe in some formal way; but once I use them in communication, they convey a message, independently of my intention. \nLet’s say I want to convey the fact, taken from the IPCC website, that between 2030 and 2052, global warming is likely to reach 1.5°C. Let’s also assume that I look up the pre-industrial average for the place where I happen to live (15°C, say), and that I want to show off my impressive meteorological knowledge. Thus I say, “… so here, guys, temperature will rise from 288.15 Kelvin to 289.65 Kelvin, on average”. This surely is an objective statement. But what if the people I’m talking to don’t know that — even though the absolute values, when expressed in Kelvin, are so much higher than when expressed in degrees Celsius — the relative differences are the same? They might get the impression, totally unintended, that not much warming is going to happen.\nIf even numbers, once used in communication, lose their objectivity, this must hold even more for anything that involves more design choices: visualizations, APIs, and, of course, written text. For visualizations, this is nicely illustrated in d’Ignazio and Klein’s Data Feminism.\nThat book is also an exemplary exercise in what appears like the only way to “deal with” the fact that objectivity is impossible: Trying to be as clear as possible about where we stand, who we are, what are the circumstances that have influenced our way of thinking. Of course, like with the unspoken premises and assumptions discussed above, this is not easy; in fact, it’s impossible to do in perfection. But one can try.\nIn fact, the above “Trying to be as clear as possible …” is deliberately ambiguous. It refers to two things: For one, to me striving to analyze how I’m privileged, and secondly, to me giving information to others. The first alone is laudable but necessarily limited; the second, as exercised by d’Ignazio and Klein, opens the door not just for better mutual understanding, but also, for feedback and learning. The person I’m talking to might lead me to insights I wouldn’t have gotten otherwise.\nPutting things slightly differently, there is no objectivity because there’s always a context. Focus on metrics and formalization detract from that context. Green relates an interesting parallel from American law history. Until the early twentieth century, US law was dominated by a formalist ethos. Ideally, all rules should be traceable to a small number of universal principles, derived from natural rights. Autonomy being such a right, in a famous 1905 case, the U.S. Supreme Court concluded that a law limiting the working hours of employees represented “unreasonable, unnecessary and arbitrary interference with the right and liberty of the individual to contract”. However,\n\nIn his dissent, Justice Oliver Wendell Holmes argued that the Court failed to consider the context of the case, noting, “General propositions do not decide concrete cases”.\n\nThus, the context matters. Autonomy is good; but it can’t be used as an excuse for exploitation.\nThe same context dependence holds in AI. It is always developed and deployed in a context, — a context shaped by history. History determines what datasets we work with; what we optimize for; who tells us what to optimize for. An daunting example is so-called “predictive policing”. Datasets used to train prediction algorithms incorporate a history of racial injustice: The very definition of “crime” they rely on wasshaped by racist and classist practice. The new method then perpetuates — more than that: exacerbates — the current system, creating a vicious cycle of positive feedback that makes it look like the algorithm was successful.\nSumming up: When there is no neutrality, everything is politics. Not acting is acting. Citing Green,\n\nBut efforts for reform are no more political than efforts to resist reform or even the choice simply to not act, both of which preserve existing systems.\n\n\n\nBut I’m just an engineer\nWhen machine learning people, or computer science people in general, are asked about their views on the societal impact of modern AI, an often-heard answer is: “But I’m just an engineer…”. This is completely understandable. Most of us are just tiny cogs in those big machines, wired together in a complex network, that run the world. We’re not in control; how could we be accountable?\nFor the AI scientist, though normally all but sitting in an “ivory tower”, it’s nevertheless the ML engineers who are responsible of how a model gets deployed, and to what consequences⁷. The ML engineer may delegate to the head of IT, who in turn had no choice but implement what was requested “by business”. And so on and so forth, ad infinitum.\nThat said, it is hard to come up with a moral imperative here. In the line of thinking exercised above: There is always a context. In some parts of the world, you have more choices than in others. Options vary based on race, gender, abledness, and more. Maybe you can just quit and get another job; maybe you can’t.\nThere is another — related — point though, on which I’d like to dwell a little longer.\n\n\nTechnology optimism\nSometimes, it’s not that the people working in AI are fully aware of, but don’t see how to counter, the harm that is being done in their field. On the contrary. They are convinced that they’re doing good. Just do a quick search for “AI for good”, and you’ll be presented with a great number of projects and initiatives. But how, actually, is decided what is good? Who decides? Who profits?\nBen Green, again, relates an instructive example:\n\nUSC’s Center for Artificial Intelligence in Society (CAIS) is emblematic of how computer science projects labeled as promoting “social good” can cause harm by wading into hotly contested political territory with a regressive perspective. One of the group’s projects involved deploying game theory and machine learning to predict and prevent behavior from “adversarial groups.” Although CAIS motivated the project by discussing “extremist organizations such as ISIS and Jabhat al-Nusra,” it quickly slipped into focusing on “criminal street gangs” [43]. In fact, the project’s only publication was a controversial paper that used neural networks to classify crimes in Los Angeles as gang-related [28, 41].\n\nPredictive policing, already mentioned above, can also be seen in this category. At first thought, isn’t it a good thing? Wouldn’t it be nice if we could make our world a bit more secure?\nPhillip Rogaway, who I’ll mention again in the concluding section, talks a lot about how technology optimism dominates among his students; he seems to be as intrigued by it as I am. Personally, I think that whether someone “intrinsically” tends to be a technology optimist or a pessimist is a persistent trait; it seems to be a matter of personality and socialization (or just call it fate: I don’t want to go into any nature-nurture debates here). That glass, is it half full or half empty?\nAll I could say to a hardcore technology optimist is that their utopia may be another person’s dystopia. Especially if that other person is poor, or black, or poor and a black woman … and so on.\nLet me just end this section with a citation from a blog post by Ali Alkhatib centered around the launch of the Stanford institute for human-centered artificial intelligence (HAI). Referring to the director’s accompanying blog post, he writes\n\nJames opens with a story of an office that senses you slouching, registers that you’re fatigued, intuits that your mood has shifted, and alters the ambiance accordingly to keep you alert throughout the day.\n\nYou can head to Alkhatib’s post (very worth reading) and read the continuation, expanding on the scenario. But for some people, this single sentence may already be enough.\n\n\nAnd now?\nAt some point, an author is expected to wrap up and present ideas for improvement. With most of the topics touched upon here, this is yet another intimidating task. I’ll give it a try anyway. The best synthesis I can come up with at the moment looks about like this.\nFirst, some approaches filed by Green under “formalist” can still make for a good start, or rather, can constitute a set of default measures, to be taken routinely. Most prominently, these include dataset and model documentation.\nBeyond the technical, I like the advice given in Baumer and Silberman’s When the Implication is Not to Design. If the consequences, especially on socially marginalized groups, of a technological approach are unforeseeable, think whether the problem can be solved in a “low-tech” way. By all means, do not start with the solution and then, go find a problem.\nIn some cases, even that may not be enough. With some goals, don’t look for alternative ways to achieve them. Sometimes the goal itself has to be questioned.\nThe same can be said for the other direction. With some technologies, there is no goal that could justify their application. This is because such a technology is certain to get misused. Facial recognition is one example.\nLastly, let me finish on a speculative note. Rogaway, already referred to above for his comments on technology optimism, calls on his colleagues, fellow cryptographers, to devise protocols in such a way that private communication stays private, that breaches are, in plain terms, impossible. While I personally can’t think see how to port the analogy to AI, maybe others will be able to, drawing inspiration from his text. Until then, changes in politics and legislation seem to be the only recourse.\n\n\n\n\n\n\nFootnotes\n\n\nFor brevity, I’ll be subsuming deep learning and other contemporary machine learning methods under AI, following common(-ish) usage.↩︎\n I can’t hope to express this better than Maciej Cegłowski did here, so I won’t elaborate on that topic any further.↩︎\nMore on that below. Metrics used in machine learning mostly are proxies for things we really care about; there are lots of ways this can go wrong.↩︎\nMeaning, a one-model-fits-all approach puts some groups at a disadvantage.↩︎\ncited after Thomas and Uminsky↩︎\nsee e.g., DataFeminism and RaceAfterTechnology.↩︎"
  },
  {
    "objectID": "posts/solms_consciousness/index.html",
    "href": "posts/solms_consciousness/index.html",
    "title": "A book I’d say everyone should read if such were a kind of thing I’d say",
    "section": "",
    "text": "However, having read around some (first and foremost, Blackmore & Troscianko’s Consciousness: An Introduction, which I liked a lot), I certainly had not made up my mind. All I knew was that, certainly, I would not want to be a dualist; that the dominant neuroscience-guided theories felt simplistic; that I wasn’t convinced quantum physics were all we need; and finally, that Daniel Dennett’s multiple drafts formulation seemed most in concordance with my views. Otherwise though, there seemed to be a Rashomon-like touch to the topic: We’d be confronted with a set of parallel narratives – just that, here, it wasn’t too clear they were even trying to tell the same story.\nIn most cases, though, at least I got an impression of the kind of story they wanted to tell. With one exception. Before I came across Blackmore & Troscianko, I did the obvious thing: look up “consciousness” in the Stanford Encyclopedia of Philosophy. And there they were: the mysterious qualia. (Very roughly, the “what it is like”s of perceiving something.1) Now perception, to me, is of high interest; and it is something we know a lot about, on various levels. But that question: What is it like to see “red”? What is it like to see? – What does it matter? The whole topic of qualia seemed utterly unimportant.\nBut that is not something you say out loud, is it? When centuries of philosophical tradition speak otherwise.\n\nIntrepid\nNow comes Mark Solms and, based on a solid foundation of brain science research, does away with the thing all in one sweep. Not at all impressed by scholarly authority, Solms argues with what I can’t call by any other name than common sense.\nJust like other modes of perception, vision works without consciousness, without us being aware of what we see. It does not matter what I “feel” when I see red; all that matters is that I react in a situationally adequate way. (Depending on the circumstances, the correct thing to do may be to pick the raspberry, or to stop in front of the traffic light; but in neither case would it be fruitful for me to halt and reflect on the redness of the thing in question.)\nWait – let me repeat that phrase: what I feel when […]. In that phrase, the magical term has made its appearance. This is the narrative, the intrepid narrative, Solms is telling: Consciousness equals awareness equals feeling.\nWhy?\nThe first equality, linking consciousness and awareness, would not appear that “far-fetched”, from a more mainstream angle of view. And the essential thing about feelings is that this their whole point: for us to be aware of them.\nFor Solms, feelings don’t start with the dramatic, the intricate, or the sublime. Hunger is a feeling; being hypothermic is; both of them of different type, but comparable in function to the primary emotions fear, rage, panic/grief, seeking, play, and care2. What all feelings have in common is that they guide our behavior. With “primitive” emotions such as thirst and freezing, the behavior they induce may directly be related to survival; “higher” emotions help in, as Solms nicely puts it, “feeling our way” through the vast (if left unfiltered) space of behavioral options opening up at any moment.\nIn a nutshell, thus, the argumentation goes like this: How can a feeling guide me if I’m not aware? The hunger I don’t feel won’t make me eat.\nThis, it seems to me, is the first great achievement of Solms’ common sense: Forget about the redness of red and the blueness of blue; instead think how it feels to be feeling. I feel, therefore I’m conscious.\nThe second achievement of common sense, however, is the principal reason I am writing this text; and it is of yet greater importance. Before we get to it, one last remark.\nAbove, I was already suggesting that, with the topic of consciousness, we may sometimes feel like we’re lost in a cobweb of narratives, with different nodes in that graph more or less densely connected, more or less compatible, more or less sharing a common language. And really it seems to me, now, that this is largely a matter of language; of the phrases we use, the economies of neural effort we derive from following the trodden paths of learned linguistic habits. When Solms challenges the neocortex-centric, analytic, higher-order, reflective view of consciousness, he does not just fight against the very solid windmills of anthropocentrism (the next thing I’ll be talking about). He also has to counteract the effects of highly established, utterly present-in-our-minds expressions like “stream of consciousness”. In fact, when first making my way through Blackmore & Troscianko’s labyrinth of viewpoints, I found myself trying to relate those to familiar concepts like this one. Stream of consciousness, that perpetual narrator’s voice … And maybe even more than people with visual imagination, those with aphantasia3, who can’t see a thing “in their mind’s eye” – do have a mind’s ear, and may not ever know how to turn off that voice. These stories they can’t help listening to, be they more like comments or like dissections, seem to suggest that consciousness were “on top of”, were more than what really there is. –\nNow, on to where common sense gets revolutionary.\n\n\nThe next (and desperately needed!) Copernican turn\nWithout further ado, let me spell it out. There are kids, born without a cortex4, who, in situations, look like they’re enjoying themselves, have fun, feel happy about something. (Probably they also feel bad, in other situations, but these are not the situations Solms relates in this book.) If they look like they enjoy “it”, and joy is a feeling, why would we doubt that they’re feeling something? Isn’t this common sense, as well: If someone, a person – let me go further already – a being looks like they sense, feel, experience, what justification do we have to claim they are just looking it?\nConcretely, what Solms says is that at least all mammals, but to different degrees, also other classes share the brain structures associated with feeling. Various degrees: There is no on-or-off, no conscious-yes-or-no. If your cat purrs as you stroke her, would you assume she doesn’t have fun? If I see a crow monitoring the grasslands, should I think there’s no intent there, no purpose? If the flower aligns itself with the sun –\nWe don’t know. All we can surmise is that there is a continuum; of intent, of experience, of feeling. Maybe there also is an other-ness, something we humans don’t have concepts for. In any case, what follows from this is that we have no right to impose ourselves on nature, the way we’ve been doing for millennia. Not only is there no creation, – man (or woman) certainly is not its culmination.\nAnd this is why I would like to write: Everyone should read this book. It is time, high time, for the next Copernican revolution, after the planetary one and the evolutionary one. We could call it the revolution of neuroscience, grounded as it is in brain science results, but I’d rather stay with what makes me think it’s so over-due: the revolution of common sense.\n\n\nA necessary remark, and two quick comments\nBefore I end, I want to make sure I haven’t, through this text, provoked a misunderstanding that might keep you from reading the book. I am not a neuroscientist, and in this write-up, I have distilled what in my view is the book’s main message. But this is not a book of groundless opinions, and less even, of politics or zealotism. The (subcortical) structures that create consciousness do have names; the book has fifty pages of end notes linking to studies underpinning the conclusions drawn. You don’t have to believe my condensation – just read the book and see for yourself.\nFinally, to make clear that this is about the book’s constitutive message, and not a product of all-embracing exaltation, let me name the two threads of argumentation I do not agree with.\nFirst – and here, again, I have to stress that I am not a neuroscientist, and thus, can only speak from what is, it seems to me, an instance of common sense as well: Solms’ espousal of Karl Friston and his framework seems to go, well, pretty far5. Nothing more convincing than the view that the brain works by constantly updating its predictions; you don’t have to be an expert in Bayes to believe that. But from there to the precise equations postulated by Friston it is a long way… (Truth be told, neither can I get over what I read in the Guardian, in 2020, when Covid was on its first steep surge … See for yourself.)\nSecondly, I cannot not mention I have significant problems with how the book ends: the announcement that Solms and his group will try to engineer artificial consciousness. The arguments in favor are all too familiar: We need to do this to prove our scientific hypothesis; if we don’t do it, someone else will; we’ll patent the whole process; and as soon as it’s done (hypothesis confirmed), we’ll shut it off. Oh brave new world …\nThose are comments I felt I needed to make, but neither takes away anything from the fact that, yes, this book is eminently important, and that yes, we, human beings, animals, earth all need that revolution to happen.\nPhoto by Hans-Jurgen Mager on Unsplash\n\n\n\n\n\nFootnotes\n\n\nThey have their own Stanford Encyclopedia entry: https://plato.stanford.edu/entries/qualia/.↩︎\nThis is the taxonomy of primary emotions embraced by Solms. It is mainly due to the work of Jaak Panksepp (see, e.g., Panksepp & Biven, The Archeology of Mind).↩︎\nAs of this writing, the only book I know on this topic is Alan Kendle’s Aphantasia: Experiences, Perceptions, and Insights.↩︎\nThis condition is called hydranencephaly.↩︎\nFor details on the theory, see also the paper he wrote with Friston, How and Why Consciousness Arises: Some Considerations from Physics and Physiology.↩︎"
  },
  {
    "objectID": "posts/graphdl/index.html",
    "href": "posts/graphdl/index.html",
    "title": "Getting started with deep learning on graphs",
    "section": "",
    "text": "If, in deep-learning world, the first half of the last decade has been the age of images, and the second, that of language, one could say that now, we’re living in the age of graphs. At least, that’s what commonly cited research metrics suggest. But as we’re all aware, deep-learning research is anything but an ivory tower. To see real-world implications, it suffices to reflect on how many things can be modeled as graphs. Some things quite naturally “are” graphs, in the sense of having nodes and edges: neurons, underground stations, social networks. Other things can fruitfully be modeled as graphs: molecules, for example; or language, concepts, three-dimensional shapes … If deep learning on graphs is desirable, what are the challenges, and what do we get for free?"
  },
  {
    "objectID": "posts/graphdl/index.html#whats-so-special-about-deep-learning-on-graphs",
    "href": "posts/graphdl/index.html#whats-so-special-about-deep-learning-on-graphs",
    "title": "Getting started with deep learning on graphs",
    "section": "What’s so special about deep learning on graphs?",
    "text": "What’s so special about deep learning on graphs?\nGraphs are different from images, language, as well as tabular data in that node numbering does not matter. In other words, graphs are permutation-invariant. Already this means that architectures established in other domains cannot be transferred verbatim. (The ideas underlying them can be transferred though. Thus, in the graph neural network (henceforth: GNN) model zoo you’ll see lots of allusions to “convolutional”, “attention”, and other established terms.) Put very simply, and in concordance with common sense, whatever algorithm is used, it will fundamentally be based on how nodes are connected: the edges, that is.\nWhen relationships are modeled as graphs, both nodes and edges can have features. This, too, adds complexity. But not everything is harder with graphs. Think of how cumbersome it can be to obtain labeled data for supervised learning. With graphs, often an astonishingly small amount of labeled data is needed. More surprisingly still, a graph can be constructed when not a single edge is present. Put differently, learning on sets can morph into learning on graphs.\nAt this point, let me switch gears and move on to the practical part: the raison d’être of this post."
  },
  {
    "objectID": "posts/graphdl/index.html#matching-concepts-and-code-pytorch-geometric",
    "href": "posts/graphdl/index.html#matching-concepts-and-code-pytorch-geometric",
    "title": "Getting started with deep learning on graphs",
    "section": "Matching concepts and code: PyTorch Geometric",
    "text": "Matching concepts and code: PyTorch Geometric\nIn this (and future) posts, we’ll make use of PyTorch Geometric (from hereon: PyG), the most popular, at this time, and fastest-growing in terms of functionality as well as user base, library dedicated to graph DL.\nDeep learning on graphs, in its most general form, is usually characterized by the term message passing. Messages are passed between nodes that are linked by an edge: If node \\(A\\) has three neighbors, it will receive three messages. Those messages have to be summarized in some meaningful way. Finally – GNNs consisting of consecutive layers – the node will have to decide how to modify its previous-layer features (a.k.a. embeddings) based on that summary.\nTogether, these make up a three-step sequence: collect messages; aggegate; update. What about the “learning” in deep learning, though? There are two places where learning can happen: Firstly, in message collection: Incoming messages could be transformed by a MLP, for example. Secondly, as part of the update step. All in all, this yields mathematical formulae like this, given in the PyG documentation:\n\\[\n\\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\right)\n\\]\nScary though this looks, once we read it from the right, we see that it nicely fits the conceptual description. The \\((\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i})\\) are the three types of incoming messages a node can receive: its own state at the previous layer, the states of its neighbors (the nodes \\(j \\in \\mathcal{N}(i)\\)) at the previous layer, and features/embeddings associated to the edge in question. (I’m leaving out edge features in this discussion completely, so as to not further enhance complexity.) These messages are (optionally) transformed by the neural network \\(\\phi\\), and whatever comes out is summarized by the aggregator function \\(\\square\\). Finally, a node will update itself based on that summary as well as its own previous-layer state, possibly by means of applying neural network \\(\\gamma\\).\nNow that we have this conceptual/mathematical representation, how does it map to code we see, or would like to write? PyG has excellent, extensive documentation, including at the beginner level. But here, I’d like to spell things out in detail – pedantically, if you like, but in a way that tells us a lot about how GNNs work.\nLet’s start by the information given in one of the key documentation pages, Creating message passing networks:\n\nPyG provides the MessagePassing base class, which helps in creating such kinds of message passing graph neural networks by automatically taking care of message propagation. The user only has to define the functions \\(\\phi\\), i.e. message(), and \\(\\gamma\\), i.e. update(), as well as the aggregation scheme to use, i.e. aggr=\"add\", aggr=\"mean\" or aggr=\"max\".\n\nScrolling down that page and looking at the two example implementations, however, we see that an implementation of update() does not have to be provided; and from inspecting the source code, it is clear that, technically, the same holds for message(). (And unless we want a form of aggregation different from the default add, we do not even need to specify that, either.)\nThus, the question becomes: What happens if we code the minimal PyG GNN? To find out, we first need to create a minimal graph, one minimal enough for us to track what is going on.\n\nA minimal graph\nNow, a basic Data object is created from three tensors. The first holds the node features: two features each for five nodes. (Both features are identical on purpose, for “cognitive ease” – on our, not the algorithm’s, part.)\n\nimport torch\n\nx = torch.tensor([[1, 1], [2, 2], [3, 3], [11, 11], [12, 12]], dtype=torch.float)\n\nThe second specifies existing connections. For undirected graphs (like ours), each edge appears twice. The tensor you see here is specified in one-edge-per-line form for convenience reasons; to the Data() constructor we’ll pass its transpose instead.\n\nedge_index = torch.tensor([\n  [0, 1],\n  [1, 0],\n  [0, 2],\n  [2, 0],\n  [1, 2],\n  [2, 1],\n  [2, 3],\n  [3, 2],\n  [2, 4],\n  [4, 2],\n  [3, 4],\n  [4, 3]\n], dtype=torch.long)\n\nThe third tensor holds the node labels. (The task will be one of node – not edge, not graph – classification.)\n\ny = torch.tensor([[0], [0], [0], [1], [1]], dtype=torch.float)\n\nConstructing and inspecting the resulting graph, we have:\n\nfrom torch_geometric.data import Data\n\ndata = Data(x = x, edge_index = edge_index.t().contiguous(), y = y)\ndata.x\ndata.edge_index\ndata.y\n\ntensor([[ 1.,  1.],\n        [ 2.,  2.],\n        [ 3.,  3.],\n        [11., 11.],\n        [12., 12.]])\n        \ntensor([[0, 1, 0, 2, 1, 2, 2, 3, 2, 4, 3, 4],\n        [1, 0, 2, 0, 2, 1, 3, 2, 4, 2, 4, 3]])\n        \ntensor([[0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.]])\nFor our upcoming experiments, it’s more helpful, though, to visualize the graph:\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom torch_geometric.utils import to_networkx\n\ndef visualize_graph(G, color, labels):\n    plt.figure(figsize=(7,7))\n    plt.axis('off')\n    nx.draw_networkx(\n      G,\n      pos = nx.spring_layout(G, seed = 777),\n      labels = labels,\n      node_color = color,\n      cmap = \"Set3\"\n      )\n    plt.show()\n\nG = to_networkx(data, to_undirected = True, node_attrs = [\"x\"])\nlabels = nx.get_node_attributes(G, \"x\")\nvisualize_graph(G, color = data.y, labels = labels)\n\n\nAlthough our experiments won’t be about training performance (how could they be, with just five nodes), let me remark in passing that this graph is small, but not boring: The middle node is equally connected to both “sides”, yet feature-wise, it would pretty clearly appear to belong on just one of them. (Which is true, given the provided class labels). Such a constellation is interesting because, in the majority of networks, edges indicate similarity.\n\n\nA minimal GNN\nNow, we code and run the minimal GNN. We’re not interested in class labels (yet); we just want to see each node’s embeddings after a single pass.\n\nfrom torch_geometric.nn import MessagePassing\n\nclass IAmLazy(MessagePassing):\n    def forward(self, x, edge_index):\n        out = self.propagate(edge_index, x = x)\n        return out\n      \nmodule = IAmLazy()\nout = module(data.x, data.edge_index)\nout\n\ntensor([[ 5.,  5.],\n        [ 4.,  4.],\n        [26., 26.],\n        [15., 15.],\n        [14., 14.]])\nEvidently, we just had to start the process – but what process, exactly? From what we know about the three stages of message passing, an essential question is what nodes do with the information that flows over the edges. Our first experiment, then, is to inspect the incoming messages.\n\n\nPoking into message()\nIn message(), we have access to a structure named x_j. This tensor holds, for each node \\(i\\), the embeddings of all nodes \\(j\\) connected to it via incoming edges. We’ll print them, and then, just return them, unchanged.\n\nclass IAmMyOthers(MessagePassing):\n    def forward(self, x, edge_index):\n        out = self.propagate(edge_index, x = x)\n        return out\n    def message(self, x_j):\n        print(\"in message, x_j is\")\n        print(x_j)\n        return x_j\n      \nmodule = IAmMyOthers()\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n\nin message, x_j is\ntensor([[ 1.,  1.],\n        [ 2.,  2.],\n        [ 1.,  1.],\n        [ 3.,  3.],\n        [ 2.,  2.],\n        [ 3.,  3.],\n        [ 3.,  3.],\n        [11., 11.],\n        [ 3.,  3.],\n        [12., 12.],\n        [11., 11.],\n        [12., 12.]])\n        \nresult is:\ntensor([[ 5.,  5.],\n        [ 4.,  4.],\n        [26., 26.],\n        [15., 15.],\n        [14., 14.]])\nLet me spell this out. In data.edge_index, repeated here for convenience:\ntensor([[0, 1, 0, 2, 1, 2, 2, 3, 2, 4, 3, 4],\n        [1, 0, 2, 0, 2, 1, 3, 2, 4, 2, 4, 3]])\nthe first pair denotes the edge from node 0 (that had features (1, 1)) to node 1. This information is found in x_j’s first row. Then the second row holds the information flowing in the opposite direction, namely, the features associated with node 1. And so on.\nInterestingly, since we’re passing through this module just once, we can see the messages that will be sent without even running it.\nNamely, since data.edge_index[0] designates the source nodes for each edge:\n\ndata.edge_index[0]\n\nwe can index into data.x to pick up what will be the incoming features for each connection.\n\ndata.x[data.edge_index[0]]\n\ntensor([[ 1.,  1.],\n        [ 2.,  2.],\n        [ 1.,  1.],\n        [ 3.,  3.],\n        [ 2.,  2.],\n        [ 3.,  3.],\n        [ 3.,  3.],\n        [11., 11.],\n        [ 3.,  3.],\n        [12., 12.],\n        [11., 11.],\n        [12., 12.]])\nNow, what does this tell us? Node 0, for example, received messages from nodes 1 and 2: (2, 2) and (3, 3), respectively. We know that the default aggregation mode is add; and so, would expect an outcome of (5, 5). Indeed, this is the new embedding for node 0.\nIn a nutshell, thus, the minimal GNN updates every node’s embedding so as to prototypically reflect the node’s neighborhood. Take care though: Nodes represent their neighborhoods, but themselves, they count for nothing. We will change that now.\n\n\nAdding self loops\nAll we need to do is modify the adjacency matrix to include edges going from each node back to itself.\n\nfrom torch_geometric.utils import add_self_loops\n\nclass IAmMyOthersAndMyselfAsWell(MessagePassing):\n    def forward(self, x, edge_index):\n        edge_index, _ = add_self_loops(edge_index, num_nodes = x.size(0))\n        print(\"in forward, augmented edge index now has shape\")\n        print(edge_index.shape)\n        out = self.propagate(edge_index, x = x)\n        return out\n    def message(self, x_j):\n        return x_j\n\nmodule = IAmMyOthersAndMyselfAsWell()\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n\nin forward, augmented edge index now has shape:\ntorch.Size([2, 17])\n\nresult is:\ntensor([[ 6.,  6.],\n        [ 6.,  6.],\n        [29., 29.],\n        [26., 26.],\n        [26., 26.]])\nAs expected, the neighborhood summary at each node now includes a contribution from each node itself.\nNow we know how to access the messages, we’d like to aggregate them in a non-standard way.\n\n\nCustomizing aggregate()\nInstead of message(), we now override aggregate(). If we wanted to use another of the “standard” aggregation modes (mean, mul, min, or max), we could just override __init__(), like so:\n\ndef __init__(self):\n        super().__init__(aggr = \"mean\")\n\nTo implement custom summaries, however, we make use of torch_scatter (one of PyG’s installation prerequisites) for optimal performance. Let me show this by means of a simple example.\n\nfrom torch_scatter import scatter\n\nclass IAmJustTheOppositeReally(MessagePassing):\n    def forward(self, x, edge_index):\n        out = self.propagate(edge_index, x = x)\n        return out\n    def aggregate(self, inputs, index):\n        print(\"in aggregate, inputs is\")\n        # same as x_j (incoming node features)\n        print(inputs)\n        print(\"in aggregate, index is\")\n        # this is data.edge_index[1]\n        print(index)\n        # see https://pytorch-scatter.readthedocs.io/en/1.3.0/index.html\n        # for other aggregation modes\n        # default dim is -1\n        return - scatter(inputs, index, dim = 0, reduce = \"add\") \n      \nmodule = IAmJustTheOppositeReally()\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n\nin aggregate, inputs is\ntensor([[ 1.,  1.],\n        [ 2.,  2.],\n        [ 1.,  1.],\n        [ 3.,  3.],\n        [ 2.,  2.],\n        [ 3.,  3.],\n        [ 3.,  3.],\n        [11., 11.],\n        [ 3.,  3.],\n        [12., 12.],\n        [11., 11.],\n        [12., 12.]])\n        \nin aggregate, index is\ntensor([1, 0, 2, 0, 2, 1, 3, 2, 4, 2, 4, 3])\n\nresult is:\ntensor([[ -5.,  -5.],\n        [ -4.,  -4.],\n        [-26., -26.],\n        [-15., -15.],\n        [-14., -14.]])\nIn aggregate(), we have two types of tensors to work with. One, inputs, holds what was returned from message(). In our case, this is identical to x_j, since we didn’t make any modifications to the default behavior. The second, index, holds the recipe for where in the aggregation those features should go. Here, the very first tuple, (1, 1), will contribute to the summary for node 1; the second, (2, 2), to that for node 0 – and so on. By the way, just like x_j (in a single-layer, single-pass setup) is “just” data.x[data.edge_index[0]], that index is “just” data.edge_index[1]. Meaning, this is the list of target nodes connected to the edges in question.\nAt this point, all kinds of manipulations could be done on either inputs or index; however, we content ourselves with just passing them through to torch_scatter.scatter(), and returning the negated sums. We’ve successfully built a network of contrarians.\nBy now, we’ve played with message() as well as aggregate(). What about update()?\n\n\nAdding memory to update()\nThere’s one thing really strange in what we’re doing. It doesn’t jump to the eye, since we’re not simulating a real training phase; we’ve been calling the layer just once. If we hadn’t, we’d have noticed that at every call, the nodes happily forget who they were before, dutifully assuming the new identities assigned. In reality, we probably want them to evolve in a more consistent way.\nFor example:\n\nclass IDoEvolveOverTime(MessagePassing):\n    def forward(self, x, edge_index):\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n        out = self.propagate(edge_index, x = x)\n        return out\n    def update(self, inputs, x):\n        print(\"in update, inputs is\")\n        print(inputs)\n        print(\"in update, x is\")\n        print(x)\n        return (inputs + x)/2\n\nmodule = IDoEvolveOverTime()\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n\nin update, inputs is\ntensor([[ 6.,  6.],\n        [ 6.,  6.],\n        [29., 29.],\n        [26., 26.],\n        [26., 26.]])\nin update, x is\ntensor([[ 1.,  1.],\n        [ 2.,  2.],\n        [ 3.,  3.],\n        [11., 11.],\n        [12., 12.]])\nresult is:\ntensor([[ 3.5000,  3.5000],\n        [ 4.0000,  4.0000],\n        [16.0000, 16.0000],\n        [18.5000, 18.5000],\n        [19.0000, 19.0000]])\nIn update(), we have access to both the final message aggregate (inputs) and the nodes’ prior states (x). Here, I’m just averaging those two.\nAt this point, we’ve successfully acquainted ourselves with the three stages of message passing: acting on individual messages, aggregating them, and self-updating based on past state and new information. But none of our models so far could be called a neural network, since there was no learning involved.\n\n\nAdding parameters\nIf we look back at the generic message passing formulation:\n\\[\n\\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\right)\n\\] we see two places where neural network modules can act on the computation: before message aggregation, and as part of the node update process. First, we illustrate the former option. For example, we can apply a MLP in forward(), before the call to aggregate():\n\nfrom torch.nn import Sequential as Seq, Linear, ReLU\n\nclass ILearnAndEvolve(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(aggr = \"sum\")\n        self.mlp = Seq(Linear(in_channels, out_channels),\n                       ReLU(),\n                       Linear(out_channels, out_channels))\n    def forward(self, x, edge_index):\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n        x = self.mlp(x)\n        out = self.propagate(edge_index = edge_index, x = x)\n        return out\n    def update(self, inputs, x):\n        return (inputs + x)/2\n\nmodule = ILearnAndEvolve(2, 2)\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n\nresult is:\ntensor([[-0.8724, -0.4407],\n        [-0.9056, -0.4623],\n        [-2.0229, -1.1240],\n        [-1.8691, -1.0867],\n        [-1.9024, -1.1082]], grad_fn=<DivBackward0>)\nFinally, we can apply network modules in both places, as exemplified next.\n\n\nGeneral message passing\nWe keep the MLP from the previous class, and add a second in update():\n\nclass ILearnAndEvolveDoubly(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(aggr = \"sum\")\n        self.mlp_msg = Seq(Linear(in_channels, out_channels),\n                       ReLU(),\n                       Linear(out_channels, out_channels))\n        self.mlp_upd = Seq(Linear(out_channels, out_channels),\n                       ReLU(),\n                       Linear(out_channels, out_channels))\n    def forward(self, x, edge_index):\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n        x = self.mlp_msg(x)\n        out = self.propagate(edge_index = edge_index, x = x)\n        return out\n    def update(self, inputs, x):\n        return self.mlp_upd((inputs + x)/2)\n\nmodule = ILearnAndEvolveDoubly(2, 2)\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n\nresult is:\ntensor([[ 0.0573, -0.6988],\n        [ 0.0358, -0.6894],\n        [-0.1730, -0.6450],\n        [-0.5855, -0.4171],\n        [-0.5890, -0.4141]], grad_fn=<AddmmBackward0>)\nAt this point, I hope you’ll feel comfortable to play around, subclassing the MessagePassing base class. Also, if now you consult the above-mentioned documentation page (Creating message passing networks), you’ll be able to map the example implementations (dedicated to popular GNN layer types) to where they “hook into” the message passing process.\nExperimentation with MessagePassing was the point of this post. However, you may be wondering: How do I actually use this for node classification? Didn’t the graph have a class defined for each node? (It did: data.y.)\nSo let me conclude with a (minimal) end-to-end example that uses one of the above modules.\n\n\nA minimal workflow\nTo that purpose, we compose that module with a linear one that performs node classification:\n\nclass Network(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, num_classes):\n        super().__init__()\n        self.conv = ILearnAndEvolveDoubly(in_channels, out_channels)\n        self.classifier = Linear(out_channels, num_classes)\n    def forward(self, x, edge_index):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv(x, edge_index)\n        return self.classifier(x)\n\nmodel = Network(2, 2, 1) \n\nWe can then train the model like any other:\n\nimport torch.nn.functional as F\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\nmodel.train()\n\nfor epoch in range(5):\n    optimizer.zero_grad()\n    out = model(data.x, data.edge_index)\n    loss = F.binary_cross_entropy_with_logits(out, data.y)\n    loss.backward()\n    optimizer.step()\n\npreds = torch.sigmoid(out)\npreds\n\ntensor([[0.6502],\n        [0.6532],\n        [0.7027],\n        [0.7145],\n        [0.7165]], grad_fn=<SigmoidBackward0>)\nAnd that’s it for this time. Stay tuned for examples of how graph models are applied in the sciences, as well as illustrations of bleeding-edge developments in Geometric Deep Learning, the principles-based, heuristics-transcending approach to neural networks.\nThanks for reading!\nPhoto by Alina Grubnyak on Unsplash"
  }
]