[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "divergences",
    "section": "",
    "text": "What are Large Language Models? What are they not?\n\n\n\n\n\n\n\nDeep Learning\n\n\nAI Societal Impact\n\n\nPhilosophy (sort of)\n\n\nDissecting the Hype\n\n\n\n\nThis is a high-level, introductory article about Large Language Models (LLMs), the core technology that enables the much-en-vogue chatbots as well as other Natural Language Processing (NLP) applications. It is directed at a general audience, possibly with some technical and/or scientific background, but no knowledge is assumed of either deep learning or NLP. Having looked at major model ingredients, training workflow, and mechanics of output generation, we also talk about what these models are not.\n\n\n\n\n\n\nJun 20, 2023\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\nGroup-equivariant neural networks with escnn\n\n\n\n\n\n\n\nDeep Learning\n\n\nGeometric Deep Learning\n\n\n\n\nEscnn, built on PyTorch, is a library that, in the spirit of Geometric Deep Learning, provides a high-level interface to designing and training group-equivariant neural networks. This post introduces important mathematical concepts, the library’s key actors, and essential library use.\n\n\n\n\n\n\nMay 9, 2023\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning and Scientific Computing with R torch: the book\n\n\n\n\n\n\n\nBooks\n\n\nDeep Learning\n\n\n\n\nMuch as we humans like to believe, consciousness is not a neocortex thing, a matter of analysis and meta-analysis. Instead – says Mark Solms, in his 2021 The Hidden Spring: A Journey to the Source of Consciousness – instead, consciousness is all about feeling. A claim that, if we take it seriously (and I don’t see why we shouldn’t) has far-ranging consequences.\n\n\n\n\n\n\nApr 5, 2023\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\nImplementing rotation equivariance: Group-equivariant CNN from scratch\n\n\n\n\n\n\n\nDeep Learning\n\n\nGeometric Deep Learning\n\n\n\n\nWe code up a simple group-equivariant convolutional neural network (GCNN) that is equivariant to rotation. The world may be upside down, but the network will know.\n\n\n\n\n\n\nMar 27, 2023\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\nUpside down, a cat’s still a cat: Evolving image recognition with Geometric Deep Learning\n\n\n\n\n\n\n\nDeep Learning\n\n\nGeometric Deep Learning\n\n\n\n\nIn this first in a series of posts on group-equivariant convolutional neural networks (GCNNs), meet the main actors — groups — and concepts (equivariance). With GCNNs, we finally revisit the topic of Geometric Deep Learning, a principled, math-driven approach to neural networks that has consistently been rising in scope and impact.\n\n\n\n\n\n\nMar 9, 2023\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\nAO, NAO, ENSO: A wavelet analysis example\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nEl Niño-Southern Oscillation (ENSO), North Atlantic Oscillation (NAO), and Arctic Oscillation (AO) are atmospheric phenomena of global impact that strongly affect people’s lives. ENSO, first and foremost, brings with it floods, droughts, and ensuing poverty, in developing countries in the Southern Hemisphere. Here, we use the new torchwavelets package to comparatively inspect patterns in the three series.\n\n\n\n\n\n\nJan 19, 2023\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\nGetting started with deep learning on graphs\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nThis post introduces deep learning on graphs by mapping its central concept - message passing - to minimal usage patterns of PyTorch Geometric’s foundation-laying MessagePassing class. Exploring those patterns, we gain some basic, very concrete insights into how graph DL works.\n\n\n\n\n\n\nNov 22, 2022\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\nA book I’d say everyone should read if such were a kind of thing I’d say\n\n\n\n\n\n\n\nBooks\n\n\nPhilosophy (sort of)\n\n\n\n\nMuch as we humans like to believe, consciousness is not a neocortex thing, a matter of analysis and meta-analysis. Instead – says Mark Solms, in his 2021 The Hidden Spring: A Journey to the Source of Consciousness – instead, consciousness is all about feeling. A claim that, if we take it seriously (and I don’t see why we shouldn’t) has far-ranging consequences.\n\n\n\n\n\n\nAug 1, 2022\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\nForecasting El Niño-Southern Oscillation (ENSO)\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nEl Niño-Southern Oscillation (ENSO) is an atmospheric phenomenon, located in the tropical Pacific, that greatly affects ecosystems as well as human well-being on a large portion of the globe. We use the convLSTM introduced in a prior post to predict the Niño 3.4 Index from spatially-ordered sequences of sea surface temperatures.\n\n\n\n\n\n\nFeb 2, 2022\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\nBeyond alchemy: A first look at geometric deep learning\n\n\n\n\n\n\n\nDeep Learning\n\n\nGeometric Deep Learning\n\n\n\n\nGeometric deep learning is a “program” that aspires to situate deep learning architectures and techniques in a framework of mathematical priors. The priors, such as various types of invariance, first arise in some physical domain. A neural network that well matches the domain will preserve as many invariances as possible. In this post, we present a very conceptual, high-level overview, and highlight a few applications.\n\n\n\n\n\n\nAug 26, 2021\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\nStarting to think about AI Fairness\n\n\n\n\n\n\n\nAI Societal Impact\n\n\nDeep Learning\n\n\n\n\nThe topic of AI fairness metrics is as important to society as it is confusing. Confusing it is due to a number of reasons: terminological proliferation, abundance of formulae, and last not least the impression that everyone else seems to know what they’re talking about. This text hopes to counteract some of that confusion by starting from a common-sense approach of contrasting two basic positions: On the one hand, the assumption that dataset features may be taken as reflecting the underlying concepts ML practitioners are interested in; on the other, that there inevitably is a gap between concept and measurement, a gap that may be bigger or smaller depending on what is being measured. In contrasting these fundamental views, we bring together concepts from ML, legal science, and political philosophy.\n\n\n\n\n\n\nJul 15, 2021\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\nConvolutional LSTM for spatial forecasting\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\nIn forecasting spatially-determined phenomena (the weather, say, or the next frame in a movie), we want to model temporal evolution, ideally using recurrence relations. At the same time, we’d like to efficiently extract spatial features, something that is normally done with convolutional filters. Ideally then, we’d have at our disposal an architecture that is both recurrent and convolutional. In this post, we build a convolutional LSTM with torch.\n\n\n\n\n\n\nDec 17, 2020\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\nAI ethics is not an optimization problem\n\n\n\n\n\n\n\nAI Societal Impact\n\n\n\n\nOften, AI researchers and engineers think of themselves as neutral and “objective”, operating in a framework of strict formalization. Fairness and absence of bias, however, are social constructs; there is no objectivity, no LaTeX-typesettable remedies, no algorithmic way out. AI models are developed based on a history and deployed in a context. In AI as in data science, the very absence of action can be of political significance.\n\n\n\n\n\n\nSep 22, 2020\n\n\nSigrid Keydana\n\n\n\n\n\n\n  \n\n\n\n\nThe hard problem of privacy\n\n\n\n\n\n\n\nPrivacy\n\n\nSocial Credit\n\n\n\n\nWe live in a world of ever-diminishing privacy and ever-increasing surveillance - and this is a statement not just about openly-authoritarian regimes. Yet, we seem not to care that much, at least not until, for whatever reasons, we are personally affected by some negative consequence. This post wants to help increase awareness, casting a spotlight on recent history and also, letting words speak for themselves: Because nothing, to me, is less revealing than the “visions” that underly the actions.\n\n\n\n\n\n\nFeb 27, 2020\n\n\nSigrid Keydana\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the personal blog of Sigrid Keydana.\nI work at Posit, PBC, where I write for the Posit AI blog.\nOpinions are exclusively mine."
  },
  {
    "objectID": "posts/hard_problem_privacy/index.html",
    "href": "posts/hard_problem_privacy/index.html",
    "title": "The hard problem of privacy",
    "section": "",
    "text": "Of course, it depends on what is understood by privacy, or conversely, by surveillance. Last summer, Rachel Thomas, widely known as co-founder of fast.ai and founding director of the USF Center for Applied Data Ethics, wrote about 8 Things You Need to Know about Surveillance. Everything she writes is highly important and deserves being thought about every single day. However, in a well-meaning community there should be little controversy about how justified these points are. My starting point here is different, and it, like the whole of this post, is highly subjective.\n\nSoul mates\nRecently, I listened to an episode of Lex Fridman’s AI Podcast. Depending on one’s interest, this podcast has the most interesting guest list one could imagine: Donald Knuth, Daniel Kahneman, Gilbert Strang, Leonard Susskind, Yoshua Bengio … to name just a few. But the podcast is nice to listen to not just because of the guest list. The interviewer himself impresses by his modest, warm and deeply human ways.\nOn that backgound, I was immensely surprised when in the episode with Cristos Goodrow, VP of Engineering at Google and head of Search and Discovery at YouTube, Fridman not just highly extolled Youtube’s recommendation algorithm, but also expressed a “dream” that Youtube, besides similar-as-per-the-recommendation-algorithm videos, also suggested similar-as-per-the-algorithm users (min. 30/31):\n\nI know I’m gonna ask for things that are impossible but I would love to cluster human beings, I would love to know who has similar trajectories as me …” .\n\nThe algorithm as a helper to find friends, or people like oneself, one would never get to meet otherwise. The most human dream ever, finding soul mates, powered by Youtube.\nIncapable to get over my surprise, I started thinking whether I should write this post.\n\n\nWhy aren’t we more scared?\nIt’s not the most fruitful question, but helpless inability to understand probably starts like this, asking: Why aren’t we more scared?\nFor starters, we’re used to getting stuff recommended all the time. On Amazon, I may have bought some books that were purchased by similar users. (On the other hand, the overall quality of recommendations is not like I would start to feel threatened by the magic.) So, advantages jump to the eye, while disadvantages are not immediately visible.\nSecondly, traditionally, in many cultures, we are accustomed to associate surveillance with the government, not private companies, just simply because historically, it was in fact the government, and the government only, that had the power to surveil and oppress. This is the classic picture of totalitarianism.\nHowever, as described by Shoshana Zuboff in her lengthy-ish, not-without-redundancies, but immensely important book The age of surveillance capitalism, the modern reality – at least in the US and Europe – is less direct and less easy to see through. (Evidently, the story is different in a country like China, where a social credit system has already been established.)\n\n\nSurveillance capitalism in a nutshell\nZuboff’s book recounts, in great detail and citing a lot of sources public and private, how we ended up living in a world where each of our online actions is tracked by numerous “interested parties”, where our phones send who-knows-what to whichever distant beneficiary, and where more and more of our “real-world”, physical activity is recorded by agents we may not even know of.\nIn short, the mechanism she describes is this: Companies big and small, the so-called “surveillance capitalists”, have their business based on gathering, extracting, and processing information – more than is needed to improve a product or a service. Instead, this “behavioral surplus” – in processed form – is sold to customers, which may be everything from other businesses over banks and insurance companies to intelligence agencies and governments.\nWith such a business model, of course a company is interested in optimizing its ML algorithms. The better the algorithms, the better the results that are sold to customers, and the better those results, the more that company is willing to pay. Online ads may look like a triviality, but they are in fact a great illustration of the principle: If our customer pays per click, and our business is “full-service” – “don’t worry about anything, we’ll place your ad for you”, then it’s in my own best interest to show ads to the most susceptible users, in the most favorable circumstances. Of course we’ll get creative, thinking about how we can best determine those users and moments.\nAs already hinted at above, with private companies in possession of data and inferences not publicly available, governments, intelligence services, and the military re-enter the picture. Zuboff describes how intelligence agencies craved information that was hard to obtain legally, but could be provided by companies operating in a lawless zone, always ahead of legislation. 9/11 was just a door opener; from then on, massive interlacing ensued between politics and companies in the information business, as seen for example, but not only, in the 2012 and 2016 US elections.\nZuboff’s book is full of interesting details, but this post is not meant to be a “best of”. Rather, I’m wondering how the extent, as well as the importance of, what’s going on could be made more salient to ML people – not just to those to whom doubt and skepticism come naturally, but to well-meaning idealists as well.\n\n\nWhy we should care\nTrying to make this more salient, I can think of three things:\n\nConcisely stating a few facts that, important though they are in Zuboff’s argumentation, deserve being pointed out even outside the concrete contexts in which they appear in the book;\nCalling attention, in a few lines instead of a time-consuming narrative, to what has already happened, in the “real world”, in the very short time span since those new dynamics took off; and (the most impressive to me, personally)\nRelating, by citation, the actual “utopias” behind the reality.\n\n\n\n(1) A few things to consider\nA common way to think about this is: We’re using a free service (like a search engine), we have to pay somehow, so we’re paying with our data. But – data is collected and processed by products we buy, as well, be it phones, wearables or household appliances.\nAlso, it is not like we get to decide whether and if so, how much privacy we’re willing to give up in exchange for a service. As often noted, no-one has time to read through all the “privacy policies” we’re bombarded with; and sometimes we’re just told “you can disable this; but then feature x [[[some vital feature, probably the one we bought it for]]] can’t be guaranteed to be working anymore”.\n\n\n(2) Not just online\nEven to people who spend most of their day online, being tracked on the web may still seem like a minor inconvenience. Of course, through our smartphones, we carry the web with us. Evidently, depending on what apps one has installed, one may receive friendly “invitations” to, say, pay a visit to McDonald’s just as one passes there on one’s lunchtime run. (Or maybe not, depending on whether the type of ongoing activity is already being taken into account.)\nSure, such notifications disrupt the flow of whatever one’s doing. But still: For some of us, it’s easy to think that who cares? I don’t have those kinds of apps installed anyway. Put more generally: Yes, all this online tracking is annoying. But as long as all that happens is me seeing more “targeted” ads, why bother?\nThe recent years have seen a consistent and speedy extension of tracking in the real world.\nThere was Google Street View, filming the outdoors, a space once thought of as ephemeral, and subsequent attempts, by various actors, to map public interiors.\nThere was Google glass, allowing to video- and audiotape people without their knowledge.\nThen, there are all kinds of wearables, also able to record not just the wearer’s, but bystanders’ data. The wearers themselves must be aware that some data is collected (why else buy the thing?), but don’t necessarily know how it’s being made use of, and what other data may be collected.\nThen, there are all kinds of IoT devices: TVs, vacuum cleaners, thermostats… Again, they are bought for a reason; but no-one can expect a TV to record what you’re saying, or a vacuum cleaner to transmit back your floor plan.\nThen – and this makes for a perfect transition to the last section, “utopias” – there is smart city: not a utopia, but actual reality, already being implemented in Toronto. Here is an excerpt from an interview with CEO Dan Doctoroff:\n\nIn effect, what we’re doing is replicating the digital experience in physical space … So ubiquitous connectivity; incredible computing power including artificial intelligence and machine learning; the ability to display data; sensing, including cameras and location data as well as other specialized sensors… We fund it all… through a very novel advertising model… We can actually then target ads to people in proximity, and then obviously over time track them through things like beacons and location services as well as their browsing activity.\n\nFrom these publicly made statements, it is hard to doubt what The Globe and Mail reported re. a leaked document, the yellow book, containing detailed outlines about how this system would be implemented:\n\nEarly on, the company notes that a Sidewalk neighbourhood would collect real-time position data “for all entities” – including people. The company would also collect a “historical record of where things have been” and “about where they are going.” Furthermore, unique data identifiers would be generated for “every person, business or object registered in the district,” helping devices communicate with each other.\n\nThe newspaper continues,\n\nThe document also describes reputation tools that would lead to a “new currency for community co-operation,” effectively establishing a social credit system. Sidewalk could use these tools to “hold people or businesses accountable” while rewarding good behaviour, such as by rewarding a business’s good customer service with an easier or cheaper renewal process on its licence.\n\nIf, to people in “Western cultures”, China, with its open endorsement and implementation of a social credit system, tended to seem “far, far away” – now nothing seems as it did before.\nAt this point, it’s time to switch gears and look at the world view(s) behind those recent developments.\n\n\n(3) Utopias\nHere is part of the vision of Mark Zuckerberg. Zuboff, aggregating information from several Zuckerberg-quoting, publicly available sources, writes (p.402)\n\nPredictive models would enable the corporation to “tell you to what bar to go to” when you arrive in a strange city. The vision is detailed: when you arrive at the bar, the bartender has your favorite drink waiting, and you’re able to look around the room and identify people just like you.\n\nExtending this to life overall, here we have a vision of total boredom; no exploration, no curiosity, no surprise, no learning. How probable is it that evolutionarily, humans are prepared to live in a world without uncertainty?\nHow about regulation? Here is Larry Page on laws and experimentation:\n\nIf you look at the different kinds of laws we make, they’re very old. The laws when we went public we’re 50 years old. A law can’t be right if it’s 50 years old, like it’s before the Internet, that’s a pretty major change, in how you may go public.[…]\n\n\nThe other thing in my mind is we also haven’t built mechanisms to allow experimentation. There’s many, many exciting and important things you could do that you just can’t do because they’re illegal, or they’re not allowed by regulation, and that makes sense, we don’t want the world to change too fast. Maybe we should set aside a small part of the world …[…]\n\n\nI think as technologists we should have some safe places where we can try out some new things and figure out what is the effect on society, what’s the effect on people, without having to deploy kind of into the normal world. […]\n\nOr Zuckerberg again, on the ancient social norm of privacy:\n\n“People have really gotten comfortable not only sharing more information and different kinds, but more openly and with more people,” he said. “That social norm is just something that has evolved over time.”\n\nIn this light, is a social credit system – perhaps packaged differently – really so far away, in Western cultures? Sometimes, a scenario sounds so silly that it’s hard to see it as a real threat; but that may be just us being naive. Here is another vision, from Rana el Kaliouby, CEO at Affectiva:\n\nWe have been in conversations with a company in that space. It is an advertising-rewards company, and its business is based on positive moments. So if you set a goal to run three miles and you run three miles, that’s a moment. Or if you set the alarm for six o’clock and you actually do get up, that’s a moment. And they monetize these moments. They sell them. Like Kleenex can send you a coupon – I don’t know – when you get over a sad moment. Right now, this company is making assumptions about what those moments are. And we’re like, ‘Guess what? We can capture them.’\n\nIf that sounds like it needn’t be taken seriously, here is a quote from Social Physics, a highly acclaimed opus by Alex Pentland, director of MIT Media Lab:\n\nThe social physics approach to getting everyone to cooperate is to use social network incentives rather than to use individual market incentives or to provide additional information. That is, we focus on changing the connections between people rather than focussing on getting people individually to change their behavior. The logic here is clear: Since exchanges between people are of enormous value to the participants, we can leverage those exchanges to generate social pressure for change.[…]\n\n\nSocial network incentives act by generating social pressure around the problem of finding cooperative behaviors, and so people experiment with new behaviors to find ones that are better.\n\nAt this point, I feel like nothing has to be added to those visions. But let’s get back to the beginning, Lex Fridman’s fantasy of clustering users.\n\n\nExpressing oneself\nZuboff quotes Zuckerberg saying, “humans have such a deep need to express themselves” (p. 403). Evidently, this is what motivates Fridman’s wish, and yes, it’s a, or the, profoundly human thing.\nBut, doesn’t expressing oneself, if seen as sharing, involve a free decision when, what, and with whom to share?\nAnd what does it do to an experience if it is not just “there”, a thing valuable in itself, but a means for others, to control and profit?\nThanks for reading!"
  },
  {
    "objectID": "posts/ai_ethics_optimization_problem/index.html",
    "href": "posts/ai_ethics_optimization_problem/index.html",
    "title": "AI ethics is not an optimization problem",
    "section": "",
    "text": "When you work in a field as intellectually-satisfying, challenging and inspiring as software design for machine learning, it is easy to focus on the technical, keeping out of sight the broader context. Some would even say it is required. How else can you keep up the necessary level of concentration?\nBut even for someone who hasn’t been in the field that long, it is evident that with every year that passes, with deep-learning-based technologies1 progressing faster and faster over ever-shorter time spans, misuse of these technologies has increased as well, not just in selected countries but all over the world.\nTo eliminate one possible misunderstanding right from the start: When I’m talking about “faster and faster” progress, I’m not over-hyping things. I’m far from thinking that AI is close to “solving” problems like language understanding, concept learning and their likes — the kind of problems some would argue hybrid models were needed for. The thing is that it doesn’t matter. It is exactly the kinds of things AI does do so well that lend themselves to misuse. It is people we should be afraid of, not machines2.\nBack to the why. Over time, it increasingly appeared to me that writing regularly about AI in a technical way, but not ever writing about its misuse, was ethically questionable in itself. However, I also became increasingly conscious of the fact that with a topic like this, once you enter the political realm — and that we have to is the main point of this text –, likelihood rises that people will disagree, or worse, feel offended for reasons not anticipated by the writer. Some will find this too radical, some not radical (explicit) enough.\nBut when the alternative is to stay silent, it seems better to try and do one’s best.\nLet’s start with two terms whose recent rise in popularity matches that of AI as a whole: bias and fairness."
  },
  {
    "objectID": "posts/solms_consciousness/index.html",
    "href": "posts/solms_consciousness/index.html",
    "title": "A book I’d say everyone should read if such were a kind of thing I’d say",
    "section": "",
    "text": "A few years ago, I found myself wanting to learn and, maybe, try to make up my mind, about a topic I – surprisingly – never had given much thought to, before: consciousness.\nHowever, having read around some (first and foremost, Blackmore & Troscianko’s Consciousness: An Introduction, which I liked a lot), I certainly had not made up my mind. All I knew was that, certainly, I would not want to be a dualist; that the dominant neuroscience-guided theories felt simplistic; that I wasn’t convinced quantum physics were all we need; and finally, that Daniel Dennett’s multiple drafts formulation seemed most in concordance with my views. Otherwise though, there seemed to be a Rashomon-like touch to the topic: We’d be confronted with a set of parallel narratives – just that, here, it wasn’t too clear they were even trying to tell the same story.\nIn most cases, though, at least I got an impression of the kind of story they wanted to tell. With one exception. Before I came across Blackmore & Troscianko, I did the obvious thing: look up “consciousness” in the Stanford Encyclopedia of Philosophy. And there they were: the mysterious qualia. (Very roughly, the “what it is like”s of perceiving something.1) Now perception, to me, is of high interest; and it is something we know a lot about, on various levels. But that question: What is it like to see “red”? What is it like to see? – What does it matter? The whole topic of qualia seemed utterly unimportant.\nBut that is not something you say out loud, is it? When centuries of philosophical tradition speak otherwise."
  },
  {
    "objectID": "posts/graphdl/index.html",
    "href": "posts/graphdl/index.html",
    "title": "Getting started with deep learning on graphs",
    "section": "",
    "text": "If, in deep-learning world, the first half of the last decade has been the age of images, and the second, that of language, one could say that now, we’re living in the age of graphs. At least, that’s what commonly cited research metrics suggest. But as we’re all aware, deep-learning research is anything but an ivory tower. To see real-world implications, it suffices to reflect on how many things can be modeled as graphs. Some things quite naturally “are” graphs, in the sense of having nodes and edges: neurons, underground stations, social networks. Other things can fruitfully be modeled as graphs: molecules, for example; or language, concepts, three-dimensional shapes … If deep learning on graphs is desirable, what are the challenges, and what do we get for free?"
  },
  {
    "objectID": "posts/graphdl/index.html#whats-so-special-about-deep-learning-on-graphs",
    "href": "posts/graphdl/index.html#whats-so-special-about-deep-learning-on-graphs",
    "title": "Getting started with deep learning on graphs",
    "section": "What’s so special about deep learning on graphs?",
    "text": "What’s so special about deep learning on graphs?\nGraphs are different from images, language, as well as tabular data in that node numbering does not matter. In other words, graphs are permutation-invariant. Already this means that architectures established in other domains cannot be transferred verbatim. (The ideas underlying them can be transferred though. Thus, in the graph neural network (henceforth: GNN) model zoo you’ll see lots of allusions to “convolutional”, “attention”, and other established terms.) Put very simply, and in concordance with common sense, whatever algorithm is used, it will fundamentally be based on how nodes are connected: the edges, that is.\nWhen relationships are modeled as graphs, both nodes and edges can have features. This, too, adds complexity. But not everything is harder with graphs. Think of how cumbersome it can be to obtain labeled data for supervised learning. With graphs, often an astonishingly small amount of labeled data is needed. More surprisingly still, a graph can be constructed when not a single edge is present. Put differently, learning on sets can morph into learning on graphs.\nAt this point, let me switch gears and move on to the practical part: the raison d’être of this post."
  },
  {
    "objectID": "posts/graphdl/index.html#matching-concepts-and-code-pytorch-geometric",
    "href": "posts/graphdl/index.html#matching-concepts-and-code-pytorch-geometric",
    "title": "Getting started with deep learning on graphs",
    "section": "Matching concepts and code: PyTorch Geometric",
    "text": "Matching concepts and code: PyTorch Geometric\nIn this (and future) posts, we’ll make use of PyTorch Geometric (from hereon: PyG), the most popular, at this time, and fastest-growing in terms of functionality as well as user base, library dedicated to graph DL.\nDeep learning on graphs, in its most general form, is usually characterized by the term message passing. Messages are passed between nodes that are linked by an edge: If node \\(A\\) has three neighbors, it will receive three messages. Those messages have to be summarized in some meaningful way. Finally – GNNs consisting of consecutive layers – the node will have to decide how to modify its previous-layer features (a.k.a. embeddings) based on that summary.\nTogether, these make up a three-step sequence: collect messages; aggegate; update. What about the “learning” in deep learning, though? There are two places where learning can happen: Firstly, in message collection: Incoming messages could be transformed by a MLP, for example. Secondly, as part of the update step. All in all, this yields mathematical formulae like this, given in the PyG documentation:\n\\[\n\\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\right)\n\\]\nScary though this looks, once we read it from the right, we see that it nicely fits the conceptual description. The \\((\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i})\\) are the three types of incoming messages a node can receive: its own state at the previous layer, the states of its neighbors (the nodes \\(j \\in \\mathcal{N}(i)\\)) at the previous layer, and features/embeddings associated to the edge in question. (I’m leaving out edge features in this discussion completely, so as to not further enhance complexity.) These messages are (optionally) transformed by the neural network \\(\\phi\\), and whatever comes out is summarized by the aggregator function \\(\\square\\). Finally, a node will update itself based on that summary as well as its own previous-layer state, possibly by means of applying neural network \\(\\gamma\\).\nNow that we have this conceptual/mathematical representation, how does it map to code we see, or would like to write? PyG has excellent, extensive documentation, including at the beginner level. But here, I’d like to spell things out in detail – pedantically, if you like, but in a way that tells us a lot about how GNNs work.\nLet’s start by the information given in one of the key documentation pages, Creating message passing networks:\n\nPyG provides the MessagePassing base class, which helps in creating such kinds of message passing graph neural networks by automatically taking care of message propagation. The user only has to define the functions \\(\\phi\\), i.e. message(), and \\(\\gamma\\), i.e. update(), as well as the aggregation scheme to use, i.e. aggr=\"add\", aggr=\"mean\" or aggr=\"max\".\n\nScrolling down that page and looking at the two example implementations, however, we see that an implementation of update() does not have to be provided; and from inspecting the source code, it is clear that, technically, the same holds for message(). (And unless we want a form of aggregation different from the default add, we do not even need to specify that, either.)\nThus, the question becomes: What happens if we code the minimal PyG GNN? To find out, we first need to create a minimal graph, one minimal enough for us to track what is going on.\n\nA minimal graph\nNow, a basic Data object is created from three tensors. The first holds the node features: two features each for five nodes. (Both features are identical on purpose, for “cognitive ease” – on our, not the algorithm’s, part.)\n\nimport torch\n\nx = torch.tensor([[1, 1], [2, 2], [3, 3], [11, 11], [12, 12]], dtype=torch.float)\n\nThe second specifies existing connections. For undirected graphs (like ours), each edge appears twice. The tensor you see here is specified in one-edge-per-line form for convenience reasons; to the Data() constructor we’ll pass its transpose instead.\n\nedge_index = torch.tensor([\n  [0, 1],\n  [1, 0],\n  [0, 2],\n  [2, 0],\n  [1, 2],\n  [2, 1],\n  [2, 3],\n  [3, 2],\n  [2, 4],\n  [4, 2],\n  [3, 4],\n  [4, 3]\n], dtype=torch.long)\n\nThe third tensor holds the node labels. (The task will be one of node – not edge, not graph – classification.)\n\ny = torch.tensor([[0], [0], [0], [1], [1]], dtype=torch.float)\n\nConstructing and inspecting the resulting graph, we have:\n\nfrom torch_geometric.data import Data\n\ndata = Data(x = x, edge_index = edge_index.t().contiguous(), y = y)\ndata.x\ndata.edge_index\ndata.y\n\ntensor([[ 1.,  1.],\n        [ 2.,  2.],\n        [ 3.,  3.],\n        [11., 11.],\n        [12., 12.]])\n        \ntensor([[0, 1, 0, 2, 1, 2, 2, 3, 2, 4, 3, 4],\n        [1, 0, 2, 0, 2, 1, 3, 2, 4, 2, 4, 3]])\n        \ntensor([[0.],\n        [0.],\n        [0.],\n        [1.],\n        [1.]])\nFor our upcoming experiments, it’s more helpful, though, to visualize the graph:\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom torch_geometric.utils import to_networkx\n\ndef visualize_graph(G, color, labels):\n    plt.figure(figsize=(7,7))\n    plt.axis('off')\n    nx.draw_networkx(\n      G,\n      pos = nx.spring_layout(G, seed = 777),\n      labels = labels,\n      node_color = color,\n      cmap = \"Set3\"\n      )\n    plt.show()\n\nG = to_networkx(data, to_undirected = True, node_attrs = [\"x\"])\nlabels = nx.get_node_attributes(G, \"x\")\nvisualize_graph(G, color = data.y, labels = labels)\n\n\nAlthough our experiments won’t be about training performance (how could they be, with just five nodes), let me remark in passing that this graph is small, but not boring: The middle node is equally connected to both “sides”, yet feature-wise, it would pretty clearly appear to belong on just one of them. (Which is true, given the provided class labels). Such a constellation is interesting because, in the majority of networks, edges indicate similarity.\n\n\nA minimal GNN\nNow, we code and run the minimal GNN. We’re not interested in class labels (yet); we just want to see each node’s embeddings after a single pass.\n\nfrom torch_geometric.nn import MessagePassing\n\nclass IAmLazy(MessagePassing):\n    def forward(self, x, edge_index):\n        out = self.propagate(edge_index, x = x)\n        return out\n      \nmodule = IAmLazy()\nout = module(data.x, data.edge_index)\nout\n\ntensor([[ 5.,  5.],\n        [ 4.,  4.],\n        [26., 26.],\n        [15., 15.],\n        [14., 14.]])\nEvidently, we just had to start the process – but what process, exactly? From what we know about the three stages of message passing, an essential question is what nodes do with the information that flows over the edges. Our first experiment, then, is to inspect the incoming messages.\n\n\nPoking into message()\nIn message(), we have access to a structure named x_j. This tensor holds, for each node \\(i\\), the embeddings of all nodes \\(j\\) connected to it via incoming edges. We’ll print them, and then, just return them, unchanged.\n\nclass IAmMyOthers(MessagePassing):\n    def forward(self, x, edge_index):\n        out = self.propagate(edge_index, x = x)\n        return out\n    def message(self, x_j):\n        print(\"in message, x_j is\")\n        print(x_j)\n        return x_j\n      \nmodule = IAmMyOthers()\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n\nin message, x_j is\ntensor([[ 1.,  1.],\n        [ 2.,  2.],\n        [ 1.,  1.],\n        [ 3.,  3.],\n        [ 2.,  2.],\n        [ 3.,  3.],\n        [ 3.,  3.],\n        [11., 11.],\n        [ 3.,  3.],\n        [12., 12.],\n        [11., 11.],\n        [12., 12.]])\n        \nresult is:\ntensor([[ 5.,  5.],\n        [ 4.,  4.],\n        [26., 26.],\n        [15., 15.],\n        [14., 14.]])\nLet me spell this out. In data.edge_index, repeated here for convenience:\ntensor([[0, 1, 0, 2, 1, 2, 2, 3, 2, 4, 3, 4],\n        [1, 0, 2, 0, 2, 1, 3, 2, 4, 2, 4, 3]])\nthe first pair denotes the edge from node 0 (that had features (1, 1)) to node 1. This information is found in x_j’s first row. Then the second row holds the information flowing in the opposite direction, namely, the features associated with node 1. And so on.\nInterestingly, since we’re passing through this module just once, we can see the messages that will be sent without even running it.\nNamely, since data.edge_index[0] designates the source nodes for each edge:\n\ndata.edge_index[0]\n\nwe can index into data.x to pick up what will be the incoming features for each connection.\n\ndata.x[data.edge_index[0]]\n\ntensor([[ 1.,  1.],\n        [ 2.,  2.],\n        [ 1.,  1.],\n        [ 3.,  3.],\n        [ 2.,  2.],\n        [ 3.,  3.],\n        [ 3.,  3.],\n        [11., 11.],\n        [ 3.,  3.],\n        [12., 12.],\n        [11., 11.],\n        [12., 12.]])\nNow, what does this tell us? Node 0, for example, received messages from nodes 1 and 2: (2, 2) and (3, 3), respectively. We know that the default aggregation mode is add; and so, would expect an outcome of (5, 5). Indeed, this is the new embedding for node 0.\nIn a nutshell, thus, the minimal GNN updates every node’s embedding so as to prototypically reflect the node’s neighborhood. Take care though: Nodes represent their neighborhoods, but themselves, they count for nothing. We will change that now.\n\n\nAdding self loops\nAll we need to do is modify the adjacency matrix to include edges going from each node back to itself.\n\nfrom torch_geometric.utils import add_self_loops\n\nclass IAmMyOthersAndMyselfAsWell(MessagePassing):\n    def forward(self, x, edge_index):\n        edge_index, _ = add_self_loops(edge_index, num_nodes = x.size(0))\n        print(\"in forward, augmented edge index now has shape\")\n        print(edge_index.shape)\n        out = self.propagate(edge_index, x = x)\n        return out\n    def message(self, x_j):\n        return x_j\n\nmodule = IAmMyOthersAndMyselfAsWell()\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n\nin forward, augmented edge index now has shape:\ntorch.Size([2, 17])\n\nresult is:\ntensor([[ 6.,  6.],\n        [ 6.,  6.],\n        [29., 29.],\n        [26., 26.],\n        [26., 26.]])\nAs expected, the neighborhood summary at each node now includes a contribution from each node itself.\nNow we know how to access the messages, we’d like to aggregate them in a non-standard way.\n\n\nCustomizing aggregate()\nInstead of message(), we now override aggregate(). If we wanted to use another of the “standard” aggregation modes (mean, mul, min, or max), we could just override __init__(), like so:\n\ndef __init__(self):\n        super().__init__(aggr = \"mean\")\n\nTo implement custom summaries, however, we make use of torch_scatter (one of PyG’s installation prerequisites) for optimal performance. Let me show this by means of a simple example.\n\nfrom torch_scatter import scatter\n\nclass IAmJustTheOppositeReally(MessagePassing):\n    def forward(self, x, edge_index):\n        out = self.propagate(edge_index, x = x)\n        return out\n    def aggregate(self, inputs, index):\n        print(\"in aggregate, inputs is\")\n        # same as x_j (incoming node features)\n        print(inputs)\n        print(\"in aggregate, index is\")\n        # this is data.edge_index[1]\n        print(index)\n        # see https://pytorch-scatter.readthedocs.io/en/1.3.0/index.html\n        # for other aggregation modes\n        # default dim is -1\n        return - scatter(inputs, index, dim = 0, reduce = \"add\") \n      \nmodule = IAmJustTheOppositeReally()\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n\nin aggregate, inputs is\ntensor([[ 1.,  1.],\n        [ 2.,  2.],\n        [ 1.,  1.],\n        [ 3.,  3.],\n        [ 2.,  2.],\n        [ 3.,  3.],\n        [ 3.,  3.],\n        [11., 11.],\n        [ 3.,  3.],\n        [12., 12.],\n        [11., 11.],\n        [12., 12.]])\n        \nin aggregate, index is\ntensor([1, 0, 2, 0, 2, 1, 3, 2, 4, 2, 4, 3])\n\nresult is:\ntensor([[ -5.,  -5.],\n        [ -4.,  -4.],\n        [-26., -26.],\n        [-15., -15.],\n        [-14., -14.]])\nIn aggregate(), we have two types of tensors to work with. One, inputs, holds what was returned from message(). In our case, this is identical to x_j, since we didn’t make any modifications to the default behavior. The second, index, holds the recipe for where in the aggregation those features should go. Here, the very first tuple, (1, 1), will contribute to the summary for node 1; the second, (2, 2), to that for node 0 – and so on. By the way, just like x_j (in a single-layer, single-pass setup) is “just” data.x[data.edge_index[0]], that index is “just” data.edge_index[1]. Meaning, this is the list of target nodes connected to the edges in question.\nAt this point, all kinds of manipulations could be done on either inputs or index; however, we content ourselves with just passing them through to torch_scatter.scatter(), and returning the negated sums. We’ve successfully built a network of contrarians.\nBy now, we’ve played with message() as well as aggregate(). What about update()?\n\n\nAdding memory to update()\nThere’s one thing really strange in what we’re doing. It doesn’t jump to the eye, since we’re not simulating a real training phase; we’ve been calling the layer just once. If we hadn’t, we’d have noticed that at every call, the nodes happily forget who they were before, dutifully assuming the new identities assigned. In reality, we probably want them to evolve in a more consistent way.\nFor example:\n\nclass IDoEvolveOverTime(MessagePassing):\n    def forward(self, x, edge_index):\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n        out = self.propagate(edge_index, x = x)\n        return out\n    def update(self, inputs, x):\n        print(\"in update, inputs is\")\n        print(inputs)\n        print(\"in update, x is\")\n        print(x)\n        return (inputs + x)/2\n\nmodule = IDoEvolveOverTime()\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n\nin update, inputs is\ntensor([[ 6.,  6.],\n        [ 6.,  6.],\n        [29., 29.],\n        [26., 26.],\n        [26., 26.]])\nin update, x is\ntensor([[ 1.,  1.],\n        [ 2.,  2.],\n        [ 3.,  3.],\n        [11., 11.],\n        [12., 12.]])\nresult is:\ntensor([[ 3.5000,  3.5000],\n        [ 4.0000,  4.0000],\n        [16.0000, 16.0000],\n        [18.5000, 18.5000],\n        [19.0000, 19.0000]])\nIn update(), we have access to both the final message aggregate (inputs) and the nodes’ prior states (x). Here, I’m just averaging those two.\nAt this point, we’ve successfully acquainted ourselves with the three stages of message passing: acting on individual messages, aggregating them, and self-updating based on past state and new information. But none of our models so far could be called a neural network, since there was no learning involved.\n\n\nAdding parameters\nIf we look back at the generic message passing formulation:\n\\[\n\\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\right)\n\\] we see two places where neural network modules can act on the computation: before message aggregation, and as part of the node update process. First, we illustrate the former option. For example, we can apply a MLP in forward(), before the call to aggregate():\n\nfrom torch.nn import Sequential as Seq, Linear, ReLU\n\nclass ILearnAndEvolve(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(aggr = \"sum\")\n        self.mlp = Seq(Linear(in_channels, out_channels),\n                       ReLU(),\n                       Linear(out_channels, out_channels))\n    def forward(self, x, edge_index):\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n        x = self.mlp(x)\n        out = self.propagate(edge_index = edge_index, x = x)\n        return out\n    def update(self, inputs, x):\n        return (inputs + x)/2\n\nmodule = ILearnAndEvolve(2, 2)\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n\nresult is:\ntensor([[-0.8724, -0.4407],\n        [-0.9056, -0.4623],\n        [-2.0229, -1.1240],\n        [-1.8691, -1.0867],\n        [-1.9024, -1.1082]], grad_fn=&lt;DivBackward0&gt;)\nFinally, we can apply network modules in both places, as exemplified next.\n\n\nGeneral message passing\nWe keep the MLP from the previous class, and add a second in update():\n\nclass ILearnAndEvolveDoubly(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(aggr = \"sum\")\n        self.mlp_msg = Seq(Linear(in_channels, out_channels),\n                       ReLU(),\n                       Linear(out_channels, out_channels))\n        self.mlp_upd = Seq(Linear(out_channels, out_channels),\n                       ReLU(),\n                       Linear(out_channels, out_channels))\n    def forward(self, x, edge_index):\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n        x = self.mlp_msg(x)\n        out = self.propagate(edge_index = edge_index, x = x)\n        return out\n    def update(self, inputs, x):\n        return self.mlp_upd((inputs + x)/2)\n\nmodule = ILearnAndEvolveDoubly(2, 2)\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n\nresult is:\ntensor([[ 0.0573, -0.6988],\n        [ 0.0358, -0.6894],\n        [-0.1730, -0.6450],\n        [-0.5855, -0.4171],\n        [-0.5890, -0.4141]], grad_fn=&lt;AddmmBackward0&gt;)\nAt this point, I hope you’ll feel comfortable to play around, subclassing the MessagePassing base class. Also, if now you consult the above-mentioned documentation page (Creating message passing networks), you’ll be able to map the example implementations (dedicated to popular GNN layer types) to where they “hook into” the message passing process.\nExperimentation with MessagePassing was the point of this post. However, you may be wondering: How do I actually use this for node classification? Didn’t the graph have a class defined for each node? (It did: data.y.)\nSo let me conclude with a (minimal) end-to-end example that uses one of the above modules.\n\n\nA minimal workflow\nTo that purpose, we compose that module with a linear one that performs node classification:\n\nclass Network(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, num_classes):\n        super().__init__()\n        self.conv = ILearnAndEvolveDoubly(in_channels, out_channels)\n        self.classifier = Linear(out_channels, num_classes)\n    def forward(self, x, edge_index):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv(x, edge_index)\n        return self.classifier(x)\n\nmodel = Network(2, 2, 1) \n\nWe can then train the model like any other:\n\nimport torch.nn.functional as F\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\nmodel.train()\n\nfor epoch in range(5):\n    optimizer.zero_grad()\n    out = model(data.x, data.edge_index)\n    loss = F.binary_cross_entropy_with_logits(out, data.y)\n    loss.backward()\n    optimizer.step()\n\npreds = torch.sigmoid(out)\npreds\n\ntensor([[0.6502],\n        [0.6532],\n        [0.7027],\n        [0.7145],\n        [0.7165]], grad_fn=&lt;SigmoidBackward0&gt;)\nAnd that’s it for this time. Stay tuned for examples of how graph models are applied in the sciences, as well as illustrations of bleeding-edge developments in Geometric Deep Learning, the principles-based, heuristics-transcending approach to neural networks.\nThanks for reading!\nPhoto by Alina Grubnyak on Unsplash"
  },
  {
    "objectID": "posts/ai-fairness/index.html",
    "href": "posts/ai-fairness/index.html",
    "title": "Starting to think about AI Fairness",
    "section": "",
    "text": "If you use deep learning for unsupervised part-of-speech tagging of Sanskrit 1, or knowledge discovery in physics 2, you probably don’t need to worry about model fairness. If you’re a data scientist working at a place where decisions are made about people, however, or an academic researching models that will be used to such ends, chances are that you’ve already been thinking about this topic. — Or feeling that you should. And thinking about this is hard.\nIt is hard for several reasons. In this text, I will go into just one."
  },
  {
    "objectID": "posts/ai-fairness/index.html#the-forest-for-the-trees",
    "href": "posts/ai-fairness/index.html#the-forest-for-the-trees",
    "title": "Starting to think about AI Fairness",
    "section": "The forest for the trees",
    "text": "The forest for the trees\nNowadays, it is hard to find a modeling framework that does not include functionality to assess fairness. (Or is at least planning to.) And the terminology sounds so familiar, as well: “calibration”, “predictive parity”, “equal true [false] positive rate”… It almost seems as though we could just take the metrics we make use of anyway (recall or precision, say), test for equality across groups, and that’s it. Let’s assume, for a second, it really was that simple. Then the question still is: Which metrics, exactly, do we choose?\nIn reality things are not simple. And it gets worse. For very good reasons, there is a close connection in the ML fairness literature to concepts that are primarily treated in other disciplines, such as the legal sciences: discrimination and disparate impact (both not being far from yet another statistical concept, statistical parity). Statistical parity means that if we have a classifier, say to decide whom to hire, it should result in as many applicants from the disadvantaged group (e.g., Black people) being hired as from the advantaged one(s). But that is quite a different requirement from, say, equal true/false positive rates!\nSo despite all that abundance of software, guides, and decision trees, even: This is not a simple, technical decision. It is, in fact, a technical decision only to a small degree."
  },
  {
    "objectID": "posts/ai-fairness/index.html#common-sense-not-math",
    "href": "posts/ai-fairness/index.html#common-sense-not-math",
    "title": "Starting to think about AI Fairness",
    "section": "Common sense, not math",
    "text": "Common sense, not math\nLet me start this section with a disclaimer: Most of the sources referenced in this text appear, or are implied on the “Guidance” page of IBM’s framework AI Fairness 360. If you read that page, and everything that’s said and not said there appears clear from the outset, then you may not need this more verbose exposition. If not, I invite you to read on.\nPapers on fairness in machine learning, as is common in fields like computer science, abound with formulae. Even the papers referenced here, though selected not for their theorems and proofs but for the ideas they harbor, are no exception. But to start thinking about fairness as it might apply to an ML process at hand, common language – and common sense – will do just fine. If, after analyzing your use case, you judge that the more technical results are relevant to the process in question, you will find that their verbal characterizations will often suffice. It is only when you doubt their correctness that you will need to work through the proofs.\nAt this point, you may be wondering what it is I am contrasting those “more technical results” with. This is the topic of the next section, where I’ll try to give a birds-eye characterization of fairness criteria and what they imply."
  },
  {
    "objectID": "posts/ai-fairness/index.html#situating-fairness-criteria",
    "href": "posts/ai-fairness/index.html#situating-fairness-criteria",
    "title": "Starting to think about AI Fairness",
    "section": "Situating fairness criteria",
    "text": "Situating fairness criteria\nThink back to the example of a hiring algorithm. What does it mean for this algorithm to be fair? We approach this question under two – incompatible, mostly – assumptions:\n\nThe algorithm is fair if it behaves the same way independent of which demographic group it is applied to. Here demographic group could be defined by ethnicity, gender, abledness, or in fact any categorization suggested by the context.\nThe algorithm is fair if it does not discriminate against any demographic group.\n\nI’ll call these the technical and societal views, respectively.\n\nFairness, viewed the technical way\nWhat does it mean for an algorithm to “behave the same way” regardless of which group it is applied to?\nIn a classification setting, we can view the relationship between prediction (\\(\\hat{Y}\\)) and target (\\(Y\\)) as a doubly directed path. In one direction: Given true target \\(Y\\), how accurate is prediction \\(\\hat{Y}\\)? In the other: Given \\(\\hat{Y}\\), how well does it predict the true class \\(Y\\)?\nBased on the direction they operate in, metrics popular in machine learning overall can be split into two categories. In the first, starting from the true target, we have recall, together with “the rates”: true positive, true negative, false positive, false negative. In the second, we have precision, together with positive (negative, resp.) predictive value.\nIf now we demand that these metrics be the same across groups, we arrive at corresponding fairness criteria: equal false positive rate, equal positive predictive value, etc. In the inter-group setting, the two types of metrics may be arranged under headings “equality of opportunity” and “predictive parity”. You’ll encounter these as actual headers in the summary table at the end of this text. (Said table organizes concepts from different areas into a three-category format. The overall narrative builds up towards that “map” in a bottom-up way – meaning, most entries will not make sense at this point.}\nWhile overall, the terminology around metrics can be confusing (to me it is), these headings have some mnemonic value. Equality of opportunity suggests that people similar in real life (\\(Y\\)) get classified similarly (\\(\\hat{Y}\\)). Predictive parity suggests that people classified similarly (\\(\\hat{Y}\\)) are, in fact, similar (\\(Y\\)).\nThe two criteria can concisely be characterized using the language of statistical independence. Following Barocas, Hardt, and Narayanan (2019), these are:\n\nSeparation: Given true target \\(Y\\), prediction \\(\\hat{Y}\\) is independent of group membership (\\(\\hat{Y} \\perp A | Y\\)).\nSufficiency: Given prediction \\(\\hat{Y}\\), target \\(Y\\) is independent of group membership (\\(Y \\perp A | \\hat{Y}\\)).\n\nGiven those two fairness criteria – and two sets of corresponding metrics – the natural question arises: Can we satisfy both? Above, I was mentioning precision and recall on purpose: to maybe “prime” you to think in the direction of “precision-recall trade-off”. And really, these two categories reflect different preferences; usually, it is impossible to optimize for both. The most famous, probably, result is due to Chouldechova (2016) : It says that predictive parity (testing for sufficiency) is incompatible with error rate balance (separation) when prevalence differs across groups. This is a theorem (yes, we’re in the realm of theorems and proofs here) that may not be surprising, in light of Bayes’ theorem, but is of great practical importance nonetheless: Unequal prevalence usually is the norm, not the exception.\nThis necessarily means we have to make a choice. And this is where the theorems and proofs do matter. For example, Yeom and Tschantz (2018) show that in this framework – the strictly technical approach to fairness – separation should be preferred over sufficiency, because the latter allows for arbitrary disparity amplification. Thus, in this framework, we may have to work through the theorems.\nWhat is the alternative?\n\n\nFairness, viewed as a social construct\nStarting with what I just wrote: No one will likely challenge fairness being a social construct. But what does that entail?\nLet me start with a biographical reminiscence. In undergraduate psychology (a long time ago), probably the most hammered-in distinction relevant to experiment planning was that between a hypothesis and its operationalization. The hypothesis is what you want to substantiate, conceptually; the operationalization is what you measure. There necessarily can’t be a one-to-one correspondence; we’re just striving to implement the best operationalization possible.\nIn the world of datasets and algorithms, all we have are measurements. And often, these are treated as though they were the concepts. This will get more concrete with an example, and we’ll stay with the hiring software scenario.\nAssume the dataset used for training, assembled from scoring previous employees, contains a set of predictors (among which, high-school grades) and a target variable, say an indicator whether an employee did “survive” probation. There is a concept-measurement mismatch on both sides.\nFor one, say the grades are intended to reflect ability to learn, and motivation to learn. But depending on the circumstances, there are influence factors of much higher impact: socioeconomic status, constantly having to struggle with prejudice, overt discrimination, and more.\nAnd then, the target variable. If the thing it’s supposed to measure is “was hired for seemed like a good fit, and was retained since was a good fit”, then all is good. But normally, HR departments are aiming for more than just a strategy of “keep doing what we’ve always been doing”.\nUnfortunately, that concept-measurement mismatch is even more fatal, and even less talked about, when it’s about the target and not the predictors. (Not accidentally, we also call the target the “ground truth”.) An infamous example is recidivism prediction, where what we really want to measure – whether someone did, in fact, commit a crime – is replaced, for measurability reasons, by whether they were convicted. These are not the same: Conviction depends on more then what someone has done – for instance, if they’ve been under intense scrutiny from the outset.\nFortunately, though, the mismatch is clearly pronounced in the AI fairness literature. Friedler, Scheidegger, and Venkatasubramanian (2016) distinguish between the construct and observed spaces; depending on whether a near-perfect mapping is assumed between these, they talk about two “worldviews”: “We’re all equal” (WAE) vs. “What you see is what you get” (WYSIWIG). If we’re all equal, membership in a societally disadvantaged group should not – in fact, may not – affect classification. In the hiring scenario, any algorithm employed thus has to result in the same proportion of applicants being hired, regardless of which demographic group they belong to. If “What you see is what you get”, we don’t question that the “ground truth” is the truth.\nThis talk of worldviews may seem unnecessary philosophical, but the authors go on and clarify: All that matters, in the end, is whether the data is seen as reflecting reality in a naïve, take-at-face-value way.\nFor example, we might be ready to concede that there could be small, albeit uninteresting effect-size-wise, statistical differences between men and women as to spatial vs. linguistic abilities, respectively. We know for sure, though, that there are much greater effects of socialization, starting in the core family and reinforced, progressively, as adolescents go through the education system. We therefore apply WAE, trying to (partly) compensate for historical injustice. This way, we’re effectively applying affirmative action, defined as\n\nA set of procedures designed to eliminate unlawful discrimination among applicants, remedy the results of such prior discrimination, and prevent such discrimination in the future.\n\nIn the already-mentioned summary table, you’ll find the WYSIWIG principle mapped to both equal opportunity and predictive parity metrics. WAE maps to the third category, one we haven’t dwelled upon yet: demographic parity, also known as statistical parity. In line with what was said before, the requirement here is for each group to be present in the positive-outcome class in proportion to its representation in the input sample. For example, if thirty percent of applicants are Black, then at least thirty percent of people selected should be Black, as well. A term commonly used for cases where this does not happen is disparate impact: The algorithm affects different groups in different ways.\nSimilar in spirit to demographic parity, but possibly leading to different outcomes in practice, is conditional demographic parity 3. Here we additionally take into account other predictors in the dataset; to be precise: all other predictors. The desiderate now is that for any choice of attributes, outcome proportions should be equal, given the protected attribute and the other attributes in question. I’ll come back to why this may sound better in theory than work in practice in the next section.\nSumming up, we’ve seen commonly used fairness metrics organized into three groups, two of which share a common assumption: that the data used for training can be taken at face value. The other starts from the outside, contemplating what historical events, and what political and societal factors have made the given data look as they do.\nBefore we conclude, I’d like to try a quick glance at other disciplines, beyond machine learning and computer science, domains where fairness figures among the central topics. This section is necessarily limited in every respect; it should be seen as a flashlight, an invitation to read and reflect rather than an orderly exposition. The short section will end with a word of caution: Since drawing analogies can feel highly enlightening (and is intellectually satisfying, for sure), it is easy to abstract away practical realities. But I’m getting ahead of myself."
  },
  {
    "objectID": "posts/ai-fairness/index.html#a-quick-glance-at-neighboring-fields-law-and-political-philosophy",
    "href": "posts/ai-fairness/index.html#a-quick-glance-at-neighboring-fields-law-and-political-philosophy",
    "title": "Starting to think about AI Fairness",
    "section": "A quick glance at neighboring fields: law and political philosophy",
    "text": "A quick glance at neighboring fields: law and political philosophy\nIn jurisprudence, fairness and discrimination constitute an important subject. A recent paper that caught my attention is Wachter, Mittelstadt, and Russell (2020a) . From a machine learning perspective, the interesting point is the classification of metrics into bias-preserving and bias-transforming. The terms speak for themselves: Metrics in the first group reflect biases in the dataset used for training; ones in the second do not. In that way, the distinction parallels Friedler, Scheidegger, and Venkatasubramanian (2016) ’s confrontation of two “worldviews”. But the exact words used also hint at how guidance by metrics feeds back into society: Seen as strategies, one preserves existing biases; the other, to consequences unknown a priori, changes the world.\nTo the ML practitioner, this framing is of great help in evaluating what criteria to apply in a project. Helpful, too, is the systematic mapping provided of metrics to the two groups; it is here that, as alluded to above, we encounter conditional demographic parity among the bias-transforming ones. I agree that in spirit, this metric can be seen as bias-transforming; if we take two sets of people who, per all available criteria, are equally qualified for a job, and then find the whites favored over the Blacks, fairness is clearly violated. But the problem here is “available”: per all available criteria. What if we have reason to assume that, in a dataset, all predictors are biased? Then it will be very hard to prove that discrimination has occurred.\nA similar problem, I think, surfaces when we look at the field of political philosophy, and consult theories on distributive justice for guidance. Heidari et al. (2018) have written a paper comparing the three criteria – demographic parity, equality of opportunity, and predictive parity – to egalitarianism, equality of opportunity (EOP) in the Rawlsian sense, and EOP seen through the glass of luck egalitarianism, respectively. While the analogy is fascinating, it too assumes that we may take what is in the data at face value. In their likening predictive parity to luck egalitarianism, they have to go to especially great lengths, in assuming that the predicted class reflects effort exerted. In the below table, I therefore take the liberty to disagree, and map a libertarian view of distributive justice to both equality of opportunity and predictive parity metrics.\nIn summary, we end up with two highly controversial categories of fairness criteria, one bias-preserving, “what you see is what you get”-assuming, and libertarian, the other bias-transforming, “we’re all equal”-thinking, and egalitarian. Here, then, is that often-announced table.\n\n\n\n\n\n\n\n\n\n\nDemographic parity\nEquality of opportunity\nPredictive parity\n\n\n\n\nA.K.A. / subsumes / related concepts\nstatistical parity, group fairness, disparate impact, conditional demographic parity 4\nequalized odds, equal false positive / negative rates\nequal positive / negative predictive values, calibration by group\n\n\nStatistical independence criterion 5\nindependence\n\\(\\hat{Y} \\perp A\\)\nseparation\n\\(\\hat{Y} \\perp A | Y\\)\nsufficiency\n\\(Y \\perp A | \\hat{Y}\\)\n\n\nIndividual / group\ngroup\ngroup (most) or individual (fairness through awareness)\ngroup\n\n\nDistributive Justice\negalitarian\nlibertarian (contra Heidari et al., see above)\nlibertarian (contra Heidari et al., see above)\n\n\nEffect on bias 6\ntransforming\npreserving\npreserving\n\n\nPolicy / “worldview” 7\nWe’re all equal (WAE)\nWhat you see is what you get (WYSIWIG)\nWhat you see is what you get (WYSIWIG)"
  },
  {
    "objectID": "posts/ai-fairness/index.html#a-conclusion",
    "href": "posts/ai-fairness/index.html#a-conclusion",
    "title": "Starting to think about AI Fairness",
    "section": "(A) Conclusion",
    "text": "(A) Conclusion\nIn line with its original goal – to provide some help in starting to think about AI fairness metrics – this article does not end with recommendations. It does, however, end with an observation. As the last section has shown, amidst all theorems and theories, all proofs and memes, it makes sense to not lose sight of the concrete: the data trained on, and the ML process as a whole. Fairness is not something to be evaluated post hoc; the feasibility of fairness is to be reflected on right from the beginning.\nIn that regard, assessing impact on fairness is not that different from that essential, but often toilsome and non-beloved, stage of modeling that precedes the modeling itself: exploratory data analysis.\nThanks for reading!\nPhoto by Anders Jildén on Unsplash"
  },
  {
    "objectID": "posts/ai-fairness/index.html#footnotes",
    "href": "posts/ai-fairness/index.html#footnotes",
    "title": "Starting to think about AI Fairness",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSrivastava et al. (2018)↩︎\nCranmer et al. (2020)↩︎\nWachter, Mittelstadt, and Russell (2020b)↩︎\nWachter, Mittelstadt, and Russell (2020b)↩︎\nBarocas, Hardt, and Narayanan (2019)↩︎\nWachter, Mittelstadt, and Russell (2020a)↩︎\nFriedler, Scheidegger, and Venkatasubramanian (2016)↩︎"
  },
  {
    "objectID": "posts/ai_ethics_optimization_problem/index.html#footnotes",
    "href": "posts/ai_ethics_optimization_problem/index.html#footnotes",
    "title": "AI ethics is not an optimization problem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor brevity, I’ll be subsuming deep learning and other contemporary machine learning methods under AI, following common(-ish) usage.↩︎\n I can’t hope to express this better than Maciej Cegłowski did here, so I won’t elaborate on that topic any further.↩︎\nMore on that below. Metrics used in machine learning mostly are proxies for things we really care about; there are lots of ways this can go wrong.↩︎\nMeaning, a one-model-fits-all approach puts some groups at a disadvantage.↩︎\ncited after Thomas and Uminsky↩︎\nsee e.g., DataFeminism and RaceAfterTechnology.↩︎"
  },
  {
    "objectID": "posts/solms_consciousness/index.html#footnotes",
    "href": "posts/solms_consciousness/index.html#footnotes",
    "title": "A book I’d say everyone should read if such were a kind of thing I’d say",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThey have their own Stanford Encyclopedia entry: https://plato.stanford.edu/entries/qualia/.↩︎\nThis is the taxonomy of primary emotions embraced by Solms. It is mainly due to the work of Jaak Panksepp (see, e.g., Panksepp & Biven, The Archeology of Mind).↩︎\nAs of this writing, the only book I know on this topic is Alan Kendle’s Aphantasia: Experiences, Perceptions, and Insights.↩︎\nThis condition is called hydranencephaly.↩︎\nFor details on the theory, see also the paper he wrote with Friston, How and Why Consciousness Arises: Some Considerations from Physics and Physiology.↩︎"
  },
  {
    "objectID": "posts/deep-learning-scientific-computing-R-torch/index.html",
    "href": "posts/deep-learning-scientific-computing-R-torch/index.html",
    "title": "Deep Learning and Scientific Computing with R torch: the book",
    "section": "",
    "text": "First things first: Where can you get it? As of today, you can download the e-book or order a print copy from the publisher, CRC Press; the free online edition is here. There is, to my knowledge, no problem to perusing the online version – besides one: It doesn’t have the squirrel that’s on the book cover.\nSo if you’re a lover of amazing creatures…"
  },
  {
    "objectID": "posts/deep-learning-scientific-computing-R-torch/index.html#footnotes",
    "href": "posts/deep-learning-scientific-computing-R-torch/index.html#footnotes",
    "title": "Deep Learning and Scientific Computing with R torch: the book",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nthough challenging, as well (though … or because?)↩︎"
  },
  {
    "objectID": "posts/group-equivariant-cnn-1/index.html",
    "href": "posts/group-equivariant-cnn-1/index.html",
    "title": "Upside down, a cat’s still a cat: Evolving image recognition with Geometric Deep Learning",
    "section": "",
    "text": "This is the first in a series of posts on group-equivariant convolutional neural networks (GCNNs). Today, we keep it short, high-level, and conceptual; examples and implementations will follow. In looking at GCNNs, we are resuming a topic we first wrote about in 2021: Geometric Deep Learning, a principled, math-driven approach to network design that, since then, has only risen in scope and impact."
  },
  {
    "objectID": "posts/group-equivariant-cnn-1/index.html#footnotes",
    "href": "posts/group-equivariant-cnn-1/index.html#footnotes",
    "title": "Upside down, a cat’s still a cat: Evolving image recognition with Geometric Deep Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThat’s when the image has a single channel. Otherwise, there are a number of grids, each mapped to a separate channel.↩︎\nFrom Wolfram Alpha (beginning of article).↩︎"
  },
  {
    "objectID": "posts/group-equivariant-cnn-2/index.html",
    "href": "posts/group-equivariant-cnn-2/index.html",
    "title": "Implementing rotation equivariance: Group-equivariant CNN from scratch",
    "section": "",
    "text": "Convolutional neural networks (CNNs) are great – they’re able to detect features in an image no matter where. Well, not exactly. They’re not indifferent to just any kind of movement. Shifting up or down, or left or right, is fine; rotating around an axis is not. That’s because of how convolution works: traverse by row, then traverse by column (or the other way round). If we want “more” (e.g., successful detection of an upside-down object), we need to extend convolution to an operation that is rotation-equivariant. An operation that is equivariant to some type of action will not only register the moved feature per se, but also, keep track of which concrete action made it appear where it is.\nThis is the second post in a series that introduces group-equivariant CNNs (GCNNs). The first was a high-level introduction to why we’d want them, and how they work. There, we introduced the key player, the symmetry group, which specifies what kinds of transformations are to be treated equivariantly. If you haven’t, please take a look at that post first, since here I’ll make use of terminology and concepts it introduced.\nToday, we code a simple GCNN from scratch. Code and presentation tightly follow a notebook provided as part of University of Amsterdam’s 2022 Deep Learning Course. They can’t be thanked enough for making available such excellent learning materials.\nIn what follows, my intent is to explain the general thinking, and how the resulting architecture is built up from smaller modules, each of which is assigned a clear purpose. For that reason, I won’t reproduce all the code here; instead, I’ll make use of the package gcnn. Its methods are heavily annotated; so to see some details, don’t hesitate to look at the code.\nAs of today, gcnn implements one symmetry group: \\(C_4\\), the one that serves as a running example throughout post one. It is straightforwardly extensible, though, making use of class hierarchies throughout."
  },
  {
    "objectID": "posts/group-equivariant-cnn-2/index.html#footnotes",
    "href": "posts/group-equivariant-cnn-2/index.html#footnotes",
    "title": "A book I’d say everyone should read if such were a kind of thing I’d say",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThey have their own Stanford Encyclopedia entry: https://plato.stanford.edu/entries/qualia/.↩︎\nThis is the taxonomy of primary emotions embraced by Solms. It is mainly due to the work of Jaak Panksepp (see, e.g., Panksepp & Biven, The Archeology of Mind).↩︎\nAs of this writing, the only book I know on this topic is Alan Kendle’s Aphantasia: Experiences, Perceptions, and Insights.↩︎\nThis condition is called hydranencephaly.↩︎\nFor details on the theory, see also the paper he wrote with Friston, How and Why Consciousness Arises: Some Considerations from Physics and Physiology.↩︎"
  },
  {
    "objectID": "posts/group-equivariant-cnn-3/index.html",
    "href": "posts/group-equivariant-cnn-3/index.html",
    "title": "Group-equivariant neural networks with escnn",
    "section": "",
    "text": "Today, we resume our exploration of group equivariance. This is the third post in the series. The first was a high-level introduction: what this is all about; how equivariance is operationalized; and why it is of relevance to many deep-learning applications. The second sought to concretize the key ideas by developing a group-equivariant CNN from scratch. That being instructive, but too tedious for practical use, today we look at a carefully designed, highly-performant library that hides the technicalities and enables a convenient workflow.\nFirst though, let me again set the context. In physics, an all-important concept is that of symmetry1, a symmetry being present whenever some quantity is being conserved. But we don’t even need to look to science. Examples arise in daily life, and – otherwise why write about it - in the tasks we apply deep learning to.\nIn daily life: Think about speech – me stating “it is cold”, for example. Formally, or denotation-wise, the sentence will have the same meaning now as in five hours. (Connotations, on the other hand, can and will probably be different!). This is a form of translation symmetry, translation in time.\nIn deep learning: Take image classification. For the usual convolutional neural network, a cat in the center of the image is just that, a cat; a cat on the bottom is, too. But one sleeping, comfortably curled like a half-moon “open to the right”, will not be “the same” as one in a mirrored position. Of course, we can train the network to treat both as equivalent by providing training images of cats in both positions, but that is not a scaleable approach. Instead, we’d like to make the network aware of these symmetries, so they are automatically preserved throughout the network architecture."
  },
  {
    "objectID": "posts/group-equivariant-cnn-3/index.html#footnotes",
    "href": "posts/group-equivariant-cnn-3/index.html#footnotes",
    "title": "Group-equivariant neural networks with escnn",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is nicely explained in, for example, Jakob Schwichtenberg’s Physics from symmetry.↩︎\nIf you have some background on representations: No, not those characters …↩︎\nYes, that word exists, although I must admit I didn’t know before typing it into a search engine.↩︎\nOne paper particularly stood out to me: [@abs-2106-06020].↩︎\nDirectly pertinent to today’s topic, thinking of the materials produced for University of Amsterdam’s course on group-equivariant deep learning.↩︎\nIf, after reading this post, you feel that maybe you would be interested in porting it – I can definitely say I think that’s a great idea!↩︎\nIf you’re new to reticulate, please consult its excellent documentation on topics like calling Python from R, determining the Python version used, installing Python packages, as well as an outstanding introduction to Python for R users.↩︎\nSee the documentation for escnn$nn$RestrictionModule for how to do this.↩︎"
  },
  {
    "objectID": "posts/enso-prediction/index.html",
    "href": "posts/enso-prediction/index.html",
    "title": "Forecasting El Niño-Southern Oscillation (ENSO)",
    "section": "",
    "text": "Today, we use the convLSTM introduced in a previous post to predict El Niño-Southern Oscillation (ENSO)."
  },
  {
    "objectID": "posts/enso-prediction/index.html#footnotes",
    "href": "posts/enso-prediction/index.html#footnotes",
    "title": "Forecasting El Niño-Southern Oscillation (ENSO)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nE.g., Forecasting El Niño with Convolutional and Recurrent Neural Networks.↩︎\nE.g., CMIP5, CNRM-CM5, or HADGEM2-ES.↩︎\nThis region extends over latitudes from 5° south to 5° north and longitudes from 120° west to 170° west.↩︎\nThat classification is based on ONI (Oceanic Niño Index), a measure representing 3-month average anomalies in the Niño 3.4 Index.↩︎\nThis may also be an artifact produced by the software stack involved in reading the file.↩︎"
  },
  {
    "objectID": "posts/geometric-deep-learning/index.html",
    "href": "posts/geometric-deep-learning/index.html",
    "title": "Beyond alchemy: A first look at geometric deep learning",
    "section": "",
    "text": "To the practitioner, it may often seem that with deep learning, there is a lot of magic involved. Magic in how hyper-parameter choices affect performance, for example. More fundamentally yet, magic in the impacts of architectural decisions. Magic, sometimes, in that it even works (or not). Sure, papers abound that strive to mathematically prove why, for specific solutions, in specific contexts, this or that technique will yield better results. But theory and practice are strangely dissociated: If a technique does turn out to be helpful in practice, doubts may still arise to whether that is, in fact, due to the purported mechanism. Moreover, level of generality often is low.\nIn this situation, one may feel grateful for approaches that aim to elucidate, complement, or replace some of the magic. By “complement or replace”, I’m alluding to attempts to incorporate domain-specific knowledge into the training process. Interesting examples exist in several sciences, and I certainly hope to be able to showcase a few of these, on this blog at a later time. As for the “elucidate”, this characterization is meant to lead on to the topic of this post: the program of geometric deep learning."
  },
  {
    "objectID": "posts/geometric-deep-learning/index.html#footnotes",
    "href": "posts/geometric-deep-learning/index.html#footnotes",
    "title": "A book I’d say everyone should read if such were a kind of thing I’d say",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThey have their own Stanford Encyclopedia entry: https://plato.stanford.edu/entries/qualia/.↩︎\nThis is the taxonomy of primary emotions embraced by Solms. It is mainly due to the work of Jaak Panksepp (see, e.g., Panksepp & Biven, The Archeology of Mind).↩︎\nAs of this writing, the only book I know on this topic is Alan Kendle’s Aphantasia: Experiences, Perceptions, and Insights.↩︎\nThis condition is called hydranencephaly.↩︎\nFor details on the theory, see also the paper he wrote with Friston, How and Why Consciousness Arises: Some Considerations from Physics and Physiology.↩︎"
  },
  {
    "objectID": "posts/torchwavelets/index.html",
    "href": "posts/torchwavelets/index.html",
    "title": "AO, NAO, ENSO: A wavelet analysis example",
    "section": "",
    "text": "Recently, we showed how to use torch for wavelet analysis. A member of the family of spectral analysis methods, wavelet analysis bears some similarity to the Fourier Transform, and specifically, to its popular two-dimensional application, the spectrogram.\nAs explained in that book excerpt, though, there are significant differences. For the purposes of the current post, it suffices to know that frequency-domain patterns are discovered by having a little “wave” (that, really, can be of any shape) “slide” over the data, computing degree of match (or mismatch) in the neighborhood of every sample.\nWith this post, then, my goal is two-fold.\nFirst, to introduce torchwavelets, a tiny, yet useful package that automates all of the essential steps involved. Compared to the Fourier Transform and its applications, the topic of wavelets is rather “chaotic” – meaning, it enjoys much less shared terminology, and much less shared practice. Consequently, it makes sense for implementations to follow established, community-embraced approaches, whenever such are available and well documented. With torchwavelets, we provide an implementation of Torrence and Compo’s 1998 “Practical Guide to Wavelet Analysis” (Torrence and Compo (1998)), an oft-cited paper that proved influential across a wide range of application domains. Code-wise, our package is mostly a port of Tom Runia’s PyTorch implementation, itself based on a prior implementation by Aaron O’Leary.\nSecond, to show an attractive use case of wavelet analysis in an area of great scientific interest and tremendous social importance (meteorology/climatology). Being by no means an expert myself, I’d hope this could be inspiring to people working in these fields, as well as to scientists and analysts in other areas where temporal data arise.\nConcretely, what we’ll do is take three different atmospheric phenomena – El Niño–Southern Oscillation (ENSO), North Atlantic Oscillation (NAO), and Arctic Oscillation (AO) – and investigate them using wavelet analysis. In each case, we also look at the overall frequency spectrum, given by the Discrete Fourier Transform (DFT), as well as a classic time-series decomposition into trend, seasonal components, and remainder."
  },
  {
    "objectID": "posts/torchwavelets/index.html#footnotes",
    "href": "posts/torchwavelets/index.html#footnotes",
    "title": "A book I’d say everyone should read if such were a kind of thing I’d say",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThey have their own Stanford Encyclopedia entry: https://plato.stanford.edu/entries/qualia/.↩︎\nThis is the taxonomy of primary emotions embraced by Solms. It is mainly due to the work of Jaak Panksepp (see, e.g., Panksepp & Biven, The Archeology of Mind).↩︎\nAs of this writing, the only book I know on this topic is Alan Kendle’s Aphantasia: Experiences, Perceptions, and Insights.↩︎\nThis condition is called hydranencephaly.↩︎\nFor details on the theory, see also the paper he wrote with Friston, How and Why Consciousness Arises: Some Considerations from Physics and Physiology.↩︎"
  },
  {
    "objectID": "posts/llm-intro/index.html",
    "href": "posts/llm-intro/index.html",
    "title": "What are Large Language Models? What are they not?",
    "section": "",
    "text": "“At this writing, the only serious ELIZA scripts which exist are some which cause ELIZA to respond roughly as would certain psychotherapists (Rogerians). ELIZA performs best when its human correspondent is initially instructed to”talk” to it, via the typewriter of course, just as one would to a psychiatrist. This mode of conversation was chosen because the psychiatric interview is one of the few examples of categorized dyadic natural language communication in which one of the participating pair is free to assume the pose of knowing almost nothing of the real world. If, for example, one were to tell a psychiatrist “I went for a long boat ride” and he responded “Tell me about boats”, one would not assume that he knew nothing about boats, but that he had some purpose in so directing the subsequent conversation. It is important to note that this assumption is one made by the speaker. Whether it is realistic or not is an altogether separate question. In any case, it has a crucial psychological utility in that it serves the speaker to maintain his sense of being heard and understood. The speaker furher defends his impression (which even in real life may be illusory) by attributing to his conversational partner all sorts of background knowledge, insights and reasoning ability. But again, these are the speaker’s contribution to the conversation.”\nJoseph Weizenbaum, creator of ELIZA (Weizenbaum 1966).\nGPT, the ancestor all numbered GPTs, was released in June, 2018 – five years ago, as I write this. Five years: that’s a long time. It certainly is as measured on the time scale of deep learning, the thing that is, usually, behind when people talk of “AI”. One year later, GPT was followed by GPT-2; another year later, by GPT-3. At this point, public attention was still modest – as expected, really, for these kinds of technologies that require lots of specialist knowledge. (For GPT-2, what may have increased attention beyond the normal, a bit, was OpenAI ’s refusal to publish the complete training code and full model weights, supposedly due to the threat posed by the model’s capabilities – alternatively, as argued by others, as a marketing strategy, or yet alternatively, as a way to preserve one’s own competitive advantage just a tiny little bit longer.\nAs of 2023, with GPT-3.5 and GPT-4 having followed, everything looks different. (Almost) everyone seems to know GPT, at least when that acronym appears prefixed by a certain syllable. Depending on who you talk to, people don’t seem to stop talking about that fantastic [insert thing here] ChatGPT generated for them, about its enormous usefulness with respect to [insert goal here]… or about the flagrant mistakes it made, and the danger that legal regulation and political enforcement will never be able to catch up.\nWhat made the difference? Obviously, it’s ChatGPT, or put differently, the fact that now, there is a means for people to make active use of such a tool, employing it for whatever their personal needs or interests are1. In fact, I’d argue it’s more than that: ChatGPT is not some impersonal tool – it talks to you, picking up your clarifications, changes of topic, mood… It is someone rather than something, or at least that’s how it seems. I’ll come back to that point in It’s us, really: Anthropomorphism unleashed. Before, let’s take a look at the underlying technology."
  },
  {
    "objectID": "posts/llm-intro/index.html#footnotes",
    "href": "posts/llm-intro/index.html#footnotes",
    "title": "What are Large Language Models? What are they not?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEvidently, this is not about singling out ChatGPT as opposed to other chatbots; rather, I’m adopting it as the prototypical such application, since it is the one omnipresent in the media these days.↩︎\nI’m using quotes to refer to how attention is operationalized in deep learning, as opposed to how it is conceptualized in cognitive science or psychology.↩︎\nIf you’re wondering how that is possible – shouldn’t there be a separate, top-level module for generation? – no, there need not be. That’s because training implies prediction.↩︎\nWhy the quotes? See Large Language Models: What they are not.↩︎\nAs a fascinating example from dynamical systems theory, take delay coordinate embeddings.↩︎\nSuitably named embedding layer.↩︎\nSee, for example, (Caliskan et al. 2022).↩︎\nFor GPT-4, even high-level model information has not been released.↩︎\nMathematically, this is achieved by a pretty standard and pervasively-used, in machine learning, operation, the dot product.↩︎\n… and the Boltzmann constant – but that being a constant, we don’t consider it here.↩︎\nThat choice of species is probably not a coincidence: see https://en.wikipedia.org/wiki/Cephalopod_intelligence.↩︎\nAs opposed to the aforementioned problems subsumed under “reasoning”, those having been constructed for research purposes.↩︎\nFrom (Spaerck 2004).↩︎\nSee https://lisafeldmanbarrett.com/books/how-emotions-are-made/.↩︎"
  },
  {
    "objectID": "posts/enso-prediction/index.html#input-sea-surface-temperatures",
    "href": "posts/enso-prediction/index.html#input-sea-surface-temperatures",
    "title": "Forecasting El Niño-Southern Oscillation (ENSO)",
    "section": "Input: Sea Surface Temperatures",
    "text": "Input: Sea Surface Temperatures\nMonthly sea surface temperatures are provided in a latitude-longitude grid of resolution 1°. Details of how the data were processed are available here.\nData files are available in GRIB format; each file contains averages computed for a single month. We can either download individual files or generate a text file of URLs for download. Once you’ve saved these URLs to a file, you can have R get the files for you like so:\n\npurrr::walk(\n   readLines(\"files\"),\n   function(f) download.file(url = f, destfile = basename(f))\n)\n\nFrom R, we can read GRIB files using stars. For example:\n\n# let's just quickly load all libraries we require to start with\n\nlibrary(torch)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(viridis)\nlibrary(ggthemes)\n\ntorch_manual_seed(777)\n\nread_stars(file.path(grb_dir, \"sst189101.grb\"))\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n sst189101.grb   \n Min.   :-274.9  \n 1st Qu.:-272.8  \n Median :-259.1  \n Mean   :-260.0  \n 3rd Qu.:-248.4  \n Max.   :-242.8  \n NA's   :21001   \ndimension(s):\n  from  to offset delta                       refsys point values    \nx    1 360      0     1 Coordinate System importe...    NA   NULL [x]\ny    1 180     90    -1 Coordinate System importe...    NA   NULL [y]\nSo in this GRIB file, we have one attribute - which we know to be sea surface temperature – on a two-dimensional grid. As to the latter, we can complement what stars tells us with additional info found in the documentation:\n\nThe east-west grid points run eastward from 0.5ºE to 0.5ºW, while the north-south grid points run northward from 89.5ºS to 89.5ºN.\n\nWe note a few things we’ll want to do with this data. For one, the temperatures seem to be given in Kelvin, but with minus signs.5 We’ll remove the minus signs and convert to degrees Celsius for convenience. We’ll also have to think about what to do with the NAs that appear for all non-maritime coordinates.\nBefore we get there though, we need to combine data from all files into a single data frame. This adds an additional dimension, time, ranging from 1891/01/01 to 2020/01/12:\n\ngrb &lt;- read_stars(\n  file.path(grb_dir, map(readLines(\"files\", warn = FALSE), basename)), along = \"time\") %&gt;%\n  st_set_dimensions(3,\n                    values = seq(as.Date(\"1891-01-01\"), as.Date(\"2020-12-01\"), by = \"months\"),\n                    names = \"time\"\n                    )\n\ngrb\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n sst189101.grb   \n Min.   :-274.9  \n 1st Qu.:-273.3  \n Median :-258.8  \n Mean   :-260.0  \n 3rd Qu.:-247.8  \n Max.   :-242.8  \n NA's   :33724   \ndimension(s):\n     from   to offset delta                       refsys point                    values    \nx       1  360      0     1 Coordinate System importe...    NA                      NULL [x]\ny       1  180     90    -1 Coordinate System importe...    NA                      NULL [y]\ntime    1 1560     NA    NA                         Date    NA 1891-01-01,...,2020-12-01    \nLet’s visually inspect the spatial distribution of monthly temperatures for one year, 2020:\n\nggplot() +\n  geom_stars(data = grb %&gt;% filter(between(time, as.Date(\"2020-01-01\"), as.Date(\"2020-12-01\"))), alpha = 0.8) +\n  facet_wrap(\"time\") +\n  scale_fill_viridis() +\n  coord_equal() +\n  theme_map() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/enso-prediction/index.html#target-niño-3.4-index",
    "href": "posts/enso-prediction/index.html#target-niño-3.4-index",
    "title": "Forecasting El Niño-Southern Oscillation (ENSO)",
    "section": "Target: Niño 3.4 Index",
    "text": "Target: Niño 3.4 Index\nFor the Niño 3.4 Index, we download the monthly data and, among the provided features, zoom in on two: the index itself (column NINO34_MEAN) and PHASE, which can be E (El Niño), L (La Niño) or N (neutral).\n\nnino &lt;- read_table2(\"ONI_NINO34_1854-2020.txt\", skip = 9) %&gt;%\n  mutate(month = as.Date(paste0(YEAR, \"-\", `MON/MMM`, \"-01\"))) %&gt;%\n  select(month, NINO34_MEAN, PHASE) %&gt;%\n  filter(between(month, as.Date(\"1891-01-01\"), as.Date(\"2020-08-01\"))) %&gt;%\n  mutate(phase_code = as.numeric(as.factor(PHASE)))\n\nnrow(nino)\n\n1556\nNext, we look at how to get the data into a format convenient for training and prediction."
  },
  {
    "objectID": "posts/enso-prediction/index.html#input",
    "href": "posts/enso-prediction/index.html#input",
    "title": "Forecasting El Niño-Southern Oscillation (ENSO)",
    "section": "Input",
    "text": "Input\nFirst, we remove all input data for points in time where ground truth data are still missing.\n\nsst &lt;- grb %&gt;% filter(time &lt;= as.Date(\"2020-08-01\"))\n\nNext, as is done by e.g. Ham, Kim, and Luo (2019), we only use grid points between 55° south and 60° north. This has the additional advantage of reducing memory requirements.\n\nsst &lt;- grb %&gt;% filter(between(y,-55, 60))\n\ndim(sst)\n\n360, 115, 1560\nAs already alluded to, with the little data we have we can’t expect much in terms of generalization. Still, we set aside a small portion of the data for validation, since we’d like for this post to serve as a useful template to be used with bigger datasets.\n\nsst_train &lt;- sst %&gt;% filter(time &lt; as.Date(\"1990-01-01\"))\nsst_valid &lt;- sst %&gt;% filter(time &gt;= as.Date(\"1990-01-01\"))\n\nFrom here on, we work with R arrays.\n\nsst_train &lt;- as.tbl_cube.stars(sst_train)$mets[[1]]\nsst_valid &lt;- as.tbl_cube.stars(sst_valid)$mets[[1]]\n\nConversion to degrees Celsius is not strictly necessary, as initial experiments showed a slight performance increase due to normalizing the input, and we’re going to do that anyway. Still, it reads nicer to humans than Kelvin.\n\nsst_train &lt;- sst_train + 273.15\nquantile(sst_train, na.rm = TRUE)\n\n     0%     25%     50%     75%    100% \n-1.8000 12.9975 21.8775 26.8200 34.3700 \nNot at all surprisingly, global warming is evident from inspecting temperature distribution on the validation set (which was chosen to span the last thirty-one years).\n\nsst_valid &lt;- sst_valid + 273.15\nquantile(sst_valid, na.rm = TRUE)\n\n    0%    25%    50%    75%   100% \n-1.800 13.425 22.335 27.240 34.870 \nThe next-to-last step normalizes both sets according to training mean and variance.\n\ntrain_mean &lt;- mean(sst_train, na.rm = TRUE)\ntrain_sd &lt;- sd(sst_train, na.rm = TRUE)\n\nsst_train &lt;- (sst_train - train_mean) / train_sd\n\nsst_valid &lt;- (sst_valid - train_mean) / train_sd\n\nFinally, what should we do about the NA entries? We set them to zero, the (training set) mean. That may not be enough of an action though: It means we’re feeding the network roughly 30% misleading data. This is something we’re not done with yet.\n\nsst_train[is.na(sst_train)] &lt;- 0\nsst_valid[is.na(sst_valid)] &lt;- 0"
  },
  {
    "objectID": "posts/enso-prediction/index.html#target",
    "href": "posts/enso-prediction/index.html#target",
    "title": "Forecasting El Niño-Southern Oscillation (ENSO)",
    "section": "Target",
    "text": "Target\nThe target data are split analogously. Let’s check though: Are phases (categorizations) distributedly similarly in both sets?\n\nnino_train &lt;- nino %&gt;% filter(month &lt; as.Date(\"1990-01-01\"))\nnino_valid &lt;- nino %&gt;% filter(month &gt;= as.Date(\"1990-01-01\"))\n\nnino_train %&gt;% group_by(phase_code, PHASE) %&gt;% summarise(count = n(), avg = mean(NINO34_MEAN))\n\n# A tibble: 3 x 4\n# Groups:   phase_code [3]\n  phase_code PHASE count   avg\n       &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;\n1          1 E       301  27.7\n2          2 L       333  25.6\n3          3 N       554  26.7\n\nnino_valid %&gt;% group_by(phase_code, PHASE) %&gt;% summarise(count = n(), avg = mean(NINO34_MEAN))\n\n# A tibble: 3 x 4\n# Groups:   phase_code [3]\n  phase_code PHASE count   avg\n       &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;\n1          1 E        93  28.1\n2          2 L        93  25.9\n3          3 N       182  27.2\nThis doesn’t look too bad. Of course, we again see the overall rise in temperature, irrespective of phase.\nLastly, we normalize the index, same as we did for the input data.\n\ntrain_mean_nino &lt;- mean(nino_train$NINO34_MEAN)\ntrain_sd_nino &lt;- sd(nino_train$NINO34_MEAN)\n\nnino_train &lt;- nino_train %&gt;% mutate(NINO34_MEAN = scale(NINO34_MEAN, center = train_mean_nino, scale = train_sd_nino))\nnino_valid &lt;- nino_valid %&gt;% mutate(NINO34_MEAN = scale(NINO34_MEAN, center = train_mean_nino, scale = train_sd_nino))\n\nOn to the torch dataset."
  },
  {
    "objectID": "posts/geometric-deep-learning/index.html#geometric-deep-learning-an-attempt-at-unification",
    "href": "posts/geometric-deep-learning/index.html#geometric-deep-learning-an-attempt-at-unification",
    "title": "Beyond alchemy: A first look at geometric deep learning",
    "section": "Geometric deep learning: An attempt at unification",
    "text": "Geometric deep learning: An attempt at unification\nGeometric deep learning (henceforth: GDL) is what a group of researchers, including Michael Bronstein, Joan Bruna, Taco Cohen, and Petar Velicković, call their attempt to build a framework that places deep learning (DL) on a solid mathematical basis.\nPrima facie, this is a scientific endeavor: They take existing architectures and practices and show where these fit into the “DL blueprint”. DL research being all but confined to the ivory tower, though, it’s fair to assume that this is not all: From those mathematical foundations, it should be possible to derive new architectures, new techniques to fit a given task. Who, then, should be interested in this? Researchers, for sure; to them, the framework may well prove highly inspirational. Secondly, everyone interested in the mathematical constructions themselves — this probably goes without saying. Finally, the rest of us, as well: Even understood at a purely conceptual level, the framework offers an exciting, inspiring view on DL architectures that – I think – is worth getting to know about as an end in itself. The goal of this post is to provide a high-level introduction .\nBefore we get started though, let me mention the primary source for this text: Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges (Bronstein et al. (2021)).\n\nGeometric priors\nA prior, in the context of machine learning, is a constraint imposed on the learning task. A generic prior could come about in different ways; a geometric prior, as defined by the GDL group, arises, originally, from the underlying domain of the task. Take image classification, for example. The domain is a two-dimensional grid. Or graphs: The domain consists of collections of nodes and edges.\nIn the GDL framework, two all-important geometric priors are symmetry and scale separation.\n\nSymmetry\nA symmetry, in physics and mathematics, is a transformation that leaves some property of an object unchanged. The appropriate meaning of “unchanged” depends on what sort of property we’re talking about. Say the property is some “essence”, or identity — what object something is. If I move a few steps to the left, I’m still myself: The essence of being “myself” is shift-invariant. (Or: translation-invariant.) But say the property is location. If I move to the left, my location moves to the left. Location is shift-equivariant. (Translation-equivariant.)\nSo here we have two forms of symmetry: invariance and equivariance. One means that when we transform an object, the thing we’re interested in stays the same. The other means that we have to transform that thing as well.\nThe next question then is: What are possible transformations? Translation we already mentioned; on images, rotation or flipping are others. Transformations are composable; I can rotate the digit 3 by thirty degrees, then move it to the left by five units; I could also do things the other way around. (In this case, though not necessarily in general, the results are the same.) Transformations can be undone: If first I rotate, in some direction, by five degrees, I can then rotate in the opposite one, also by five degrees, and end up in the original position. We’ll see why this matters when we cross the bridge from the domain (grids, sets, etc.) to the learning algorithm.\n\n\nScale separation\nAfter symmetry, another important geometric prior is scale separation. Scale separation means that even if something is very “big” (extends a long way in, say, one or two dimensions), we can still start from small patches and “work our way up”. For example, take a cuckoo clock. To discern the hands, you don’t need to pay attention to the pendulum. And vice versa. And once you’ve taken inventory of hands and pendulum, you don’t have to care about their texture or exact position anymore.\nIn a nutshell, given scale separation, the top-level structure can be determined through successive steps of coarse-graining. We’ll see this prior nicely reflected in some neural-network algorithms.\n\n\n\nFrom domain priors to algorithmic ones\nSo far, all we’ve really talked about is the domain, using the word in the colloquial sense of “on what structure”, or “in terms of what structure”, something is given. In mathematical language, though, domain is used in a more narrow way, namely, for the “input space” of a function. And a function, or rather, two of them, is what we need to get from priors on the (physical) domain to priors on neural networks.\nThe first function maps from the physical domain to signal space. If, for images, the domain was the two-dimensional grid, the signal space now consists of images the way they are represented in a computer, and will be worked with by a learning algorithm. For example, in the case of RGB images, that representation is three-dimensional, with a color dimension on top of the inherited spatial structure. What matters is that by this function, the priors are preserved. If something is translation-invariant before “real-to-virtual” conversion, it will still be translation-invariant thereafter.\nNext, we have another function: the algorithm, or neural network, acting on signal space. Ideally, this function, again, would preserve the priors. Below, we’ll see how basic neural-network architectures typically preserve some important symmetries, but not necessarily all of them. We’ll also see how, at this point, the actual task makes a difference. Depending on what we’re trying to achieve, we may want to maintain some symmetry, but not care about another. The task here is analogous to the property in physical space. Just like in physical space, a movement to the left does not alter identity, a classifier, presented with that same shift, won’t care at all. But a segmentation algorithm will – mirroring the real-world shift in position.\nNow that we’ve made our way to algorithm space, the above requirement, formulated on physical space – that transformations be composable – makes sense in another light: Composing functions is exactly what neural networks do; we want these compositions to work just as deterministically as those of real-world transformations.\nIn sum, the geometric priors and the way they impose constraints, or desiderates, rather, on the learning algorithm lead to what the GDL group call their deep learning “blueprint”. Namely, a network should be composed of the following types of modules:\n\nLinear group-equivariant layers. (Here group is the group of transformations whose symmetries we’re interested to preserve.)\nNonlinearities. (This really does not follow from geometric arguments, but from the observation, often stated in introductions to DL, that without nonlinearities, there is no hierarchical composition of features, since all operations can be implemented in a single matrix multiplication.)\nLocal pooling layers. (These achieve the effect of coarse-graining, as enabled by the scale separation prior.)\nA group-invariant layer (global pooling). (Not every task will require such a layer to be present.)\n\nHaving talked so much about the concepts, which are highly fascinating, this list may seem a bit underwhelming. That’s what we’ve been doing anyway, right? Maybe; but once you look at a few domains and associated network architectures, the picture gets colorful again. So colorful, in fact, that we can only present a very sparse selection of highlights."
  },
  {
    "objectID": "posts/geometric-deep-learning/index.html#domains-priors-architectures",
    "href": "posts/geometric-deep-learning/index.html#domains-priors-architectures",
    "title": "Beyond alchemy: A first look at geometric deep learning",
    "section": "Domains, priors, architectures",
    "text": "Domains, priors, architectures\nGiven cues like “local” and “pooling”, what better architecture is there to start with than CNNs, the (still) paradigmatic deep learning architecture? Probably, it’s also the one a prototypic practitioner would be most familiar with.\n\nImages and CNNs\nVanilla CNNs are easily mapped to the four types of layers that make up the blueprint. Skipping over the nonlinearities, which, in this context, are of least interest, we next have two kinds of pooling.\nFirst, a local one, corresponding to max- or average-pooling layers with small strides (2 or 3, say). This reflects the idea of successive coarse-graining, where, once we’ve made use of some fine-grained information, all we need to proceed is a summary.\nSecond, a global one, used to effectively remove the spatial dimensions. In practice, this would usually be global average pooling. Here, there’s an interesting detail worth mentioning. A common practice, in image classification, is to replace global pooling by a combination of flattening and one or more feedforward layers. Since with feedforward layers, position in the input matters, this will do away with translation invariance.\nHaving covered three of the four layer types, we come to the most interesting one. In CNNs, the local, group-equivariant layers are the convolutional ones. What kinds of symmetries does convolution preserve? Think about how a kernel slides over an image, computing a dot product at every location. Say that, through training, it has developed an inclination toward singling out penguin bills. It will detect, and mark, one everywhere in an image — be it shifted left, right, top or bottom in the image. What about rotational motion, though? Since kernels move vertically and horizontally, but not in a circle, a rotated bill will be missed. Convolution is shift-equivariant, not rotation-invariant.\nThere is something that can be done about this, though, while fully staying within the framework of GDL. Convolution, in a more generic sense, does not have to imply constraining filter movement to horizontal and vertical translation. When reflecting a general group convolution, that motion is determined by whatever transformations constitute the group action. If, for example, that action included translation by sixty degrees, we could rotate the filter to all valid positions, then take these filters and have them slide over the image. In effect, we’d just wind up with more channels in the subsequent layer – the intended base number of filters times the number of attainable positions.\nThis, it must be said, it just one way to do it. A more elegant one is to apply the filter in the Fourier domain, where convolution maps to multiplication. The Fourier domain, however, is as fascinating as it is out of scope for this post.\nThe same goes for extensions of convolution from the Euclidean grid to manifolds, where distances are no longer measured by a straight line as we know it. Often on manifolds, we’re interested in invariances beyond translation or rotation: Namely, algorithms may have to support various types of deformation. (Imagine, for example, a moving rabbit, with its muscles stretching and contracting as it hobbles.) If you’re interested in these kinds of problems, the GDL book goes into those in great detail.\nFor group convolution on grids – in fact, we may want to say “on things that can be arranged in a grid” – the authors give two illustrative examples. (One thing I like about these examples is something that extends to the whole book: Many applications are from the world of natural sciences, encouraging some optimism as to the role of deep learning (“AI”) in society.)\nOne example is from medical volumetric imaging (MRI or CT, say), where signals are represented on a three-dimensional grid. Here the task calls not just for translation in all directions, but also, rotations, of some sensible degree, about all three spatial axes. The other is from DNA sequencing, and it brings into play a new kind of invariance we haven’t mentioned yet: reverse-complement symmetry. This is because once we’ve decoded one strand of the double helix, we already know the other one.\nFinally, before we wrap up the topic of CNNs, let’s mention how through creativity, one can achieve – or put cautiously, try to achieve – certain invariances by means other than network architecture. A great example, originally associated mostly with images, is data augmentation. Through data augmentation, we may hope to make training invariant to things like slight changes in color, illumination, perspective, and the like.\n\n\nGraphs and GNNs\nAnother type of domain, underlying many scientific and non-scientific applications, are graphs. Here, we are going to be a lot more brief. One reason is that so far, we have not had many posts on deep learning on graphs, so to the readers of this blog, the topic may seem fairly abstract. The other reason is complementary: That state of affairs is exactly something we’d like to see changing. Once we write more about graph DL, occasions to talk about respective concepts will be plenty.\nIn a nutshell, though, the dominant type of invariance in graph DL is permutation equivariance. Permutation, because when you stack a node and its features in a matrix, it doesn’t matter whether node one is in row three or row fifteen. Equivariance, because once you do permute the nodes, you also have to permute the adjacency matrix, the matrix that captures which node is linked to what other nodes. This is very different from what holds for images: We can’t just randomly permute the pixels.\n\n\nSequences and RNNs\nWith RNNs, we are going be very brief as well, although for a different reason. My impression is that so far, this area of research – meaning, GDL as it relates to sequences – has not received too much attention yet, and (maybe) for that reason, seems of lesser impact on real-world applications.\nIn a nutshell, the authors refer two types of symmetry: First, translation-invariance, as long as a sequence is left-padded for a sufficient number of steps. (This is due to the hidden units having to be initialized somehow.) This holds for RNNs in general.\nSecond, time warping: If a network can be trained that correctly works on a sequence measured on some time scale, there is another network, of the same architecture but likely with different weights, that will work equivalently on re-scaled time. This invariance only applies to gated RNNs, such as the LSTM.\n\n\nWhat’s next?\nAt this point, we conclude this conceptual introduction. If you want to learn more, and are not too scared by the math, definitely check out the book. (I’d also say it lends itself well to incremental understanding, as in, iteratively going back to some details once one has acquired more background.)\nSomething else to wish for certainly is practice. There is an intimate connection between GDL and deep learning on graphs; which is one reason we’re hoping to be able to feature the latter more frequently in the future. The other is the wealth of interesting applications that take graphs as their input. Until then, thanks for reading!\nPhoto by NASA on Unsplash"
  },
  {
    "objectID": "posts/torchwavelets/index.html#three-oscillations",
    "href": "posts/torchwavelets/index.html#three-oscillations",
    "title": "AO, NAO, ENSO: A wavelet analysis example",
    "section": "Three oscillations",
    "text": "Three oscillations\nBy far the best-known – the most infamous, I should say – among the three is El Niño–Southern Oscillation (ENSO), a.k.a. El Niño/La Niña. The term refers to a changing pattern of sea surface temperatures and sea-level pressures occurring in the equatorial Pacific. Both El Niño and La Niña can and do have catastrophic impact on people’s lives, most notably, for people in developing countries west and east of the Pacific.\nEl Niño occurs when surface water temperatures in the eastern Pacific are higher than normal, and the strong winds that normally blow from east to west are unusually weak. From April to October, this leads to hot, extremely wet weather conditions along the coasts of northern Peru and Ecuador, continually resulting in major floods. La Niña, on the other hand, causes a drop in sea surface temperatures over Southeast Asia as well as heavy rains over Malaysia, the Philippines, and Indonesia. While these are the areas most gravely impacted, changes in ENSO reverberate across the globe.\nLess well known than ENSO, but highly influential as well, is the North Atlantic Oscillation (NAO). It strongly affects winter weather in Europe, Greenland, and North America. Its two states relate to the size of the pressure difference between the Icelandic High and the Azores Low. When the pressure difference is high, the jet stream – those strong westerly winds that blow between North America and Northern Europe – is yet stronger than normal, leading to warm, wet European winters and calmer-than-normal conditions in Eastern North America. With a lower-than-normal pressure difference, however, the American East tends to incur more heavy storms and cold-air outbreaks, while winters in Northern Europe are colder and more dry.\nFinally, the Arctic Oscillation (AO) is a ring-like pattern of sea-level pressure anomalies centered at the North Pole. (Its Southern-hemisphere equivalent is the Antarctic Oscillation.) AO’s influence extends beyond the Arctic Circle, however; it is indicative of whether and how much Arctic air flows down into the middle latitudes. AO and NAO are strongly related, and might designate the same physical phenomenon at a fundamental level.\nNow, let’s make these characterizations more concrete by looking at actual data."
  },
  {
    "objectID": "posts/torchwavelets/index.html#analysis-enso",
    "href": "posts/torchwavelets/index.html#analysis-enso",
    "title": "AO, NAO, ENSO: A wavelet analysis example",
    "section": "Analysis: ENSO",
    "text": "Analysis: ENSO\nWe begin with the best-known of these phenomena: ENSO. Data are available from 1854 onwards; however, for comparability with AO, we discard all records prior to January, 1950. For analysis, we pick NINO34_MEAN, the monthly average sea surface temperature in the Niño 3.4 region (i.e., the area between 5° South, 5° North, 190° East, and 240° East). Finally, we convert to a tsibble, the format expected by feasts::STL().\n\nlibrary(tidyverse)\nlibrary(tsibble)\n\ndownload.file(\n  \"https://bmcnoldy.rsmas.miami.edu/tropics/oni/ONI_NINO34_1854-2022.txt\",\n  destfile = \"ONI_NINO34_1854-2022.txt\"\n)\n\nenso &lt;- read_table(\"ONI_NINO34_1854-2022.txt\", skip = 9) %&gt;%\n  mutate(x = yearmonth(as.Date(paste0(YEAR, \"-\", `MON/MMM`, \"-01\")))) %&gt;%\n  select(x, enso = NINO34_MEAN) %&gt;%\n  filter(x &gt;= yearmonth(\"1950-01\"), x &lt;= yearmonth(\"2022-09\")) %&gt;%\n  as_tsibble(index = x)\n\nenso\n\n# A tsibble: 873 x 2 [1M]\n          x  enso\n      &lt;mth&gt; &lt;dbl&gt;\n 1 1950 Jan  24.6\n 2 1950 Feb  25.1\n 3 1950 Mar  25.9\n 4 1950 Apr  26.3\n 5 1950 May  26.2\n 6 1950 Jun  26.5\n 7 1950 Jul  26.3\n 8 1950 Aug  25.9\n 9 1950 Sep  25.7\n10 1950 Oct  25.7\n# … with 863 more rows\nAs already announced, we want to look at seasonal decomposition, as well. In terms of seasonal periodicity, what do we expect? Unless told otherwise, feasts::STL() will happily pick a window size for us. However, there’ll likely be several important frequencies in the data. (Not wanting to ruin the suspense, but for AO and NAO, this will definitely be the case!). Besides, we want to compute the Fourier Transform anyway, so why not do that first?\nHere is the power spectrum:\n\nlibrary(torch)\nfft &lt;- torch_fft_fft(as.numeric(scale(enso$enso)))\n\nIn the below plot, the x axis corresponds to frequencies, expressed as “number of times per year”. We only display frequencies up to and including the Nyquist frequency, i.e., half the sampling rate, which in our case is 12 (per year).\n\nnum_samples &lt;- nrow(enso)\nnyquist_cutoff &lt;- ceiling(num_samples / 2) # highest discernible frequency\nbins_below_nyquist &lt;- 0:nyquist_cutoff\n\nsampling_rate &lt;- 12 # per year\nfrequencies_per_bin &lt;- sampling_rate / num_samples\nfrequencies &lt;- frequencies_per_bin * bins_below_nyquist\n\ndf &lt;- data.frame(f = frequencies, y = as.numeric(fft[1:(nyquist_cutoff + 1)]$abs()))\ndf %&gt;% ggplot(aes(f, y)) +\n  geom_line() +\n  xlab(\"frequency (per year)\") +\n  ylab(\"magnitude\") +\n  ggtitle(\"Spectrum of Niño 3.4 data\")\n\n\n\n\nFrequency spectrum of monthly average sea surface temperature in the Niño 3.4 region, 1950 to present.\n\n\nThere is one dominant frequency, corresponding to about once a year. From this component alone, we’d expect one El Niño event – or equivalently, one La Niña – per year. But let’s locate important frequencies more precisely. With not many other periodicities standing out, we may as well restrict ourselves to three:\n\nstrongest &lt;- torch_topk(fft[1:(nyquist_cutoff/2)]$abs(), 3)\nstrongest\n\n[[1]]\ntorch_tensor\n233.9855\n172.2784\n142.3784\n[ CPUFloatType{3} ]\n\n[[2]]\ntorch_tensor\n74\n21\n7\n[ CPULongType{3} ]\nWhat we have here are the magnitudes of the dominant components, as well as their respective bins in the spectrum. Let’s see which actual frequencies these correspond to:\n\nimportant_freqs &lt;- frequencies[as.numeric(strongest[[2]])]\nimportant_freqs\n\n[1] 1.00343643 0.27491409 0.08247423 \nThat’s once per year, once per quarter, and once every twelve years, approximately. Or, expressed as periodicity, in terms of months (i.e., how many months are there in a period):\n\nnum_observations_in_season &lt;- 12/important_freqs  \nnum_observations_in_season\n\n[1] 11.95890  43.65000 145.50000  \nWe now pass these to feasts::STL(), to obtain a five-fold decomposition into trend, seasonal components, and remainder.\n\nlibrary(feasts)\nenso %&gt;%\n  model(STL(enso ~ season(period = 12) + season(period = 44) +\n              season(period = 145))) %&gt;%\n  components() %&gt;%\n  autoplot()\n\n\n\n\nDecomposition of ENSO data into trend, seasonal components, and remainder by feasts::STL().\n\n\nAccording to Loess decomposition, there still is significant noise in the data – the remainder remaining high despite our hinting at important seasonalities. In fact, there is no big surprise in that: Looking back at the DFT output, not only are there many, close to one another, low- and lowish-frequency components, but in addition, high-frequency components just won’t cease to contribute. And really, as of today, ENSO forecasting – tremendously important in terms of human impact – is focused on predicting oscillation state just a year in advance. This will be interesting to keep in mind for when we proceed to the other series – as you’ll see, it’ll only get worse.\nBy now, we’re well informed about how dominant temporal rhythms determine, or fail to determine, what actually happens in atmosphere and ocean. But we don’t know anything about whether, and how, those rhythms may have varied in strength over the time span considered. This is where wavelet analysis comes in.\nIn torchwavelets, the central operation is a call to wavelet_transform(), to instantiate an object that takes care of all required operations. One argument is required: signal_length, the number of data points in the series. And one of the defaults we need to override: dt, the time between samples, expressed in the unit we’re working with. In our case, that’s year, and, having monthly samples, we need to pass a value of 1/12. With all other defaults untouched, analysis will be done using the Morlet wavelet (available alternatives are Mexican Hat and Paul), and the transform will be computed in the Fourier domain (the fastest way, unless you have a GPU).\n\nlibrary(torchwavelets)\nenso_idx &lt;- enso$enso %&gt;% as.numeric() %&gt;% torch_tensor()\ndt &lt;- 1/12\nwtf &lt;- wavelet_transform(length(enso_idx), dt = dt)\n\nA call to power() will then compute the wavelet transform:\n\npower_spectrum &lt;- wtf$power(enso_idx)\npower_spectrum$shape\n\n[1]  71 873\nThe result is two-dimensional. The second dimension holds measurement times, i.e., the months between January, 1950 and September, 2022. The first dimension warrants some more explanation.\nNamely, we have here the set of scales the transform has been computed for. If you’re familiar with the Fourier Transform and its analogue, the spectrogram, you’ll probably think in terms of time versus frequency. With wavelets, there is an additional parameter, the scale, that determines the spread of the analysis pattern.\n\nThe aforementioned book excerpt discusses this in detail.\n\nSome wavelets have both a scale and a frequency, in which case these can interact in complex ways. Others are defined such that no separate frequency appears. In the latter case, you immediately end up with the time vs. scale layout we see in wavelet diagrams (scaleograms). In the former, most software hides the complexity by merging scale and frequency into one, leaving just scale as a user-visible parameter. In torchwavelets, too, the wavelet frequency (if existent) has been “streamlined away”. Consequently, we’ll end up plotting time versus scale, as well. I’ll say more when we actually see such a scaleogram.\nFor visualization, we transpose the data and put it into a ggplot-friendly format:\n\ntimes &lt;- lubridate::year(enso$x) + lubridate::month(enso$x) / 12\nscales &lt;- as.numeric(wtf$scales)\n\ndf &lt;- as_tibble(as.matrix(power_spectrum$t()), .name_repair = \"universal\") %&gt;%\n  mutate(time = times) %&gt;%\n  pivot_longer(!time, names_to = \"scale\", values_to = \"power\") %&gt;%\n  mutate(scale = scales[scale %&gt;%\n    str_remove(\"[\\\\.]{3}\") %&gt;%\n    as.numeric()])\ndf %&gt;% glimpse()\n\nRows: 61,983\nColumns: 3\n$ time  &lt;dbl&gt; 1950.083, 1950.083, 1950.083, 1950.083, 195…\n$ scale &lt;dbl&gt; 0.1613356, 0.1759377, 0.1918614, 0.2092263,…\n$ power &lt;dbl&gt; 0.03617507, 0.05985500, 0.07948010, 0.09819…\nThere is one additional piece of information to be incorporated, still: the so-called “cone of influence” (COI). Visually, this is a shading that tells us which part of the plot reflects incomplete, and thus, unreliable and to-be-disregarded, data. Namely, the bigger the scale, the more spread-out the analysis wavelet, and the more incomplete the overlap at the borders of the series when the wavelet slides over the data. You’ll see what I mean in a second.\nThe COI gets its own data frame:\n\ncoi &lt;- wtf$coi(times[1], times[length(enso_idx)])\ncoi_df &lt;- data.frame(x = as.numeric(coi[[1]]), y = as.numeric(coi[[2]]))\n\nAnd now we’re ready to create the scaleogram:\n\nlabeled_scales &lt;- c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64)\nlabeled_frequencies &lt;- round(as.numeric(wtf$fourier_period(labeled_scales)), 1)\n\nggplot(df) +\n  scale_y_continuous(\n    trans = scales::compose_trans(scales::log2_trans(), scales::reverse_trans()),\n    breaks = c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64),\n    limits = c(max(scales), min(scales)),\n    expand = c(0, 0),\n    sec.axis = dup_axis(\n      labels = scales::label_number(labeled_frequencies),\n      name = \"Fourier period (years)\"\n    )\n  ) +\n  ylab(\"scale (years)\") +\n  scale_x_continuous(breaks = seq(1950, 2020, by = 5), expand = c(0, 0)) +\n  xlab(\"year\") +\n  geom_contour_filled(aes(time, scale, z = power), show.legend = FALSE) +\n  scale_fill_viridis_d(option = \"turbo\") +\n  geom_ribbon(data = coi_df, aes(x = x, ymin = y, ymax = max(scales)),\n              fill = \"black\", alpha = 0.6) +\n  theme(legend.position = \"none\")\n\n\n\n\nScaleogram of ENSO data.\n\n\nWhat we see here is how, in ENSO, different rhythms have prevailed over time. Instead of “rhythms”, I could have said “scales”, or “frequencies”, or “periods” – all those translate into one another. Since, to us humans, wavelet scales don’t mean that much, the period (in years) is displayed on an additional y axis on the right.\nSo, we see that in the eighties, an (approximately) four-year period had exceptional influence. Thereafter, yet longer periodicities gained in dominance. And, in accordance with what we expect from prior analysis, there is a basso continuo of annual similarity.\nAlso, note how, at first sight, there seems to have been a decade where a six-year period stood out: right at the beginning of where (for us) measurement starts, in the fifties. However, the dark shading – the COI – tells us that, in this region, the data is not to be trusted.\nSumming up, the two-dimensional analysis nicely complements the more compressed characterization we got from the DFT. Before we move on to the next series, however, let me just quickly address one question, in case you were wondering (if not, just read on, since I won’t be going into details anyway): How is this different from a spectrogram?\nIn a nutshell, the spectrogram splits the data into several “windows”, and computes the DFT independently on all of them. To compute the scaleogram, on the other hand, the analysis wavelet slides continuously over the data, resulting in a spectrum-equivalent for the neighborhood of each sample in the series. With the spectrogram, a fixed window size means that not all frequencies are resolved equally well: The higher frequencies appear more frequently in the interval than the lower ones, and thus, will allow for better resolution. Wavelet analysis, in contrast, is done on a set of scales deliberately arranged so as to capture a broad range of frequencies theoretically visible in a series of given length.\n\nTake a look at the aforementioned book excerpt to see spectrogram and scaleogram compared in a nice application."
  },
  {
    "objectID": "posts/torchwavelets/index.html#analysis-nao",
    "href": "posts/torchwavelets/index.html#analysis-nao",
    "title": "AO, NAO, ENSO: A wavelet analysis example",
    "section": "Analysis: NAO",
    "text": "Analysis: NAO\nThe data file for NAO is in fixed-table format. After conversion to a tsibble, we have:\n\ndownload.file(\n \"https://crudata.uea.ac.uk/cru/data//nao/nao.dat\",\n destfile = \"nao.dat\"\n)\n\n# needed for AO, as well\nuse_months &lt;- seq.Date(\n  from = as.Date(\"1950-01-01\"),\n  to = as.Date(\"2022-09-01\"),\n  by = \"months\"\n)\n\nnao &lt;-\n  read_table(\n    \"nao.dat\",\n    col_names = FALSE,\n    na = \"-99.99\",\n    skip = 3\n  ) %&gt;%\n  select(-X1, -X14) %&gt;%\n  as.matrix() %&gt;%\n  t() %&gt;%\n  as.vector() %&gt;%\n  .[1:length(use_months)] %&gt;%\n  tibble(\n    x = use_months,\n    nao = .\n  ) %&gt;%\n  mutate(x = yearmonth(x)) %&gt;%\n  fill(nao) %&gt;%\n  as_tsibble(index = x)\n\nnao\n\n# A tsibble: 873 x 2 [1M]\n          x   nao\n      &lt;mth&gt; &lt;dbl&gt;\n 1 1950 Jan -0.16\n 2 1950 Feb  0.25\n 3 1950 Mar -1.44\n 4 1950 Apr  1.46\n 5 1950 May  1.34\n 6 1950 Jun -3.94\n 7 1950 Jul -2.75\n 8 1950 Aug -0.08\n 9 1950 Sep  0.19\n10 1950 Oct  0.19\n# … with 863 more rows\nLike before, we start with the spectrum:\n\nfft &lt;- torch_fft_fft(as.numeric(scale(nao$nao)))\n\nnum_samples &lt;- nrow(nao)\nnyquist_cutoff &lt;- ceiling(num_samples / 2)\nbins_below_nyquist &lt;- 0:nyquist_cutoff\n\nsampling_rate &lt;- 12 \nfrequencies_per_bin &lt;- sampling_rate / num_samples\nfrequencies &lt;- frequencies_per_bin * bins_below_nyquist\n\ndf &lt;- data.frame(f = frequencies, y = as.numeric(fft[1:(nyquist_cutoff + 1)]$abs()))\ndf %&gt;% ggplot(aes(f, y)) +\n  geom_line() +\n  xlab(\"frequency (per year)\") +\n  ylab(\"magnitude\") +\n  ggtitle(\"Spectrum of NAO data\")\n\n\n\n\nSpectrum of NAO data, 1950 to present.\n\n\nHave you been wondering for a tiny moment whether this was time-domain data – not spectral? It does look a lot more noisy than the ENSO spectrum for sure. And really, with NAO, predictability is much worse - forecast lead time usually amounts to just one or two weeks.\nProceeding as before, we pick dominant seasonalities (at least this still is possible!) to pass to feasts::STL().\n\nstrongest &lt;- torch_topk(fft[1:(nyquist_cutoff/2)]$abs(), 6)\nstrongest\n\n[[1]]\ntorch_tensor\n102.7191\n80.5129\n76.1179\n75.9949\n72.9086\n60.8281\n[ CPUFloatType{6} ]\n\n[[2]]\ntorch_tensor\n147\n99\n146\n59\n33\n78\n[ CPULongType{6} ]\n\nimportant_freqs &lt;- frequencies[as.numeric(strongest[[2]])]\nimportant_freqs\n\n[1] 2.0068729 1.3470790 1.9931271 0.7972509 0.4398625 1.0584192\n\nnum_observations_in_season &lt;- 12/important_freqs  \nnum_observations_in_season\n\n[1]  5.979452  8.908163  6.020690 15.051724 27.281250 11.337662\nImportant seasonal periods are of length six, nine, eleven, fifteen, and twenty-seven months, approximately - pretty close together indeed! No wonder that, in STL decomposition, the remainder is even more significant than with ENSO:\n\nnao %&gt;%\n  model(STL(nao ~ season(period = 6) + season(period = 9) +\n              season(period = 15) + season(period = 27) +\n              season(period = 12))) %&gt;%\n  components() %&gt;%\n  autoplot()\n\n\n\n\nDecomposition of NAO data into trend, seasonal components, and remainder by feasts::STL().\n\n\nNow, what will we see in terms of temporal evolution? Much of the code that follows is the same as for ENSO, repeated here for the reader’s convenience:\n\nnao_idx &lt;- nao$nao %&gt;% as.numeric() %&gt;% torch_tensor()\ndt &lt;- 1/12 # same interval as for ENSO\nwtf &lt;- wavelet_transform(length(nao_idx), dt = dt)\npower_spectrum &lt;- wtf$power(nao_idx)\n\ntimes &lt;- lubridate::year(nao$x) + lubridate::month(nao$x)/12 # also same\nscales &lt;- as.numeric(wtf$scales) # will be same because both series have same length\n\ndf &lt;- as_tibble(as.matrix(power_spectrum$t()), .name_repair = \"universal\") %&gt;%\n  mutate(time = times) %&gt;%\n  pivot_longer(!time, names_to = \"scale\", values_to = \"power\") %&gt;%\n  mutate(scale = scales[scale %&gt;%\n    str_remove(\"[\\\\.]{3}\") %&gt;%\n    as.numeric()])\n\ncoi &lt;- wtf$coi(times[1], times[length(nao_idx)])\ncoi_df &lt;- data.frame(x = as.numeric(coi[[1]]), y = as.numeric(coi[[2]]))\n\nlabeled_scales &lt;- c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64) # same since scales are same \nlabeled_frequencies &lt;- round(as.numeric(wtf$fourier_period(labeled_scales)), 1)\n\nggplot(df) +\n  scale_y_continuous(\n    trans = scales::compose_trans(scales::log2_trans(), scales::reverse_trans()),\n    breaks = c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64),\n    limits = c(max(scales), min(scales)),\n    expand = c(0, 0),\n    sec.axis = dup_axis(\n      labels = scales::label_number(labeled_frequencies),\n      name = \"Fourier period (years)\"\n    )\n  ) +\n  ylab(\"scale (years)\") +\n  scale_x_continuous(breaks = seq(1950, 2020, by = 5), expand = c(0, 0)) +\n  xlab(\"year\") +\n  geom_contour_filled(aes(time, scale, z = power), show.legend = FALSE) +\n  scale_fill_viridis_d(option = \"turbo\") +\n  geom_ribbon(data = coi_df, aes(x = x, ymin = y, ymax = max(scales)),\n              fill = \"black\", alpha = 0.6) +\n  theme(legend.position = \"none\")\n\n\n\n\nScaleogram of NAO data.\n\n\nThat, literally, is a much more colorful picture than with ENSO! High frequencies are present, and continually dominant, over the whole time period.\nInterestingly, though, we see similarities to ENSO, as well: In both, there is an important pattern, of periodicity four or slightly more years, that exerces influence during the eighties, nineties, and early two-thousands – only with ENSO, it shows peak impact during the nineties, while with NAO, its dominance is most visible in the first decade of this century. Also, both phenomena exhibit a strongly visible peak, of period two years, around 1970. So, is there a close(-ish) connection between both oscillations? This question, of course, is for the domain experts to answer. At least I found a recent study (Scaife et al. (2014)) that not only suggests there is, but uses one (ENSO, the more predictable one) to inform forecasts of the other:\n\nPrevious studies have shown that the El Niño–Southern Oscillation can drive interannual variations in the NAO [Brönnimann et al., 2007] and hence Atlantic and European winter climate via the stratosphere [Bell et al., 2009]. […] this teleconnection to the tropical Paciﬁc is active in our experiments, with forecasts initialized in El Niño/La Niña conditions in November tending to be followed by negative/positive NAO conditions in winter.\n\nWill we see a similar relationship for AO, our third series under investigation? We might expect so, since AO and NAO are closely related (or even, two sides of the same coin)."
  },
  {
    "objectID": "posts/torchwavelets/index.html#analysis-ao",
    "href": "posts/torchwavelets/index.html#analysis-ao",
    "title": "AO, NAO, ENSO: A wavelet analysis example",
    "section": "Analysis: AO",
    "text": "Analysis: AO\nFirst, the data:\n\ndownload.file(\n \"https://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/monthly.ao.index.b50.current.ascii.table\",\n destfile = \"ao.dat\"\n)\n\nao &lt;-\n  read_table(\n    \"ao.dat\",\n    col_names = FALSE,\n    skip = 1\n  ) %&gt;%\n  select(-X1) %&gt;%\n  as.matrix() %&gt;% \n  t() %&gt;%\n  as.vector() %&gt;%\n  .[1:length(use_months)] %&gt;%\n  tibble(x = use_months,\n         ao = .) %&gt;%\n  mutate(x = yearmonth(x)) %&gt;%\n  fill(ao) %&gt;%\n  as_tsibble(index = x) \n\nao\n\n# A tsibble: 873 x 2 [1M]\n          x     ao\n      &lt;mth&gt;  &lt;dbl&gt;\n 1 1950 Jan -0.06 \n 2 1950 Feb  0.627\n 3 1950 Mar -0.008\n 4 1950 Apr  0.555\n 5 1950 May  0.072\n 6 1950 Jun  0.539\n 7 1950 Jul -0.802\n 8 1950 Aug -0.851\n 9 1950 Sep  0.358\n10 1950 Oct -0.379\n# … with 863 more rows\nAnd the spectrum:\n\nfft &lt;- torch_fft_fft(as.numeric(scale(ao$ao)))\n\nnum_samples &lt;- nrow(ao)\nnyquist_cutoff &lt;- ceiling(num_samples / 2)\nbins_below_nyquist &lt;- 0:nyquist_cutoff\n\nsampling_rate &lt;- 12 # per year\nfrequencies_per_bin &lt;- sampling_rate / num_samples\nfrequencies &lt;- frequencies_per_bin * bins_below_nyquist\n\ndf &lt;- data.frame(f = frequencies, y = as.numeric(fft[1:(nyquist_cutoff + 1)]$abs()))\ndf %&gt;% ggplot(aes(f, y)) +\n  geom_line() +\n  xlab(\"frequency (per year)\") +\n  ylab(\"magnitude\") +\n  ggtitle(\"Spectrum of AO data\")\n\n\n\n\nSpectrum of AO data, 1950 to present.\n\n\nWell, this spectrum looks even more random than NAO’s, in that not even a single frequency stands out. For completeness, here is the STL decomposition:\n\nstrongest &lt;- torch_topk(fft[1:(nyquist_cutoff/2)]$abs(), 5)\n\nimportant_freqs &lt;- frequencies[as.numeric(strongest[[2]])]\nimportant_freqs\n# [1] 0.01374570 0.35738832 1.77319588 1.27835052 0.06872852\n\nnum_observations_in_season &lt;- 12/important_freqs  \nnum_observations_in_season\n# [1] 873.000000  33.576923   6.767442   9.387097 174.600000 \n\nao %&gt;%\n  model(STL(ao ~ season(period = 33) + season(period = 7) +\n              season(period = 9) + season(period = 174))) %&gt;%\n  components() %&gt;%\n  autoplot()\n\n\n\n\nDecomposition of NAO data into trend, seasonal components, and remainder by feasts::STL().\n\n\nFinally, what can the scaleogram tell us about dominant patterns?\n\nao_idx &lt;- ao$ao %&gt;% as.numeric() %&gt;% torch_tensor()\ndt &lt;- 1/12 # same interval as for ENSO and NAO\nwtf &lt;- wavelet_transform(length(ao_idx), dt = dt)\npower_spectrum &lt;- wtf$power(ao_idx)\n\ntimes &lt;- lubridate::year(ao$x) + lubridate::month(ao$x)/12 # also same\nscales &lt;- as.numeric(wtf$scales) # will be same because all series have same length\n\ndf &lt;- as_tibble(as.matrix(power_spectrum$t()), .name_repair = \"universal\") %&gt;%\n  mutate(time = times) %&gt;%\n  pivot_longer(!time, names_to = \"scale\", values_to = \"power\") %&gt;%\n  mutate(scale = scales[scale %&gt;%\n    str_remove(\"[\\\\.]{3}\") %&gt;%\n    as.numeric()])\n\ncoi &lt;- wtf$coi(times[1], times[length(ao_idx)])\ncoi_df &lt;- data.frame(x = as.numeric(coi[[1]]), y = as.numeric(coi[[2]]))\n\nlabeled_scales &lt;- c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64) # same since scales are same \nlabeled_frequencies &lt;- round(as.numeric(wtf$fourier_period(labeled_scales)), 1)\n\nggplot(df) +\n  scale_y_continuous(\n    trans = scales::compose_trans(scales::log2_trans(), scales::reverse_trans()),\n    breaks = c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64),\n    limits = c(max(scales), min(scales)),\n    expand = c(0, 0),\n    sec.axis = dup_axis(\n      labels = scales::label_number(labeled_frequencies),\n      name = \"Fourier period (years)\"\n    )\n  ) +\n  ylab(\"scale (years)\") +\n  scale_x_continuous(breaks = seq(1950, 2020, by = 5), expand = c(0, 0)) +\n  xlab(\"year\") +\n  geom_contour_filled(aes(time, scale, z = power), show.legend = FALSE) +\n  scale_fill_viridis_d(option = \"turbo\") +\n  geom_ribbon(data = coi_df, aes(x = x, ymin = y, ymax = max(scales)),\n              fill = \"black\", alpha = 0.6) +\n  theme(legend.position = \"none\")\n\n\n\n\nScaleogram of AO data.\n\n\nHaving seen the overall spectrum, the lack of strongly dominant patterns in the scaleogram does not come as a big surprise. It is tempting – for me, at least – to see a reflection of ENSO around 1970, all the more since by transitivity, AO and ENSO should be related in some way. But here, qualified judgment really is reserved to the experts."
  },
  {
    "objectID": "posts/torchwavelets/index.html#conclusion",
    "href": "posts/torchwavelets/index.html#conclusion",
    "title": "AO, NAO, ENSO: A wavelet analysis example",
    "section": "Conclusion",
    "text": "Conclusion\nLike I said in the beginning, this post would be about inspiration, not technical detail or reportable results. And I hope that inspirational it has been, at least a little bit. If you’re experimenting with wavelets yourself, or plan to – or if you work in the atmospheric sciences, and would like to provide some insight on the above data/phenomena – we’d love to hear from you!\nAs always, thanks for reading!\nPhoto by ActionVance on Unsplash"
  },
  {
    "objectID": "posts/torch-convlstm/index.html",
    "href": "posts/torch-convlstm/index.html",
    "title": "Convolutional LSTM for spatial forecasting",
    "section": "",
    "text": "This post is the first in a loose series exploring forecasting of spatially-determined data over time. By spatially-determined I mean that whatever the quantities we’re trying to predict – be they univariate or multivariate time series, of spatial dimensionality or not – the input data are given on a spatial grid.\nFor example, the input could be atmospheric measurements, such as sea surface temperature or pressure, given at some set of latitudes and longitudes. The target to be predicted could then span that same (or another) grid. Alternatively, it could be a univariate time series, like a meteorological index.\nBut wait a second, you may be thinking. For time-series prediction, we have that time-honored set of recurrent architectures (e.g., LSTM, GRU), right? Right. We do; but, once we feed spatial data to an RNN, treating different locations as different input features, we lose an essential structural relationship. Importantly, we need to operate in both space and time. We want both: recurrence relations and convolutional filters. Enter convolutional RNNs."
  },
  {
    "objectID": "posts/torch-convlstm/index.html#what-to-expect-from-this-post",
    "href": "posts/torch-convlstm/index.html#what-to-expect-from-this-post",
    "title": "Convolutional LSTM for spatial forecasting",
    "section": "What to expect from this post",
    "text": "What to expect from this post\nToday, we won’t jump into real-world applications just yet. Instead, we’ll take our time to build a convolutional LSTM (henceforth: convLSTM) in torch. For one, we have to – there is no official PyTorch implementation.\n(Keras, on the other hand, has one. If you’re interested in quickly playing around with a Keras convLSTM, check out this nice example.)\nWhat’s more, this post can serve as an introduction to building your own modules. This is something you may be familiar with from Keras or not – depending on whether you’ve used custom models or rather, preferred the declarative define -&gt; compile -&gt; fit style. (Yes, I’m implying there’s some transfer going on if one comes to torch from Keras custom training. Syntactic and semantic details may be different, but both share the object-oriented style that allows for great flexibility and control.)\nLast but not least, we’ll also use this as a hands-on experience with RNN architectures (the LSTM, specifically). While the general concept of recurrence may be easy to grasp, it is not necessarily self-evident how those architectures should, or could, be coded. Personally, I find that independent of the framework used, RNN-related documentation leaves me confused. What exactly is being returned from calling an LSTM, or a GRU? (In Keras this depends on how you’ve defined the layer in question.) I suspect that once we’ve decided what we want to return, the actual code won’t be that complicated. Consequently, we’ll take a detour clarifying what it is that torch and Keras are giving us. Implementing our convLSTM will be a lot more straightforward thereafter."
  },
  {
    "objectID": "posts/torch-convlstm/index.html#a-torch-convlstm",
    "href": "posts/torch-convlstm/index.html#a-torch-convlstm",
    "title": "Convolutional LSTM for spatial forecasting",
    "section": "A torch convLSTM",
    "text": "A torch convLSTM\nThe code discussed here may be found on GitHub. (Depending on when you’re reading this, the code in that repository may have evolved though.)\nMy starting point was one of the PyTorch implementations found on the net, namely, this one. If you search for “PyTorch convGRU” or “PyTorch convLSTM”, you will find stunning discrepancies in how these are realized – discrepancies not just in syntax and/or engineering ambition, but on the semantic level, right at the center of what the architectures may be expected to do. As they say, let the buyer beware. (Regarding the implementation I ended up porting, I am confident that while numerous optimizations will be possible, the basic mechanism matches my expectations.)\nWhat do I expect? Let’s approach this task in a top-down way.\n\nInput and output\nThe convLSTM’s input will be a time series of spatial data, each observation being of size (time steps, channels, height, width).\nCompare this with the usual RNN input format, be it in torch or Keras. In both frameworks, RNNs expect tensors of size (timesteps, input_dim)1. input_dim is \\(1\\) for univariate time series and greater than \\(1\\) for multivariate ones. Conceptually, we may match this to convLSTM’s channels dimension: There could be a single channel, for temperature, say – or there could be several, such as for pressure, temperature, and humidity. The two additional dimensions found in convLSTM, height and width, are spatial indexes into the data.\nIn sum, we want to be able to pass data that:\n\nconsist of one or more features,\nevolve in time, and\nare indexed in two spatial dimensions.\n\nHow about the output? We want to be able to return forecasts for as many time steps as we have in the input sequence. This is something that torch RNNs do by default, while Keras equivalents do not. (You have to pass return_sequences = TRUE to obtain that effect.) If we’re interested in predictions for just a single point in time, we can always pick the last time step in the output tensor.\nHowever, with RNNs, it is not all about outputs. RNN architectures also carry through hidden states.\nWhat are hidden states? I carefully phrased that sentence to be as general as possible – deliberately circling around the confusion that, in my view, often arises at this point. We’ll attempt to clear up some of that confusion in a second, but let’s first finish our high-level requirements specification.\nWe want our convLSTM to be usable in different contexts and applications. Various architectures exist that make use of hidden states, most prominently perhaps, encoder-decoder architectures. Thus, we want our convLSTM to return those as well. Again, this is something a torch LSTM does by default, while in Keras it is achieved using return_state = TRUE.\nNow though, it really is time for that interlude. We’ll sort out the ways things are called by both torch and Keras, and inspect what you get back from their respective GRUs and LSTMs.\n\n\nInterlude: Outputs, states, hidden values … what’s what?\nFor this to remain an interlude, I summarize findings on a high level. The code snippets in the appendix show how to arrive at these results. Heavily commented, they probe return values from both Keras and torch GRUs and LSTMs. Running these will make the upcoming summaries seem a lot less abstract.\nFirst, let’s look at the ways you create an LSTM in both frameworks. (I will generally use LSTM as the “prototypical RNN example”, and just mention GRUs when there are differences significant in the context in question.)\nIn Keras, to create an LSTM you may write something like this:\n\nlstm &lt;- layer_lstm(units = 1)\n\nThe torch equivalent would be:\n\nlstm &lt;- nn_lstm(\n  input_size = 2, # number of input features\n  hidden_size = 1 # number of hidden (and output!) features\n)\n\nDon’t focus on torch‘s input_size parameter for this discussion. (It’s the number of features in the input tensor.) The parallel occurs between Keras’ units and torch’s hidden_size. If you’ve been using Keras, you’re probably thinking of units as the thing that determines output size (equivalently, the number of features in the output). So when torch lets us arrive at the same result using hidden_size, what does that mean? It means that somehow we’re specifying the same thing, using different terminology. And it does make sense, since at every time step current input and previous hidden state are added2:\n\\[\n\\mathbf{h}_t = \\mathbf{W}_{x}\\mathbf{x}_t + \\mathbf{W}_{h}\\mathbf{h}_{t-1}\n\\]\nNow, about those hidden states.\nWhen a Keras LSTM is defined with return_state = TRUE, its return value is a structure of three entities called output, memory state, and carry state. In torch, the same entities are referred to as output, hidden state, and cell state. (In torch, we always get all of them.)\nSo are we dealing with three different types of entities? We are not.\nThe cell, or carry state is that special thing that sets apart LSTMs from GRUs deemed responsible for the “long” in “long short-term memory”. Technically, it could be reported to the user at all points in time; as we’ll see shortly though, it is not.\nWhat about outputs and hidden, or memory states? Confusingly, these really are the same thing. Recall that for each input item in the input sequence, we’re combining it with the previous state, resulting in a new state, to be made used of in the next step3:\n\\[\n\\mathbf{h}_t = \\mathbf{W}_{x}\\mathbf{x}_t + \\mathbf{W}_{h}\\mathbf{h}_{t-1}\n\\]\nNow, say that we’re interested in looking at just the final time step – that is, the default output of a Keras LSTM. From that point of view, we can consider those intermediate computations as “hidden”. Seen like that, output and hidden states feel different.\nHowever, we can also request to see the outputs for every time step. If we do so, there is no difference – the outputs (plural) equal the hidden states. This can be verified using the code in the appendix.\nThus, of the three things returned by an LSTM, two are really the same. How about the GRU, then? As there is no “cell state”, we really have just one type of thing left over – call it outputs or hidden states.\nLet’s summarize this in a table.\n\n\n\nTable 1: RNN terminology. Comparing torch-speak and Keras-speak. In row 1, the terms are parameter names. In rows 2 and 3, they are pulled from current documentation.\n\n\n\n\n\n\n\nReferring to this entity:\ntorch says:\nKeras says:\n\n\n\n\nNumber of features in the output\nThis determines both how many output features there are and the dimensionality of the hidden states.\nhidden_size\nunits\n\n\nPer-time-step output; latent state; intermediate state …\nThis could be named “public state” in the sense that we, the users, are able to obtain all values.\nhidden state\nmemory state\n\n\nCell state; inner state … (LSTM only)\nThis could be named “private state” in that we are able to obtain a value only for the last time step. More on that in a second.\ncell state\ncarry state\n\n\n\n\n\nNow, about that public vs. private distinction. In both frameworks, we can obtain outputs (hidden states) for every time step. The cell state, however, we can access only for the very last time step. This is purely an implementation decision. As we’ll see when building our own recurrent module, there are no obstacles inherent in keeping track of cell states and passing them back to the user.\nIf you dislike the pragmatism of this distinction, you can always go with the math. When a new cell state has been computed (based on prior cell state, input, forget, and cell gates – the specifics of which we are not going to get into here), it is transformed to the hidden (a.k.a. output) state making use of yet another, namely, the output gate:\n\\[\nh_t = o_t \\odot \\tanh(c_t)\n\\]\nDefinitely, then, hidden state (output, resp.) builds on cell state, adding additional modeling power.\nNow it is time to get back to our original goal and build that convLSTM. First though, let’s summarize the return values obtainable from torch and Keras.\n\n\n\nTable 2: Contrasting ways of obtaining various return values in torch vs. Keras. Cf. the appendix for complete examples.\n\n\n\n\n\n\n\nTo achieve this goal:\nin torch do:\nin Keras do:\n\n\n\n\naccess all intermediate outputs ( = per-time-step outputs)\nret[[1]]\nreturn_sequences = TRUE\n\n\naccess both “hidden state” (output) and “cell state” from final time step (only!)\nret[[2]]\nreturn_state = TRUE\n\n\naccess all intermediate outputs and the final “cell state”\nboth of the above\nreturn_sequences = TRUE, return_state = TRUE\n\n\naccess all intermediate outputs and “cell states” from all time steps\nno way\nno way\n\n\n\n\n\n\n\nconvLSTM, the plan\nIn both torch and Keras RNN architectures, single time steps are processed by corresponding Cell classes: There is an LSTM Cell matching the LSTM, a GRU Cell matching the GRU, and so on. We do the same for ConvLSTM. In convlstm_cell(), we first define what should happen to a single observation; then in convlstm(), we build up the recurrence logic.\nOnce we’re done, we create a dummy dataset, as reduced-to-the-essentials as can be. With more complex datasets, even artificial ones, chances are that if we don’t see any training progress, there are hundreds of possible explanations. We want a sanity check that, if failed, leaves no excuses. Realistic applications are left to future posts.\n\n\nA single step: convlstm_cell\nOur convlstm_cell’s constructor takes arguments input_dim , hidden_dim, and bias, just like a torch LSTM Cell.\nBut we’re processing two-dimensional input data. Instead of the usual affine combination of new input and previous state, we use a convolution of kernel size kernel_size. Inside convlstm_cell, it is self$conv that takes care of this.\nNote how the channels dimension, which in the original input data would correspond to different variables, is creatively used to consolidate four convolutions into one: Each channel output will be passed to just one of the four cell gates. Once in possession of the convolution output, forward() applies the gate logic, resulting in the two types of states it needs to send back to the caller.\n\nlibrary(torch)\nlibrary(zeallot)\n\nconvlstm_cell &lt;- nn_module(\n  \n  initialize = function(input_dim, hidden_dim, kernel_size, bias) {\n    \n    self$hidden_dim &lt;- hidden_dim\n    \n    padding &lt;- kernel_size %/% 2\n    \n    self$conv &lt;- nn_conv2d(\n      in_channels = input_dim + self$hidden_dim,\n      # for each of input, forget, output, and cell gates\n      out_channels = 4 * self$hidden_dim,\n      kernel_size = kernel_size,\n      padding = padding,\n      bias = bias\n    )\n  },\n  \n  forward = function(x, prev_states) {\n\n    c(h_prev, c_prev) %&lt;-% prev_states\n    \n    combined &lt;- torch_cat(list(x, h_prev), dim = 2)  # concatenate along channel axis\n    combined_conv &lt;- self$conv(combined)\n    c(cc_i, cc_f, cc_o, cc_g) %&lt;-% torch_split(combined_conv, self$hidden_dim, dim = 2)\n    \n    # input, forget, output, and cell gates (corresponding to torch's LSTM)\n    i &lt;- torch_sigmoid(cc_i)\n    f &lt;- torch_sigmoid(cc_f)\n    o &lt;- torch_sigmoid(cc_o)\n    g &lt;- torch_tanh(cc_g)\n    \n    # cell state\n    c_next &lt;- f * c_prev + i * g\n    # hidden state\n    h_next &lt;- o * torch_tanh(c_next)\n    \n    list(h_next, c_next)\n  },\n  \n  init_hidden = function(batch_size, height, width) {\n    \n    list(\n      torch_zeros(batch_size, self$hidden_dim, height, width, device = self$conv$weight$device),\n      torch_zeros(batch_size, self$hidden_dim, height, width, device = self$conv$weight$device))\n  }\n)\n\nNow convlstm_cell has to be called for every time step. This is done by convlstm.\n\n\nIteration over time steps: convlstm\nA convlstm may consist of several layers, just like a torch LSTM. For each layer, we are able to specify hidden and kernel sizes individually.\nDuring initialization, each layer gets its own convlstm_cell. On call, convlstm executes two loops. The outer one iterates over layers. At the end of each iteration, we store the final pair (hidden state, cell state) for later reporting. The inner loop runs over input sequences, calling convlstm_cell at each time step.\nWe also keep track of intermediate outputs, so we’ll be able to return the complete list of hidden_states seen during the process. Unlike a torch LSTM, we do this for every layer.\n\nconvlstm &lt;- nn_module(\n  \n  # hidden_dims and kernel_sizes are vectors, with one element for each layer in n_layers\n  initialize = function(input_dim, hidden_dims, kernel_sizes, n_layers, bias = TRUE) {\n \n    self$n_layers &lt;- n_layers\n    \n    self$cell_list &lt;- nn_module_list()\n    \n    for (i in 1:n_layers) {\n      cur_input_dim &lt;- if (i == 1) input_dim else hidden_dims[i - 1]\n      self$cell_list$append(convlstm_cell(cur_input_dim, hidden_dims[i], kernel_sizes[i], bias))\n    }\n  },\n  \n  # we always assume batch-first\n  forward = function(x) {\n    \n    c(batch_size, seq_len, num_channels, height, width) %&lt;-% x$size()\n   \n    # initialize hidden states\n    init_hidden &lt;- vector(mode = \"list\", length = self$n_layers)\n    for (i in 1:self$n_layers) {\n      init_hidden[[i]] &lt;- self$cell_list[[i]]$init_hidden(batch_size, height, width)\n    }\n    \n    # list containing the outputs, of length seq_len, for each layer\n    # this is the same as h, at each step in the sequence\n    layer_output_list &lt;- vector(mode = \"list\", length = self$n_layers)\n    \n    # list containing the last states (h, c) for each layer\n    layer_state_list &lt;- vector(mode = \"list\", length = self$n_layers)\n\n    cur_layer_input &lt;- x\n    hidden_states &lt;- init_hidden\n    \n    # loop over layers\n    for (i in 1:self$n_layers) {\n      \n      # every layer's hidden state starts from 0 (non-stateful)\n      c(h, c) %&lt;-% hidden_states[[i]]\n      # outputs, of length seq_len, for this layer\n      # equivalently, list of h states for each time step\n      output_sequence &lt;- vector(mode = \"list\", length = seq_len)\n      \n      # loop over time steps\n      for (t in 1:seq_len) {\n        c(h, c) %&lt;-% self$cell_list[[i]](cur_layer_input[ , t, , , ], list(h, c))\n        # keep track of output (h) for every time step\n        # h has dim (batch_size, hidden_size, height, width)\n        output_sequence[[t]] &lt;- h\n      }\n\n      # stack hs for all time steps over seq_len dimension\n      # stacked_outputs has dim (batch_size, seq_len, hidden_size, height, width)\n      # same as input to forward (x)\n      stacked_outputs &lt;- torch_stack(output_sequence, dim = 2)\n      \n      # pass the list of outputs (hs) to next layer\n      cur_layer_input &lt;- stacked_outputs\n      \n      # keep track of list of outputs or this layer\n      layer_output_list[[i]] &lt;- stacked_outputs\n      # keep track of last state for this layer\n      layer_state_list[[i]] &lt;- list(h, c)\n    }\n \n    list(layer_output_list, layer_state_list)\n  }\n    \n)\n\n\n\nCalling the convlstm\nLet’s see the input format expected by convlstm, and how to access its different outputs.\nHere is a suitable input tensor.\n\n# batch_size, seq_len, channels, height, width\nx &lt;- torch_rand(c(2, 4, 3, 16, 16))\n\nFirst we make use of a single layer.\n\nmodel &lt;- convlstm(input_dim = 3, hidden_dims = 5, kernel_sizes = 3, n_layers = 1)\n\nc(layer_outputs, layer_last_states) %&lt;-% model(x)\n\nWe get back a list of length two, which we immediately split up into the two types of output returned: intermediate outputs from all layers, and final states (of both types) for the last layer.\nWith just a single layer, layer_outputs[[1]]holds all of the layer’s intermediate outputs, stacked on dimension two.\n\ndim(layer_outputs[[1]])\n# [1]  2  4  5 16 16\n\nlayer_last_states[[1]]is a list of tensors, the first of which holds the single layer’s final hidden state, and the second, its final cell state.\n\ndim(layer_last_states[[1]][[1]])\n# [1]  2  5 16 16\ndim(layer_last_states[[1]][[2]])\n# [1]  2  5 16 16\n\nFor comparison, this is how return values look for a multi-layer architecture.\n\nmodel &lt;- convlstm(input_dim = 3, hidden_dims = c(5, 5, 1), kernel_sizes = rep(3, 3), n_layers = 3)\nc(layer_outputs, layer_last_states) %&lt;-% model(x)\n\n# for each layer, tensor of size (batch_size, seq_len, hidden_size, height, width)\ndim(layer_outputs[[1]])\n# 2  4  5 16 16\ndim(layer_outputs[[3]])\n# 2  4  1 16 16\n\n# list of 2 tensors for each layer\nstr(layer_last_states)\n# List of 3\n#  $ :List of 2\n#   ..$ :Float [1:2, 1:5, 1:16, 1:16]\n#   ..$ :Float [1:2, 1:5, 1:16, 1:16]\n#  $ :List of 2\n#   ..$ :Float [1:2, 1:5, 1:16, 1:16]\n#   ..$ :Float [1:2, 1:5, 1:16, 1:16]\n#  $ :List of 2\n#   ..$ :Float [1:2, 1:1, 1:16, 1:16]\n#   ..$ :Float [1:2, 1:1, 1:16, 1:16]\n\n# h, of size (batch_size, hidden_size, height, width)\ndim(layer_last_states[[3]][[1]])\n# 2  1 16 16\n\n# c, of size (batch_size, hidden_size, height, width)\ndim(layer_last_states[[3]][[2]])\n# 2  1 16 16\n\nNow we want to sanity-check this module with the simplest-possible dummy data.\n\n\nSanity-checking the convlstm\nWe generate black-and-white “movies” of diagonal beams successively translated in space.\nEach sequence consists of six time steps, and each beam of six pixels. Just a single sequence is created manually. To create that one sequence, we start from a single beam:\n\nlibrary(torchvision)\n\nbeams &lt;- vector(mode = \"list\", length = 6)\nbeam &lt;- torch_eye(6) %&gt;% nnf_pad(c(6, 12, 12, 6)) # left, right, top, bottom\nbeams[[1]] &lt;- beam\n\nUsing torch_roll() , we create a pattern where this beam moves up diagonally, and stack the individual tensors along the timesteps dimension.\n\nfor (i in 2:6) {\n  beams[[i]] &lt;- torch_roll(beam, c(-(i-1),i-1), c(1, 2))\n}\n\ninit_sequence &lt;- torch_stack(beams, dim = 1)\n\nThat’s a single sequence. Thanks to torchvision::transform_random_affine(), we almost effortlessly produce a dataset of a hundred sequences. Moving beams start at random points in the spatial frame, but they all share that upward-diagonal motion.\n\nsequences &lt;- vector(mode = \"list\", length = 100)\nsequences[[1]] &lt;- init_sequence\n\nfor (i in 2:100) {\n  sequences[[i]] &lt;- transform_random_affine(init_sequence, degrees = 0, translate = c(0.5, 0.5))\n}\n\ninput &lt;- torch_stack(sequences, dim = 1)\n\n# add channels dimension\ninput &lt;- input$unsqueeze(3)\ndim(input)\n# [1] 100   6  1  24  24\n\nThat’s it for the raw data. Now we still need a dataset and a dataloader. Of the six time steps, we use the first five as input and try to predict the last one.\n\ndummy_ds &lt;- dataset(\n  \n  initialize = function(data) {\n    self$data &lt;- data\n  },\n  \n  .getitem = function(i) {\n    list(x = self$data[i, 1:5, ..], y = self$data[i, 6, ..])\n  },\n  \n  .length = function() {\n    nrow(self$data)\n  }\n)\n\nds &lt;- dummy_ds(input)\ndl &lt;- dataloader(ds, batch_size = 100)\n\nHere is a tiny-ish convLSTM, trained for motion prediction:\n\nmodel &lt;- convlstm(input_dim = 1, hidden_dims = c(64, 1), kernel_sizes = c(3, 3), n_layers = 2)\n\noptimizer &lt;- optim_adam(model$parameters)\n\nnum_epochs &lt;- 100\n\nfor (epoch in 1:num_epochs) {\n  \n  model$train()\n  batch_losses &lt;- c()\n  \n  for (b in enumerate(dl)) {\n    \n    optimizer$zero_grad()\n    \n    # last-time-step output from last layer\n    preds &lt;- model(b$x)[[2]][[2]][[1]]\n  \n    loss &lt;- nnf_mse_loss(preds, b$y)\n    batch_losses &lt;- c(batch_losses, loss$item())\n    \n    loss$backward()\n    optimizer$step()\n  }\n  \n  if (epoch %% 10 == 0)\n    cat(sprintf(\"\\nEpoch %d, training loss:%3f\\n\", epoch, mean(batch_losses)))\n}\n\nEpoch 10, training loss:0.008522\n\nEpoch 20, training loss:0.008079\n\nEpoch 30, training loss:0.006187\n\nEpoch 40, training loss:0.003828\n\nEpoch 50, training loss:0.002322\n\nEpoch 60, training loss:0.001594\n\nEpoch 70, training loss:0.001376\n\nEpoch 80, training loss:0.001258\n\nEpoch 90, training loss:0.001218\n\nEpoch 100, training loss:0.001171\nLoss decreases, but that in itself is not a guarantee the model has learned anything. Has it? Let’s inspect its forecast for the very first sequence and see.\nFor printing, I’m zooming in on the relevant region in the 24x24-pixel frame. Here is the ground truth for time step six:\n\nb$y[1, 1, 6:15, 10:19]\n\n0  0  0  0  0  0  0  0  0  0\n0  0  0  0  0  0  0  0  0  0\n0  0  1  0  0  0  0  0  0  0\n0  0  0  1  0  0  0  0  0  0\n0  0  0  0  1  0  0  0  0  0\n0  0  0  0  0  1  0  0  0  0\n0  0  0  0  0  0  1  0  0  0\n0  0  0  0  0  0  0  1  0  0\n0  0  0  0  0  0  0  0  0  0\n0  0  0  0  0  0  0  0  0  0\nAnd here is the forecast. This does not look bad at all, given there was neither experimentation nor tuning involved.\n\nround(as.matrix(preds[1, 1, 6:15, 10:19]), 2)\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\n [1,]  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00     0\n [2,] -0.02  0.36  0.01  0.06  0.00  0.00  0.00  0.00  0.00     0\n [3,]  0.00 -0.01  0.71  0.01  0.06  0.00  0.00  0.00  0.00     0\n [4,] -0.01  0.04  0.00  0.75  0.01  0.06  0.00  0.00  0.00     0\n [5,]  0.00 -0.01 -0.01 -0.01  0.75  0.01  0.06  0.00  0.00     0\n [6,]  0.00  0.01  0.00 -0.07 -0.01  0.75  0.01  0.06  0.00     0\n [7,]  0.00  0.01 -0.01 -0.01 -0.07 -0.01  0.75  0.01  0.06     0\n [8,]  0.00  0.00  0.01  0.00  0.00 -0.01  0.00  0.71  0.00     0\n [9,]  0.00  0.00  0.00  0.01  0.01  0.00  0.03 -0.01  0.37     0\n[10,]  0.00  0.00  0.00  0.00  0.00  0.00 -0.01 -0.01 -0.01     0\nThis should suffice for a sanity check. If you made it till the end, thanks for your patience! In the best case, you’ll be able to apply this architecture (or a similar one) to your own data – but even if not, I hope you’ve enjoyed learning about torch model coding and/or RNN weirdness ;-)\nI, for one, am certainly looking forward to exploring convLSTMs on real-world problems in the near future. Thanks for reading!"
  },
  {
    "objectID": "posts/torch-convlstm/index.html#appendix",
    "href": "posts/torch-convlstm/index.html#appendix",
    "title": "Convolutional LSTM for spatial forecasting",
    "section": "Appendix",
    "text": "Appendix\nThis appendix contains the code used to create tables 1 and 2 above.\n\nKeras\n\nLSTM\n\nlibrary(keras)\n\n# batch of 3, with 4 time steps each and a single feature\ninput &lt;- k_random_normal(shape = c(3L, 4L, 1L))\ninput\n\n# default args\n# return shape = (batch_size, units)\nlstm &lt;- layer_lstm(\n  units = 1,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\nlstm(input)\n\n# return_sequences = TRUE\n# return shape = (batch_size, time steps, units)\n#\n# note how for each item in the batch, the value for time step 4 equals that obtained above\nlstm &lt;- layer_lstm(\n  units = 1,\n  return_sequences = TRUE,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n  # bias is by default initialized to 0\n)\nlstm(input)\n\n# return_state = TRUE\n# return shape = list of:\n#                - outputs, of shape: (batch_size, units)\n#                - \"memory states\" for the last time step, of shape: (batch_size, units)\n#                - \"carry states\" for the last time step, of shape: (batch_size, units)\n#\n# note how the first and second list items are identical!\nlstm &lt;- layer_lstm(\n  units = 1,\n  return_state = TRUE,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\nlstm(input)\n\n# return_state = TRUE, return_sequences = TRUE\n# return shape = list of:\n#                - outputs, of shape: (batch_size, time steps, units)\n#                - \"memory\" states for the last time step, of shape: (batch_size, units)\n#                - \"carry states\" for the last time step, of shape: (batch_size, units)\n#\n# note how again, the \"memory\" state found in list item 2 matches the final-time step outputs reported in item 1\nlstm &lt;- layer_lstm(\n  units = 1,\n  return_sequences = TRUE,\n  return_state = TRUE,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\nlstm(input)\n\n\n\nGRU\n\n# default args\n# return shape = (batch_size, units)\ngru &lt;- layer_gru(\n  units = 1,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\ngru(input)\n\n# return_sequences = TRUE\n# return shape = (batch_size, time steps, units)\n#\n# note how for each item in the batch, the value for time step 4 equals that obtained above\ngru &lt;- layer_gru(\n  units = 1,\n  return_sequences = TRUE,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\ngru(input)\n\n# return_state = TRUE\n# return shape = list of:\n#    - outputs, of shape: (batch_size, units)\n#    - \"memory\" states for the last time step, of shape: (batch_size, units)\n#\n# note how the list items are identical!\ngru &lt;- layer_gru(\n  units = 1,\n  return_state = TRUE,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\ngru(input)\n\n# return_state = TRUE, return_sequences = TRUE\n# return shape = list of:\n#    - outputs, of shape: (batch_size, time steps, units)\n#    - \"memory states\" for the last time step, of shape: (batch_size, units)\n#\n# note how again, the \"memory state\" found in list item 2 matches the final-time-step outputs reported in item 1\ngru &lt;- layer_gru(\n  units = 1,\n  return_sequences = TRUE,\n  return_state = TRUE,\n  kernel_initializer = initializer_constant(value = 1),\n  recurrent_initializer = initializer_constant(value = 1)\n)\ngru(input)\n\n\n\n\ntorch\n\nLSTM (non-stacked architecture)\n\nlibrary(torch)\n\n# batch of 3, with 4 time steps each and a single feature\n# we will specify batch_first = TRUE when creating the LSTM\ninput &lt;- torch_randn(c(3, 4, 1))\ninput\n\n# default args\n# return shape = (batch_size, units)\n#\n# note: there is an additional argument num_layers that we could use to specify a stacked LSTM - effectively composing two LSTM modules\n# default for num_layers is 1 though \nlstm &lt;- nn_lstm(\n  input_size = 1, # number of input features\n  hidden_size = 1, # number of hidden (and output!) features\n  batch_first = TRUE # for easy comparability with Keras\n)\n\nnn_init_constant_(lstm$weight_ih_l1, 1)\nnn_init_constant_(lstm$weight_hh_l1, 1)\nnn_init_constant_(lstm$bias_ih_l1, 0)\nnn_init_constant_(lstm$bias_hh_l1, 0)\n\n# returns a list of length 2, namely\n#   - outputs, of shape (batch_size, time steps, hidden_size) - given we specified batch_first\n#       Note 1: If this is a stacked LSTM, these are the outputs from the last layer only.\n#               For our current purpose, this is irrelevant, as we're restricting ourselves to single-layer LSTMs.\n#       Note 2: hidden_size here is equivalent to units in Keras - both specify number of features\n#  - list of:\n#    - hidden state for the last time step, of shape (num_layers, batch_size, hidden_size)\n#    - cell state for the last time step, of shape (num_layers, batch_size, hidden_size)\n#      Note 3: For a single-layer LSTM, the hidden states are already provided in the first list item.\n\nlstm(input)\n\n\n\nGRU (non-stacked architecture)\n\n# default args\n# return shape = (batch_size, units)\n#\n# note: there is an additional argument num_layers that we could use to specify a stacked GRU - effectively composing two GRU modules\n# default for num_layers is 1 though \ngru &lt;- nn_gru(\n  input_size = 1, # number of input features\n  hidden_size = 1, # number of hidden (and output!) features\n  batch_first = TRUE # for easy comparability with Keras\n)\n\nnn_init_constant_(gru$weight_ih_l1, 1)\nnn_init_constant_(gru$weight_hh_l1, 1)\nnn_init_constant_(gru$bias_ih_l1, 0)\nnn_init_constant_(gru$bias_hh_l1, 0)\n\n# returns a list of length 2, namely\n#   - outputs, of shape (batch_size, time steps, hidden_size) - given we specified batch_first\n#       Note 1: If this is a stacked GRU, these are the outputs from the last layer only.\n#               For our current purpose, this is irrelevant, as we're restricting ourselves to single-layer GRUs.\n#       Note 2: hidden_size here is equivalent to units in Keras - both specify number of features\n#  - list of:\n#    - hidden state for the last time step, of shape (num_layers, batch_size, hidden_size)\n#    - cell state for the last time step, of shape (num_layers, batch_size, hidden_size)\n#       Note 3: For a single-layer GRU, these values are already provided in the first list item.\ngru(input)"
  },
  {
    "objectID": "posts/torch-convlstm/index.html#footnotes",
    "href": "posts/torch-convlstm/index.html#footnotes",
    "title": "Convolutional LSTM for spatial forecasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLeaving aside the batch dimension in this discussion.↩︎\nIn theory, it would be possible for them to be of different sizes if the respective weight matrices transformed their operands to the same output size.↩︎\nYes, this is the same formula as above.↩︎"
  },
  {
    "objectID": "posts/group-equivariant-cnn-1/index.html#from-alchemy-to-science-geometric-deep-learning-in-two-minutes",
    "href": "posts/group-equivariant-cnn-1/index.html#from-alchemy-to-science-geometric-deep-learning-in-two-minutes",
    "title": "Upside down, a cat’s still a cat: Evolving image recognition with Geometric Deep Learning",
    "section": "From alchemy to science: Geometric Deep Learning in two minutes",
    "text": "From alchemy to science: Geometric Deep Learning in two minutes\nIn a nutshell, Geometric Deep Learning is all about deriving network structure from two things: the domain, and the task. The posts will go into a lot of detail, but let me give a quick preview here:\n\nBy domain, I’m referring to the underlying physical space, and the way it is represented in the input data. For example, images are usually coded as a two-dimensional grid, with values indicating pixel intensities.1\nThe task is what we’re training the network to do: classification, say, or segmentation. Tasks may be different at different stages in the architecture. At each stage, the task in question will have its word to say about how layer design should look.\n\nFor instance, take MNIST. The dataset consists of images of ten digits, 0 to 10, all gray-scale. The task – unsurprisingly – is to assign each image the digit represented.\nFirst, consider the domain. A \\(7\\) is a \\(7\\) wherever it appears on the grid. We thus need an operation that is translation-equivariant: It flexibly adapts to shifts (translations) in its input. More concretely, in our context, equivariant operations are able to detect some object’s properties even if that object has been moved, vertically and/or horizontally, to another location. Convolution, ubiquitous not just in deep learning, is just such a shift-equivariant operation.\nLet me call special attention to the fact that, in equivariance, the essential thing is that “flexible adaptation.” Translation-equivariant operations do care about an object’s new position; they record a feature not abstractly, but at the object’s new position. To see why this is important, consider the network as a whole. When we compose convolutions, we build a hierarchy of feature detectors. That hierarchy should be functional no matter where in the image. In addition, it has to be consistent: Location information needs to be preserved between layers.\nTerminology-wise, thus, it is important to distinguish equivariance from invariance. An invariant operation, in our context, would still be able to spot a feature wherever it occurs; however, it would happily forget where that feature happened to be. Clearly, then, to build up a hierarchy of features, translation-invariance is not enough.\nWhat we’ve done right now is derive a requirement from the domain, the input grid. What about the task? If, finally, all we’re supposed to do is name the digit, now suddenly location does not matter anymore. In other words, once the hierarchy exists, invariance is enough. In neural networks, pooling is an operation that forgets about (spatial) detail. It only cares about the mean, say, or the maximum value itself. This is what makes it suited to “summing up” information about a region, or a complete image, if at the end we only care about returning a class label.\nIn a nutshell, we were able to formulate a design wishlist based on (1) what we’re given and (2) what we’re tasked with.\nAfter this high-level sketch of Geometric Deep Learning, we zoom in on this series of posts’ designated topic: group-equivariant convolutional neural networks.\nThe why of “equivariant” should not, by now, pose too much of a riddle. What about that “group” prefix, though?"
  },
  {
    "objectID": "posts/group-equivariant-cnn-1/index.html#the-group-in-group-equivariance",
    "href": "posts/group-equivariant-cnn-1/index.html#the-group-in-group-equivariance",
    "title": "Upside down, a cat’s still a cat: Evolving image recognition with Geometric Deep Learning",
    "section": "The “group” in group-equivariance",
    "text": "The “group” in group-equivariance\nAs you may have guessed from the introduction, talking of “principled” and “math-driven”, this really is about groups in the “math sense.” Depending on your background, the last time you heard about groups was in school, and with not even a hint at why they matter. I’m certainly not qualified to summarize the whole richness of what they’re good for, but I hope that by the end of this post, their importance in deep learning will make intuitive sense.\n\nGroups from symmetries\nHere is a square.\n\n\n\n\n\nNow close your eyes.\nNow look again. Did something happen to the square?\n\n\n\n\n\nYou can’t tell. Maybe it was rotated; maybe it was not. On the other hand, what if the vertices were numbered?\n\n\n\n\n\nNow you’d know.\nWithout the numbering, could I have rotated the square in any way I wanted? Evidently not. This would not go through unnoticed:\n\n\n\n\n\nThere are exactly four ways I could have rotated the square without raising suspicion. Those ways can be referred to in different ways; one simple way is by degree of rotation: 90, 180, or 270 degrees. Why not more? Any further addition of 90 degrees would result in a configuration we’ve already seen.\n\n\n\n\n\nThe above picture shows three squares, but I’ve listed three possible rotations. What about the situation on the left, the one I’ve taken as an initial state? It could be reached by rotating 360 degrees (or twice that, or thrice, or …) But the way this is handled, in math, is by treating it as some sort of “null rotation”, analogously to how \\(0\\) acts in addition, \\(1\\) in multiplication, or the identity matrix in linear algebra.\nAltogether, we thus have four actions that could be performed on the square (an un-numbered square!) that would leave it as-is, or invariant. These are called the symmetries of the square. A symmetry, in math/physics, is a quantity that remains the same no matter what happens as time evolves. And this is where groups come in. Groups – concretely, their elements – effectuate actions like rotation.\nBefore I spell out how, let me give another example. Take this sphere.\n\n\n\n\n\nHow many symmetries does a sphere have? Infinitely many. This implies that whatever group is chosen to act on the square, it won’t be much good to represent the symmetries of the sphere.\n\n\nViewing groups through the action lens\nFollowing these examples, let me generalize. Here is typical definition. 2\n\nA group \\(G\\) is a finite or infinite set of elements together with a binary operation (called the group operation) that together satisfy the four fundamental properties of closure, associativity, the identity property, and the inverse property. The operation with respect to which a group is defined is often called the “group operation,” and a set is said to be a group “under” this operation. Elements \\(A\\), \\(B\\), \\(C\\), … with binary operation between \\(A\\) and \\(B\\) denoted \\(AB\\) form a group if\n\nClosure: If \\(A\\) and \\(B\\) are two elements in \\(G\\), then the product \\(AB\\) is also in \\(G\\).\nAssociativity: The defined multiplication is associative, i.e., for all \\(A\\),\\(B\\),\\(C\\) in \\(G\\), \\((AB)C=A(BC)\\).\nIdentity: There is an identity element \\(I\\) (a.k.a. \\(1\\), \\(E\\), or \\(e\\)) such that \\(IA=AI=A\\) for every element \\(A\\) in \\(G\\).\nInverse: There must be an inverse (a.k.a. reciprocal) of each element. Therefore, for each element \\(A\\) of \\(G\\), the set contains an element \\(B=A^{-1}\\) such that \\(AA^{-1}=A^{-1}A=I\\).\n\n\nIn action-speak, group elements specify allowable actions; or more precisely, ones that are distinguishable from each other. Two actions can be composed; that’s the “binary operation”. The requirements now make intuitive sense:\n\nA combination of two actions – two rotations, say – is still an action of the same type (a rotation).\nIf we have three such actions, it doesn’t matter how we group them. (Their order of application has to remain the same, though.)\nOne possible action is always the “null action”. (Just like in life.) As to “doing nothing”, it doesn’t make a difference if that happens before or after a “something”; that “something” is always the final result.\nEvery action needs to have an “undo button”. In the squares example, if I rotate by 180 degrees, and then, by 180 degrees again, I am back in the original state. It is if I had done nothing.\n\nResuming a more “birds-eye view”, what we’ve seen right now is the definition of a group by how its elements act on each other. But if groups are to matter “in the real world”, they need to act on something outside (neural network components, for example). How this works is the topic of the following posts, but I’ll briefly outline the intuition here."
  },
  {
    "objectID": "posts/group-equivariant-cnn-1/index.html#outlook-group-equivariant-cnn",
    "href": "posts/group-equivariant-cnn-1/index.html#outlook-group-equivariant-cnn",
    "title": "Upside down, a cat’s still a cat: Evolving image recognition with Geometric Deep Learning",
    "section": "Outlook: Group-equivariant CNN",
    "text": "Outlook: Group-equivariant CNN\nAbove, we noted that, in image classification, a translation-invariant operation (like convolution) is needed: A \\(1\\) is a \\(1\\) whether moved horizontally, vertically, both ways, or not at all. What about rotations, though? Standing on its head, a digit is still what it is. Conventional convolution does not support this type of action.\nWe can add to our architectural wishlist by specifying a symmetry group. What group? If we wanted to detect squares aligned to the axes, a suitable group would be \\(C_4\\), the cyclic group of order four. (Above, we saw that we needed four elements, and that we could cycle through the group.) If, on the other hand, we don’t care about alignment, we’d want any position to count. In principle, we should end up in the same situation as we did with the sphere. However, images live on discrete grids; there won’t be an unlimited number of rotations in practice.\nWith more realistic applications, we need to think more carefully. Take digits. When is a number “the same”? For one, it depends on the context. Were it about a hand-written address on an envelope, would we accept a \\(7\\) as such had it been rotated by 90 degrees? Maybe. (Although we might wonder what would make someone change ball-pen position for just a single digit.) What about a \\(7\\) standing on its head? On top of similar psychological considerations, we should be seriously unsure about the intended message, and, at least, down-weight the data point were it part of our training set.\nImportantly, it also depends on the digit itself. A \\(6\\), upside-down, is a \\(9\\).\nZooming in on neural networks, there is room for yet more complexity. We know that CNNs build up a hierarchy of features, starting from simple ones, like edges and corners. Even if, for later layers, we may not want rotation equivariance, we would still like to have it in the initial set of layers. (The output layer – we’ve hinted at that already – is to be considered separately in any case, since its requirements result from the specifics of what we’re tasked with.)\nThat’s it for today. Hopefully, I’ve managed to illuminate a bit of why we would want to have group-equivariant neural networks. The question remains: How do we get them? This is what the subsequent posts in the series will be about.\nTill then, and thanks for reading!\nPhoto by Ihor OINUA on Unsplash"
  },
  {
    "objectID": "posts/group-equivariant-cnn-2/index.html#step-1-the-symmetry-group-c_4",
    "href": "posts/group-equivariant-cnn-2/index.html#step-1-the-symmetry-group-c_4",
    "title": "Implementing rotation equivariance: Group-equivariant CNN from scratch",
    "section": "Step 1: The symmetry group \\(C_4\\)",
    "text": "Step 1: The symmetry group \\(C_4\\)\nIn coding a GCNN, the first thing we need to provide is an implementation of the symmetry group we’d like to use. Here, it is \\(C_4\\), the four-element group that rotates by 90 degrees.\nWe can ask gcnn to create one for us, and inspect its elements.\n\n# remotes::install_github(\"skeydan/gcnn\")\nlibrary(gcnn)\nlibrary(torch)\n\nC_4 &lt;- CyclicGroup(order = 4)\nelems &lt;- C_4$elements()\nelems\n\ntorch_tensor\n 0.0000\n 1.5708\n 3.1416\n 4.7124\n[ CPUFloatType{4} ]\nElements are represented by their respective rotation angles: \\(0\\), \\(\\frac{\\pi}{2}\\), \\(\\pi\\), and \\(\\frac{3 \\pi}{2}\\).\nGroups are aware of the identity, and know how to construct an element’s inverse:\n\nC_4$identity\n\ng1 &lt;- elems[2]\nC_4$inverse(g1)\n\ntorch_tensor\n 0\n[ CPUFloatType{1} ]\n\ntorch_tensor\n4.71239\n[ CPUFloatType{} ]\nHere, what we care about most is the group elements’ action. Implementation-wise, we need to distinguish between them acting on each other, and their action on the vector space \\(\\mathbb{R}^2\\), where our input images live. The former part is the easy one: It may simply be implemented by adding angles. In fact, this is what gcnn does when we ask it to let g1 act on g2:\n\ng2 &lt;- elems[3]\n\n# in C_4$left_action_on_H(), H stands for the symmetry group\nC_4$left_action_on_H(torch_tensor(g1)$unsqueeze(1), torch_tensor(g2)$unsqueeze(1))\n\ntorch_tensor\n 4.7124\n[ CPUFloatType{1,1} ]\nWhat’s with the unsqueeze()s? Since \\(C_4\\)’s ultimate raison d’être is to be part of a neural network, left_action_on_H() works with batches of elements, not scalar tensors.\nThings are a bit less straightforward where the group action on \\(\\mathbb{R}^2\\) is concerned. Here, we need the concept of a group representation. This is an involved topic, which we won’t go into here. In our current context, it works about like this: We have an input signal, a tensor we’d like to operate on in some way. (That “some way” will be convolution, as we’ll see soon.) To render that operation group-equivariant, we first have the representation apply the inverse group action to the input. That accomplished, we go on with the operation as though nothing had happened.\nTo give a concrete example, let’s say the operation is a measurement. Imagine a runner, standing at the foot of some mountain trail, ready to run up the climb. We’d like to record their height. One option we have is to take the measurement, then let them run up. Our measurement will be as valid up the mountain as it was down here. Alternatively, we might be polite and not make them wait. Once they’re up there, we ask them to come down, and when they’re back, we measure their height. The result is the same: Body height is equivariant (more than that: invariant, even) to the action of running up or down. (Of course, height is a pretty dull measure. But something more interesting, such as heart rate, would not have worked so well in this example.)\nReturning to the implementation, it turns out that group actions are encoded as matrices. There is one matrix for each group element. For \\(C_4\\), the so-called standard representation is a rotation matrix:\n\\[\n\\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix}\n\\]\nIn gcnn, the function applying that matrix is left_action_on_R2(). Like its sibling, it is designed to work with batches (of group elements as well as \\(\\mathbb{R}^2\\) vectors). Technically, what it does is rotate the grid the image is defined on, and then, re-sample the image. To make this more concrete, that method’s code looks about as follows.\nHere is a goat.\n\nimg_path &lt;- system.file(\"imgs\", \"z.jpg\", package = \"gcnn\")\nimg &lt;- torchvision::base_loader(img_path) |&gt; torchvision::transform_to_tensor()\nimg$permute(c(2, 3, 1)) |&gt; as.array() |&gt; as.raster() |&gt; plot()\n\n\n\n\n\n\nFirst, we call C_4$left_action_on_R2() to rotate the grid.\n\n# Grid shape is [2, 1024, 1024], for a 2d, 1024 x 1024 image.\nimg_grid_R2 &lt;- torch::torch_stack(torch::torch_meshgrid(\n    list(\n      torch::torch_linspace(-1, 1, dim(img)[2]),\n      torch::torch_linspace(-1, 1, dim(img)[3])\n    )\n))\n\n# Transform the image grid with the matrix representation of some group element.\ntransformed_grid &lt;- C_4$left_action_on_R2(C_4$inverse(g1)$unsqueeze(1), img_grid_R2)\n\nSecond, we re-sample the image on the transformed grid. The goat now looks up to the sky.\n\ntransformed_img &lt;- torch::nnf_grid_sample(\n  img$unsqueeze(1), transformed_grid,\n  align_corners = TRUE, mode = \"bilinear\", padding_mode = \"zeros\"\n)\n\ntransformed_img[1,..]$permute(c(2, 3, 1)) |&gt; as.array() |&gt; as.raster() |&gt; plot()"
  },
  {
    "objectID": "posts/group-equivariant-cnn-2/index.html#step-2-the-lifting-convolution",
    "href": "posts/group-equivariant-cnn-2/index.html#step-2-the-lifting-convolution",
    "title": "Implementing rotation equivariance: Group-equivariant CNN from scratch",
    "section": "Step 2: The lifting convolution",
    "text": "Step 2: The lifting convolution\nWe want to make use of existing, efficient torch functionality as much as possible. Concretely, we want to use nn_conv2d(). What we need, though, is a convolution kernel that’s equivariant not just to translation, but also to the action of \\(C_4\\). This can be achieved by having one kernel for each possible rotation.\nImplementing that idea is exactly what LiftingConvolution does. The principle is the same as before: First, the grid is rotated, and then, the kernel (weight matrix) is re-sampled to the transformed grid.\nWhy, though, call this a lifting convolution? The usual convolution kernel operates on \\(\\mathbb{R}^2\\); while our extended version operates on combinations of \\(\\mathbb{R}^2\\) and \\(C_4\\). In math speak, it has been lifted to the semi-direct product \\(\\mathbb{R}^2\\rtimes C_4\\).\n\nlifting_conv &lt;- LiftingConvolution(\n    group = CyclicGroup(order = 4),\n    kernel_size = 5,\n    in_channels = 3,\n    out_channels = 8\n  )\n\nx &lt;- torch::torch_randn(c(2, 3, 32, 32))\ny &lt;- lifting_conv(x)\ny$shape\n\n[1]  2  8  4 28 28\nSince, internally, LiftingConvolution uses an additional dimension to realize the product of translations and rotations, the output is not four-, but five-dimensional."
  },
  {
    "objectID": "posts/group-equivariant-cnn-2/index.html#step-3-group-convolutions",
    "href": "posts/group-equivariant-cnn-2/index.html#step-3-group-convolutions",
    "title": "Implementing rotation equivariance: Group-equivariant CNN from scratch",
    "section": "Step 3: Group convolutions",
    "text": "Step 3: Group convolutions\nNow that we’re in “group-extended space”, we can chain a number of layers where both input and output are group convolution layers. For example:\n\ngroup_conv &lt;- GroupConvolution(\n  group = CyclicGroup(order = 4),\n    kernel_size = 5,\n    in_channels = 8,\n    out_channels = 16\n)\n\nz &lt;- group_conv(y)\nz$shape\n\n[1]  2 16  4 24 24\nAll that remains to be done is package this up. That’s what gcnn::GroupEquivariantCNN() does."
  },
  {
    "objectID": "posts/group-equivariant-cnn-2/index.html#step-4-group-equivariant-cnn",
    "href": "posts/group-equivariant-cnn-2/index.html#step-4-group-equivariant-cnn",
    "title": "Implementing rotation equivariance: Group-equivariant CNN from scratch",
    "section": "Step 4: Group-equivariant CNN",
    "text": "Step 4: Group-equivariant CNN\nWe can call GroupEquivariantCNN() like so.\n\ncnn &lt;- GroupEquivariantCNN(\n    group = CyclicGroup(order = 4),\n    kernel_size = 5,\n    in_channels = 1,\n    out_channels = 1,\n    num_hidden = 2, # number of group convolutions\n    hidden_channels = 16 # number of channels per group conv layer\n)\n\nimg &lt;- torch::torch_randn(c(4, 1, 32, 32))\ncnn(img)$shape\n\n[1] 4 1\nAt casual glance, this GroupEquivariantCNN looks like any old CNN … weren’t it for the group argument.\nNow, when we inspect its output, we see that the additional dimension is gone. That’s because after a sequence of group-to-group convolution layers, the module projects down to a representation that, for each batch item, retains channels only. It thus averages not just over locations – as we normally do – but over the group dimension as well. A final linear layer will then provide the requested classifier output (of dimension out_channels).\nAnd there we have the complete architecture. It is time for a real-world(ish) test."
  },
  {
    "objectID": "posts/group-equivariant-cnn-2/index.html#rotated-digits",
    "href": "posts/group-equivariant-cnn-2/index.html#rotated-digits",
    "title": "Implementing rotation equivariance: Group-equivariant CNN from scratch",
    "section": "Rotated digits!",
    "text": "Rotated digits!\nThe idea is to train two convnets, a “normal” CNN and a group-equivariant one, on the usual MNIST training set. Then, both are evaluated on an augmented test set where each image is randomly rotated by a continuous rotation between 0 and 360 degrees. We don’t expect GroupEquivariantCNN to be “perfect” – not if we equip with \\(C_4\\) as a symmetry group. Strictly, with \\(C_4\\), equivariance extends over four positions only. But we do hope it will perform significantly better than the shift-equivariant-only standard architecture.\nFirst, we prepare the data; in particular, the augmented test set.\n\ndir &lt;- \"/tmp/mnist\"\n\ntrain_ds &lt;- torchvision::mnist_dataset(\n  dir,\n  download = TRUE,\n  transform = torchvision::transform_to_tensor\n)\n\ntest_ds &lt;- torchvision::mnist_dataset(\n  dir,\n  train = FALSE,\n  transform = function(x) {\n    x |&gt;\n      torchvision::transform_to_tensor() |&gt;\n      torchvision::transform_random_rotation(\n        degrees = c(0, 360),\n        resample = 2,\n        fill = 0\n      )\n  }\n)\n\ntrain_dl &lt;- dataloader(train_ds, batch_size = 128, shuffle = TRUE)\ntest_dl &lt;- dataloader(test_ds, batch_size = 128)\n\nHow does it look?\n\ntest_images &lt;- coro::collect(\n  test_dl, 1\n)[[1]]$x[1:32, 1, , ] |&gt; as.array()\n\npar(mfrow = c(4, 8), mar = rep(0, 4), mai = rep(0, 4))\ntest_images |&gt;\n  purrr::array_tree(1) |&gt;\n  purrr::map(as.raster) |&gt;\n  purrr::iwalk(~ {\n    plot(.x)\n  })\n\n\n\n\n\n\nWe first define and train a conventional CNN. It is as similar to GroupEquivariantCNN(), architecture-wise, as possible, and is given twice the number of hidden channels, so as to have comparable capacity overall.\n\n default_cnn &lt;- nn_module(\n   \"default_cnn\",\n   initialize = function(kernel_size, in_channels, out_channels, num_hidden, hidden_channels) {\n     self$conv1 &lt;- torch::nn_conv2d(in_channels, hidden_channels, kernel_size)\n     self$convs &lt;- torch::nn_module_list()\n     for (i in 1:num_hidden) {\n       self$convs$append(torch::nn_conv2d(hidden_channels, hidden_channels, kernel_size))\n     }\n     self$avg_pool &lt;- torch::nn_adaptive_avg_pool2d(1)\n     self$final_linear &lt;- torch::nn_linear(hidden_channels, out_channels)\n   },\n   forward = function(x) {\n     x &lt;- x |&gt;\n       self$conv1() |&gt;\n       (\\(.) torch::nnf_layer_norm(., .$shape[2:4]))() |&gt;\n       torch::nnf_relu()\n     for (i in 1:(length(self$convs))) {\n       x &lt;- x |&gt;\n         self$convs[[i]]() |&gt;\n         (\\(.) torch::nnf_layer_norm(., .$shape[2:4]))() |&gt;\n         torch::nnf_relu()\n     }\n     x &lt;- x |&gt;\n       self$avg_pool() |&gt;\n       torch::torch_squeeze() |&gt;\n       self$final_linear()\n     x\n   }\n )\n\nfitted &lt;- default_cnn |&gt;\n    luz::setup(\n      loss = torch::nn_cross_entropy_loss(),\n      optimizer = torch::optim_adam,\n      metrics = list(\n        luz::luz_metric_accuracy()\n      )\n    ) |&gt;\n    luz::set_hparams(\n      kernel_size = 5,\n      in_channels = 1,\n      out_channels = 10,\n      num_hidden = 4,\n      hidden_channels = 32\n    ) %&gt;%\n    luz::set_opt_hparams(lr = 1e-2, weight_decay = 1e-4) |&gt;\n    luz::fit(train_dl, epochs = 10, valid_data = test_dl) \n\nTrain metrics: Loss: 0.0498 - Acc: 0.9843\nValid metrics: Loss: 3.2445 - Acc: 0.4479\nUnsurprisingly, accuracy on the test set is not that great.\nNext, we train the group-equivariant version.\n\nfitted &lt;- GroupEquivariantCNN |&gt;\n  luz::setup(\n    loss = torch::nn_cross_entropy_loss(),\n    optimizer = torch::optim_adam,\n    metrics = list(\n      luz::luz_metric_accuracy()\n    )\n  ) |&gt;\n  luz::set_hparams(\n    group = CyclicGroup(order = 4),\n    kernel_size = 5,\n    in_channels = 1,\n    out_channels = 10,\n    num_hidden = 4,\n    hidden_channels = 16\n  ) |&gt;\n  luz::set_opt_hparams(lr = 1e-2, weight_decay = 1e-4) |&gt;\n  luz::fit(train_dl, epochs = 10, valid_data = test_dl)\n\nTrain metrics: Loss: 0.1102 - Acc: 0.9667\nValid metrics: Loss: 0.4969 - Acc: 0.8549\nFor the group-equivariant CNN, accuracies on test and training sets are a lot closer. That is a nice result! Let’s wrap up today’s exploit resuming a thought from the first, more high-level post."
  },
  {
    "objectID": "posts/group-equivariant-cnn-2/index.html#a-challenge",
    "href": "posts/group-equivariant-cnn-2/index.html#a-challenge",
    "title": "Implementing rotation equivariance: Group-equivariant CNN from scratch",
    "section": "A challenge",
    "text": "A challenge\nGoing back to the augmented test set, or rather, the samples of digits displayed, we notice a problem. In row two, column four, there is a digit that “under normal circumstances”, should be a 9, but, most probably, is an upside-down 6. (To a human, what suggests this is the squiggle-like thing that seems to be found more often with sixes than with nines.) However, you could ask: does this have to be a problem? Maybe the network just needs to learn the subtleties, the kinds of things a human would spot?\nThe way I view it, it all depends on the context: What really should be accomplished, and how an application is going to be used. With digits on a letter, I’d see no reason why a single digit should appear upside-down; accordingly, full rotation equivariance would be counter-productive. In a nutshell, we arrive at the same canonical imperative advocates of fair, just machine learning keep reminding us of:\n\nAlways think of the way an application is going to be used!\n\nIn our case, though, there is another aspect to this, a technical one. gcnn::GroupEquivariantCNN() is a simple wrapper, in that its layers all make use of the same symmetry group. In principle, there is no need to do this. With more coding effort, different groups can be used depending on a layer’s position in the feature-detection hierarchy.\nHere, let me finally tell you why I chose the goat picture. The goat is seen through a red-and-white fence, a pattern – slightly rotated, due to the viewing angle – made up of squares (or edges, if you like). Now, for such a fence, types of rotation equivariance such as that encoded by \\(C_4\\) make a lot of sense. The goat itself, though, we’d rather not have look up to the sky, the way I illustrated \\(C_4\\) action before. Thus, what we’d do in a real-world image-classification task is use rather flexible layers at the bottom, and increasingly restrained layers at the top of the hierarchy.\nThanks for reading!\nPhoto by Marjan Blan | @marjanblan on Unsplash"
  },
  {
    "objectID": "posts/group-equivariant-cnn-3/index.html#purpose-and-scope-of-this-post",
    "href": "posts/group-equivariant-cnn-3/index.html#purpose-and-scope-of-this-post",
    "title": "Group-equivariant neural networks with escnn",
    "section": "Purpose and scope of this post",
    "text": "Purpose and scope of this post\nHere, I introduce escnn, a PyTorch extension that implements forms of group equivariance for CNNs operating on the plane or in (3d) space. The library is used in various, amply illustrated research papers; it is appropriately documented; and it comes with introductory notebooks both relating the math and exercising the code. Why, then, not just refer to the first notebook, and immediately start using it for some experiment?\nIn fact, this post should – as quite a few texts I’ve written – be regarded as an introduction to an introduction. To me, this topic seems anything but easy, for various reasons. Of course, there’s the math. But as so often in machine learning, you don’t need to go to great depths to be able to apply an algorithm correctly. So if not the math itself, what generates the difficulty? For me, it’s two things.\nFirst, to map my understanding of the mathematical concepts to the terminology used in the library, and from there, to correct use and application. Expressed schematically: We have a concept A, which figures (among other concepts) in technical term (or object class) B. What does my understanding of A tell me about how object class B is to be used correctly? More importantly: How do I use it to best attain my goal C? This first difficulty I’ll address in a very pragmatic way. I’ll neither dwell on mathematical details, nor try to establish the links between A, B, and C in detail. Instead, I’ll present the characters2 in this story by asking what they’re good for.\nSecond – and this will be of relevance to just a subset of readers – the topic of group equivariance, particularly as applied to image processing, is one where visualizations can be of tremendous help. The quaternity3 of conceptual explanation, math, code, and visualization can, together, produce an understanding of emergent-seeming quality… if, and only if, all of these explanation modes “work” for you. (Or if, in an area, a mode that does not wouldn’t contribute that much anyway.) Here, it so happens that from what I saw, several papers have excellent visualizations4, and the same holds for some lecture slides and accompanying notebooks5. But for those among us with limited spatial-imagination capabilities – e.g., people with Aphantasia – these illustrations, intended to help, can be very hard to make sense of themselves. If you’re not one of these, I totally recommend checking out the resources linked in the above footnotes. This text, though, will try to make the best possible use of verbal explanation to introduce the concepts involved, the library, and how to use it.\nThat said, let’s start with the software."
  },
  {
    "objectID": "posts/group-equivariant-cnn-3/index.html#using-escnn",
    "href": "posts/group-equivariant-cnn-3/index.html#using-escnn",
    "title": "Group-equivariant neural networks with escnn",
    "section": "Using escnn",
    "text": "Using escnn\nEscnn depends on PyTorch. Yes, PyTorch, not torch; unfortunately, the library hasn’t been ported to R yet.6 For now, thus, we’ll employ reticulate7 to access the Python objects directly.\nThe way I’m doing this is install escnn in a virtual environment, with PyTorch version 1.13.1. As of this writing, Python 3.11 is not yet supported by one of escnn’s dependencies; the virtual environment thus builds on Python 3.10. As to the library itself, I am using the development version from GitHub, running pip install git+https://github.com/QUVA-Lab/escnn.\nOnce you’re ready, issue\n\nlibrary(reticulate)\n# Verify correct environment is used.\n# Different ways exist to ensure this; I've found most convenient to configure this on\n# a per-project basis in RStudio's project file (&lt;myproj&gt;.Rproj)\npy_config()\n\n# bind to required libraries and get handles to their namespaces\ntorch &lt;- import(\"torch\")\nescnn &lt;- import(\"escnn\")\n\nEscnn loaded, let me introduce its main objects and their roles in the play."
  },
  {
    "objectID": "posts/group-equivariant-cnn-3/index.html#spaces-groups-and-representations-escnngspaces",
    "href": "posts/group-equivariant-cnn-3/index.html#spaces-groups-and-representations-escnngspaces",
    "title": "Group-equivariant neural networks with escnn",
    "section": "Spaces, groups, and representations: escnn$gspaces",
    "text": "Spaces, groups, and representations: escnn$gspaces\nWe start by peeking into gspaces, one of the two sub-modules we are going to make direct use of.\n\ngspaces &lt;- escnn$gspaces\npy_list_attributes(gspaces) |&gt; (\\(vec) grep(\"On\", vec, value = TRUE))() |&gt; sort()\n\n[1] \"conicalOnR3\" \"cylindricalOnR3\" \"dihedralOnR3\" \"flip2dOnR2\" \"flipRot2dOnR2\" \"flipRot3dOnR3\"\n[7] \"fullCylindricalOnR3\" \"fullIcoOnR3\" \"fullOctaOnR3\" \"icoOnR3\" \"invOnR3\" \"mirOnR3 \"octaOnR3\"\n[14] \"rot2dOnR2\" \"rot2dOnR3\" \"rot3dOnR3\" \"trivialOnR2\" \"trivialOnR3\"    \nThe methods I’ve listed instantiate a gspace. If you look closely, you see that they’re all composed of two strings, joined by “On”. In all instances, the second part is either R2 or R3. These two are the available base spaces – \\(\\mathbb{R}^2\\) and \\(\\mathbb{R}^3\\) – an input signal can live in. Signals can, thus, be images, made up of pixels, or three-dimensional volumes, composed of voxels. The first part refers to the group you’d like to use. Choosing a group means choosing the symmetries to be respected. For example, rot2dOnR2() implies equivariance as to rotations, flip2dOnR2() guarantees the same for mirroring actions, and flipRot2dOnR2() subsumes both.\nLet’s define such a gspace. Here we ask for rotation equivariance on the Euclidean plane, making use of the same cyclic group – \\(C_4\\) – we developed in our from-scratch implementation:\n\nr2_act &lt;- gspaces$rot2dOnR2(N = 4L)\nr2_act$fibergroup\n\nIn this post, I’ll stay with that setup, but we could as well pick another rotation angle – N = 8, say, resulting in eight equivariant positions separated by forty-five degrees. Alternatively, we might want any rotated position to be accounted for. The group to request then would be SO(2), called the special orthogonal group, of continuous, distance- and orientation-preserving transformations on the Euclidean plane:\n\n(gspaces$rot2dOnR2(N = -1L))$fibergroup\n\nSO(2)\nGoing back to \\(C_4\\), let’s investigate its representations:\n\nr2_act$representations\n\n$irrep_0\nC4|[irrep_0]:1\n\n$irrep_1\nC4|[irrep_1]:2\n\n$irrep_2\nC4|[irrep_2]:1\n\n$regular\nC4|[regular]:4\nA representation, in our current context and very roughly speaking, is a way to encode a group action as a matrix, meeting certain conditions. In escnn, representations are central, and we’ll see how in the next section.\nFirst, let’s inspect the above output. Four representations are available, three of which share an important property: they’re all irreducible. On \\(C_4\\), any non-irreducible representation can be decomposed into into irreducible ones. These irreducible representations are what escnn works with internally. Of those three, the most interesting one is the second. To see its action, we need to choose a group element. How about counterclockwise rotation by ninety degrees:\n\nelem_1 &lt;- r2_act$fibergroup$element(1L)\nelem_1\n\n1[2pi/4]\nAssociated to this group element is the following matrix:\n\nr2_act$representations[[2]](elem_1)\n\n             [,1]          [,2]\n[1,] 6.123234e-17 -1.000000e+00\n[2,] 1.000000e+00  6.123234e-17\nThis is the so-called standard representation,\n\\[\n\\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix}\n\\]\n, evaluated at \\(\\theta = \\pi/2\\). (It is called the standard representation because it directly comes from how the group is defined (namely, a rotation by \\(\\theta\\) in the plane).\nThe other interesting representation to point out is the fourth: the only one that’s not irreducible.\n\nr2_act$representations[[4]](elem_1)\n\n[1,]  5.551115e-17 -5.551115e-17 -8.326673e-17  1.000000e+00\n[2,]  1.000000e+00  5.551115e-17 -5.551115e-17 -8.326673e-17\n[3,]  5.551115e-17  1.000000e+00  5.551115e-17 -5.551115e-17\n[4,] -5.551115e-17  5.551115e-17  1.000000e+00  5.551115e-17\nThis is the so-called regular representation. The regular representation acts via permutation of group elements, or, to be more precise, of the basis vectors that make up the matrix. Obviously, this is only possible for finite groups like \\(C_n\\), since otherwise there’d be an infinite amount of basis vectors to permute.\nTo better see the action encoded in the above matrix, we clean up a bit:\n\nround(r2_act$representations[[4]](elem_1))\n\n    [,1] [,2] [,3] [,4]\n[1,]    0    0    0    1\n[2,]    1    0    0    0\n[3,]    0    1    0    0\n[4,]    0    0    1    0\nThis is a step-one shift to the right of the identity matrix. The identity matrix, mapped to element 0, is the non-action; this matrix instead maps the zeroth action to the first, the first to the second, the second to the third, and the third to the first.\nWe’ll see the regular representation used in a neural network soon. Internally – but that need not concern the user – escnn works with its decomposition into irreducible matrices. Here, that’s just the bunch of irreducible representations we saw above, numbered from one to three.\nHaving looked at how groups and representations figure in escnn, it is time we approach the task of building a network."
  },
  {
    "objectID": "posts/group-equivariant-cnn-3/index.html#representations-for-real-escnnnnfieldtype",
    "href": "posts/group-equivariant-cnn-3/index.html#representations-for-real-escnnnnfieldtype",
    "title": "Group-equivariant neural networks with escnn",
    "section": "Representations, for real: escnn$nn$FieldType",
    "text": "Representations, for real: escnn$nn$FieldType\nSo far, we’ve characterized the input space (\\(\\mathbb{R}^2\\)), and specified the group action. But once we enter the network, we’re not in the plane anymore, but in a space that has been extended by the group action. Rephrasing, the group action produces feature vector fields that assign a feature vector to each spatial position in the image.\nNow we have those feature vectors, we need to specify how they transform under the group action. This is encoded in an escnn$nn$FieldType . Informally, we could say that a field type is the data type of a feature space. In defining it, we indicate two things: the base space, a gspace, and the representation type(s) to be used.\nIn an equivariant neural network, field types play a role similar to that of channels in a convnet. Each layer has an input and an output field type. Assuming we’re working with grey-scale images, we can specify the input type for the first layer like this:\n\nnn &lt;- escnn$nn\nfeat_type_in &lt;- nn$FieldType(r2_act, list(r2_act$trivial_repr))\n\nThe trivial representation is used to indicate that, while the image as a whole will be rotated, the pixel values themselves should be left alone. If this were an RGB image, instead of r2_act$trivial_repr we’d pass a list of three such objects.\nSo we’ve characterized the input. At any later stage, though, the situation will have changed. We will have performed convolution once for every group element. Moving on to the next layer, these feature fields will have to transform equivariantly, as well. This can be achieved by requesting the regular representation for an output field type:\n\nfeat_type_out &lt;- nn$FieldType(r2_act, list(r2_act$regular_repr))\n\nThen, a convolutional layer may be defined like so:\n\nconv &lt;- nn$R2Conv(feat_type_in, feat_type_out, kernel_size = 3L)"
  },
  {
    "objectID": "posts/group-equivariant-cnn-3/index.html#group-equivariant-convolution",
    "href": "posts/group-equivariant-cnn-3/index.html#group-equivariant-convolution",
    "title": "Group-equivariant neural networks with escnn",
    "section": "Group-equivariant convolution",
    "text": "Group-equivariant convolution\nWhat does such a convolution do to its input? Just like, in a usual convnet, capacity can be increased by having more channels, an equivariant convolution can pass on several feature vector fields, possibly of different type (assuming that makes sense). In the code snippet below, we request a list of three, all behaving according to the regular representation.\n\nfeat_type_in &lt;- nn$FieldType(r2_act, list(r2_act$trivial_repr))\nfeat_type_out &lt;- nn$FieldType(\n  r2_act,\n  list(r2_act$regular_repr, r2_act$regular_repr, r2_act$regular_repr)\n)\n\nconv &lt;- nn$R2Conv(feat_type_in, feat_type_out, kernel_size = 3L)\n\nWe then perform convolution on a batch of images, made aware of their “data type” by wrapping them in feat_type_in:\n\nx &lt;- torch$rand(2L, 1L, 32L, 32L)\nx &lt;- feat_type_in(x)\ny &lt;- conv(x)\ny$shape |&gt; unlist()\n\n[1]  2  12 30 30\nThe output has twelve “channels”, this being the product of group cardinality – four distinguished positions – and number of feature vector fields (three).\nIf we choose the simplest possible, approximately, test case, we can verify that such a convolution is equivariant by direct inspection. Here’s my setup:\n\nfeat_type_in &lt;- nn$FieldType(r2_act, list(r2_act$trivial_repr))\nfeat_type_out &lt;- nn$FieldType(r2_act, list(r2_act$regular_repr))\nconv &lt;- nn$R2Conv(feat_type_in, feat_type_out, kernel_size = 3L)\n\ntorch$nn$init$constant_(conv$weights, 1.)\nx &lt;- torch$vander(torch$arange(0,4))$view(tuple(1L, 1L, 4L, 4L)) |&gt; feat_type_in()\nx\n\ng_tensor([[[[ 0.,  0.,  0.,  1.],\n            [ 1.,  1.,  1.,  1.],\n            [ 8.,  4.,  2.,  1.],\n            [27.,  9.,  3.,  1.]]]], [C4_on_R2[(None, 4)]: {irrep_0 (x1)}(1)])\nInspection could be performed using any group element. I’ll pick rotation by \\(\\pi/2\\):\n\nall &lt;- iterate(r2_act$testing_elements)\ng1 &lt;- all[[2]]\ng1\n\nJust for fun, let’s see how we can – literally – come whole circle by letting this element act on the input tensor four times:\n\nall &lt;- iterate(r2_act$testing_elements)\ng1 &lt;- all[[2]]\n\nx1 &lt;- x$transform(g1)\nx1$tensor\nx2 &lt;- x1$transform(g1)\nx2$tensor\nx3 &lt;- x2$transform(g1)\nx3$tensor\nx4 &lt;- x3$transform(g1)\nx4$tensor\n\ntensor([[[[ 1.,  1.,  1.,  1.],\n          [ 0.,  1.,  2.,  3.],\n          [ 0.,  1.,  4.,  9.],\n          [ 0.,  1.,  8., 27.]]]])\n          \ntensor([[[[ 1.,  3.,  9., 27.],\n          [ 1.,  2.,  4.,  8.],\n          [ 1.,  1.,  1.,  1.],\n          [ 1.,  0.,  0.,  0.]]]])\n          \ntensor([[[[27.,  8.,  1.,  0.],\n          [ 9.,  4.,  1.,  0.],\n          [ 3.,  2.,  1.,  0.],\n          [ 1.,  1.,  1.,  1.]]]])\n          \ntensor([[[[ 0.,  0.,  0.,  1.],\n          [ 1.,  1.,  1.,  1.],\n          [ 8.,  4.,  2.,  1.],\n          [27.,  9.,  3.,  1.]]]])\nYou see that at the end, we are back at the original “image”.\nNow, for equivariance. We could first apply a rotation, then convolve.\nRotate:\n\nx_rot &lt;- x$transform(g1)\nx_rot$tensor\n\nThis is the first in the above list of four tensors.\nConvolve:\n\ny &lt;- conv(x_rot)\ny$tensor\n\ntensor([[[[ 1.1955,  1.7110],\n          [-0.5166,  1.0665]],\n\n         [[-0.0905,  2.6568],\n          [-0.3743,  2.8144]],\n\n         [[ 5.0640, 11.7395],\n          [ 8.6488, 31.7169]],\n\n         [[ 2.3499,  1.7937],\n          [ 4.5065,  5.9689]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\nAlternatively, we can do the convolution first, then rotate its output.\nConvolve:\n\ny_conv &lt;- conv(x)\ny_conv$tensor\n\ntensor([[[[-0.3743, -0.0905],\n          [ 2.8144,  2.6568]],\n\n         [[ 8.6488,  5.0640],\n          [31.7169, 11.7395]],\n\n         [[ 4.5065,  2.3499],\n          [ 5.9689,  1.7937]],\n\n         [[-0.5166,  1.1955],\n          [ 1.0665,  1.7110]]]], grad_fn=&lt;ConvolutionBackward0&gt;)\nRotate:\n\ny &lt;- y_conv$transform(g1)\ny$tensor\n\ntensor([[[[ 1.1955,  1.7110],\n          [-0.5166,  1.0665]],\n\n         [[-0.0905,  2.6568],\n          [-0.3743,  2.8144]],\n\n         [[ 5.0640, 11.7395],\n          [ 8.6488, 31.7169]],\n\n         [[ 2.3499,  1.7937],\n          [ 4.5065,  5.9689]]]])\nIndeed, final results are the same.\nAt this point, we know how to employ group-equivariant convolutions. The final step is to compose the network."
  },
  {
    "objectID": "posts/group-equivariant-cnn-3/index.html#a-group-equivariant-neural-network",
    "href": "posts/group-equivariant-cnn-3/index.html#a-group-equivariant-neural-network",
    "title": "Group-equivariant neural networks with escnn",
    "section": "A group-equivariant neural network",
    "text": "A group-equivariant neural network\nBasically, we have two questions to answer. The first concerns the non-linearities; the second is how to get from extended space to the data type of the target.\nFirst, about the non-linearities. This is a potentially intricate topic, but as long as we stay with point-wise operations (such as that performed by ReLU) equivariance is given intrinsically.\nIn consequence, we can already assemble a model:\n\nfeat_type_in &lt;- nn$FieldType(r2_act, list(r2_act$trivial_repr))\nfeat_type_hid &lt;- nn$FieldType(\n  r2_act,\n  list(r2_act$regular_repr, r2_act$regular_repr, r2_act$regular_repr, r2_act$regular_repr)\n  )\nfeat_type_out &lt;- nn$FieldType(r2_act, list(r2_act$regular_repr))\n\nmodel &lt;- nn$SequentialModule(\n  nn$R2Conv(feat_type_in, feat_type_hid, kernel_size = 3L),\n  nn$InnerBatchNorm(feat_type_hid),\n  nn$ReLU(feat_type_hid),\n  nn$R2Conv(feat_type_hid, feat_type_hid, kernel_size = 3L),\n  nn$InnerBatchNorm(feat_type_hid),\n  nn$ReLU(feat_type_hid),\n  nn$R2Conv(feat_type_hid, feat_type_out, kernel_size = 3L)\n)$eval()\n\nmodel\n\nSequentialModule(\n  (0): R2Conv([C4_on_R2[(None, 4)]:\n       {irrep_0 (x1)}(1)], [C4_on_R2[(None, 4)]: {regular (x4)}(16)], kernel_size=3, stride=1)\n  (1): InnerBatchNorm([C4_on_R2[(None, 4)]:\n       {regular (x4)}(16)], eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace=False, type=[C4_on_R2[(None, 4)]: {regular (x4)}(16)])\n  (3): R2Conv([C4_on_R2[(None, 4)]:\n       {regular (x4)}(16)], [C4_on_R2[(None, 4)]: {regular (x4)}(16)], kernel_size=3, stride=1)\n  (4): InnerBatchNorm([C4_on_R2[(None, 4)]:\n       {regular (x4)}(16)], eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (5): ReLU(inplace=False, type=[C4_on_R2[(None, 4)]: {regular (x4)}(16)])\n  (6): R2Conv([C4_on_R2[(None, 4)]:\n       {regular (x4)}(16)], [C4_on_R2[(None, 4)]: {regular (x1)}(4)], kernel_size=3, stride=1)\n)\nCalling this model on some input image, we get:\n\nx &lt;- torch$randn(1L, 1L, 17L, 17L)\nx &lt;- feat_type_in(x)\nmodel(x)$shape |&gt; unlist()\n\n[1]  1  4 11 11\nWhat we do now depends on the task. Since we didn’t preserve the original resolution anyway – as would have been required for, say, segmentation – we probably want one feature vector per image. That we can achieve by spatial pooling:\n\navgpool &lt;- nn$PointwiseAvgPool(feat_type_out, 11L)\ny &lt;- avgpool(model(x))\ny$shape |&gt; unlist()\n\n[1] 1 4 1 1\nWe still have four “channels”, corresponding to four group elements. This feature vector is (approximately) translation-invariant, but rotation-equivariant, in the sense expressed by the choice of group. Often, the final output will be expected to be group-invariant as well as translation-invariant (as in image classification). If that’s the case, we pool over group elements, as well:\n\ninvariant_map &lt;- nn$GroupPooling(feat_type_out)\ny &lt;- invariant_map(avgpool(model(x)))\ny$tensor\n\ntensor([[[[-0.0293]]]], grad_fn=&lt;CopySlices&gt;)\nWe end up with an architecture that, from the outside, will look like a standard convnet, while on the inside, all convolutions have been performed in a rotation-equivariant way. Training and evaluation then are no different from the usual procedure."
  },
  {
    "objectID": "posts/group-equivariant-cnn-3/index.html#where-to-from-here",
    "href": "posts/group-equivariant-cnn-3/index.html#where-to-from-here",
    "title": "Group-equivariant neural networks with escnn",
    "section": "Where to from here",
    "text": "Where to from here\nThis “introduction to an introduction” has been the attempt to draw a high-level map of the terrain, so you can decide if this is useful to you. If it’s not just useful, but interesting theory-wise as well, you’ll find lots of excellent materials linked from the README. The way I see it, though, this post already should enable you to actually experiment with different setups.\nOne such experiment, that would be of high interest to me, might investigate how well different types and degrees of equivariance actually work for a given task and dataset. Overall, a reasonable assumption is that, the higher “up” we go in the feature hierarchy, the less equivariance we require. For edges and corners, taken by themselves, full rotation equivariance seems desirable, as does equivariance to reflection; for higher-level features, we might want to successively restrict allowed operations, maybe ending up with equivariance to mirroring merely. Experiments could be designed to compare different ways, and levels, of restriction.8\nThanks for reading!\nPhoto by Volodymyr Tokar on Unsplash"
  },
  {
    "objectID": "posts/llm-intro/index.html#large-language-models-what-they-are",
    "href": "posts/llm-intro/index.html#large-language-models-what-they-are",
    "title": "What are Large Language Models? What are they not?",
    "section": "Large Language Models: What they are",
    "text": "Large Language Models: What they are\nHow is it even possible to build a machine that talks to you? One way is to have that machine listen a lot. And listen is what these machines do; they do it a lot. But listening alone would never be enough to attain results as impressive as those we see. Instead, LLMs practice some form of “maximally active listening”: Continuously, they try to predict the speaker’s next utterance. By “continuously”, I mean word-by-word: At each training step, the model is asked to produce the subsequent word in a text.\nMaybe in my last sentence, you noted the term “train”. As per common sense, “training” implies some form of supervision. It also implies some form of method. Since learning material is scraped from the internet, the true continuation is always known. The precondition for supervision is thus always fulfilled: A supervisor can just compare model prediction with what really follows in the text. Remains the question of method. That’s where we need to talk about deep learning, and we’ll do that in Model training.\n\nOverall architecture\nToday’s LLMs are, in some way or the other, based on an architecture known as the Transformer. This architecture was originally introduced in a paper catchily titled “Attention is all you need” (Vaswani et al. 2017). Of course, this was not the first attempt at automating natural-language generation – not even in deep learning, the sub-type of machine learning whose defining characteristic are many-layered (“deep”) artificial neural networks. But there, in deep learning, it constituted some kind of paradigm change. Before, models designed to solve sequence-prediction tasks (time-series forecasting, text generation…) tended to be based on some form of recurrent architecture, introduced in the 1990’s (eternities ago, on the time scale of deep-learning) by (Hochreiter and Schmidhuber 1997). Basically, the concept of recurrence, with its associated threading of a latent state, was replaced by “attention”. That’s what the paper’s title was meant to communicate: The authors did not introduce “attention”2; instead, they fundamentally expanded its usage so as to render recurrence superfluous.\nHow did that ancestral Transformer look? – One prototypical task in natural language processing is machine translation. In translation, be it done by a machine or by a human, there is an input (in one language) and an output (in another). That input, call it a code. Whoever wants to establish its counterpart in the target language first needs to decode it. Indeed, one of two top-level building blocks of the archetypal Transformer was a decoder, or rather, a stack of decoders applied in succession. At its end, out popped a phrase in the target language3. What, then, was the other high-level block? It was an encoder, something that takes text (or tokens, rather, i.e., something that has undergone tokenization) and converts it into a form the decoder can make sense of. (Obviously, there is no analogue to this in human translation.)\nFrom this two-stack architecture, subsequent developments tended to keep just one. The GPT family, together with many others, just kept the decoder stack. Now, doesn’t the decoder need some kind of input – if not to translate to a different language, then to reply to, as in the chatbot scenario? Turns out that no, it doesn’t – and that’s why you can also have the bot initiate the conversation. Unbeknownst to you, there will, in fact, be an input to the model – some kind of token signifying “end of input”. In that case, the model will draw on its training experience to generate a word likely to start out a phrase. That one word will then become the new input to continue from, and so forth. Summing up so far, then, GPT-like LLMs are Transformer Decoders.\nThe question is, how does such a stack of decoders succeed in fulfilling the task?\n\n\nGPT-type models up close\nIn opening the black box, we focus on its two interfaces – input and output – as well as on the internals, its core.\n\nInput\nFor simplicity, let me speak of words, not tokens. Now imagine a machine that is to work with – more even: “understand”4 – words. For a computer to process non-numeric data, a conversion to numbers necessarily has to happen. The straightforward way to effectuate this is to decide on a fixed lexicon, and assign each word a number. And this works: The way deep neural networks are trained, they don’t need semantic relationships to exist between entities in the training data to memorize formal structure. Does this mean they will appear perfect while training, but fail in real-world prediction? – If the training data are representative of how we converse, all will be fine. In a world of perfect surveillance, machines could exist that have internalized our every spoken word. Before that happens, though, the training data will be imperfect.\nA much more promising approach than to simply index words, then, is to represent them in a richer, higher-dimensional space, an embedding space. This idea, popular not just in deep learning but in natural language processing overall, really goes far beyond anything domain-specific – linguistic entities, say5. You may be able to fruitfully employ it in virtually any domain – provided you can devise a method to sensibly map the given data into that space. In deep learning, these embeddings are obtained in a clever way: as a by-product of sorts of the overall training workflow. Technically, this is achieved by means of a dedicated neural-network layer6 tasked with evolving these mappings. Note how, smart though this strategy may be, it implies that the overall setting – everything from training data via model architecture to optimization algorithms employed – necessarily affects the resulting embeddings. And since these may be extracted and made use of in down-stream tasks, this matters7.\nAs to the GPT family, such an embedding layer constitutes part of its input interface – one “half”, so to say. Technically, the second makes use of the same type of layer, but with a different purpose. To contrast the two, let me spell out clearly what, in the part we’ve talked about already, is getting mapped to what. The mapping is between a word index – a sequence 1, 2, …, &lt;vocabulary size&gt; – on the one hand and a set of continuous-valued vectors of some length – 100, say – on the other. (One of them could like this: \\(\\begin{bmatrix} 1.002 & 0.71 & 0.0004 &...\\\\ \\end{bmatrix}\\)) Thus, we obtain an embedding for every word. But language is more than an unordered assembly of words. Rearranging words, if syntactically allowed, may result in drastically changed semantics. In the pre-transformer paradigma, threading a sequentially-updated hidden state took care of this. Put differently, in that type of model, information about input order never got lost throughout the layers. Transformer-type architectures, however, need to find a different way. Here, a variety of rivaling methods exists. Some assume an underlying periodicity in semanto-syntactic structure. Others – and the GPT family, as yet and insofar we know, has been part of them8 – approach the challenge in exactly the same way as for the lexical units: They make learning these so-called position embeddings a by-product of model training. Implementation-wise, the only difference is that now the input to the mapping looks like this: 1, 2, …, &lt;maximum position&gt; where “maximum position” reflects choice of maximal sequence length supported.\nSumming up, verbal input is thus encoded – embedded, enriched – twofold as it enters the machine. The two types of embedding are combined and passed on to the model core, the already-mentioned decoder stack.\n\n\nCore Processing\nThe decoder stack is made up of some number of identical blocks (12, in the case of GPT-2). (By “identical” I mean that the architecture is the same; the weights – the place where a neural-network layer stores what it “knows” – are not. More on these “weights” soon.)\nInside each block, some sub-layers are pretty much “business as usual”. One is not: the attention module, the “magic” ingredient that enabled Transformer-based architectures to forego keeping a latent state. To explain how this works, let’s take translation as an example.\nIn the classical encoder-decoder setup, the one most intuitive for machine translation, imagine the very first decoder in the stack of decoders. It receives as input a length-seven cypher, the encoded version of an original length-seven phrase. Since, due to how the encoder blocks are built, input order is conserved, we have a faithful representation of source-language word order. In the target language, however, word order can be very different. A decoder module, in producing the translation, had rather not do this by translating each word as it appears. Instead, it would be desirable for it to know which among the already-seen tokens is most relevant right now, to generate the very next output token. Put differently, it had better know where to direct its attention.\nThus, figure out how to distribute focus is what attention modules do. How do they do it? They compute, for each available input-language token, how good a match, a fit, it is for their own current input. Remember that every token, at every processing stage, is encoded as a vector of continuous values. How good a match any of, say, three source-language vectors is is then computed by projecting one’s current input vector onto each of the three. The closer the vectors, the longer the projected vector. 9 Based on the projection onto each source-input token, that token is weighted, and the attention module passes on the aggregated assessments to the ensuing neural-network module.\nTo explain what attention modules are for, I’ve made use of the machine-translation scenario, a scenario that should lend a certain intuitiveness to the operation. But for GPT-family models, we need to abstract this a bit. First, there is no encoder stack, so “attention” is computed among decoder-resident tokens only. And second – remember I said a stack was built up of identical modules? – this happens in every decoder block. That is, when intermediate results are bubbled up the stack, at each stage the input is weighted as appropriate at that stage. While this is harder to intuit than what happened in the translation scenario, I’d argue that in the abstract, it makes a lot of sense. For an analogy, consider some form of hierarchical categorization of entities. As higher-level categories are built from lower-level ones, at each stage the process needs to look at its input afresh, and decide on a sensible way of subsuming similar-in-some-way categories.\n\n\nOutput\nStack of decoders traversed, the multi-dimensional codes that pop out need to be converted into something that can be compared with the actual phrase continuation we see in the training corpus. Technically, this involves a projection operation as well a strategy for picking the output word – that word in target-language vocabulary that has the highest probability. How do you decide on a strategy? I’ll say more about that in the section Mechanics of text generation, where I assume a chatbot user’s perspective.\n\n\n\nModel training\nBefore we get there, just a quick word about model training. LLMs are deep neural networks, and as such, they are trained like any network is. First, assuming you have access to the so-called “ground truth”, you can always compare model prediction with the true target. You then quantify the difference – by which algorithm will affect training results. Then, you communicate that difference – the loss – to the network. It, in turn, goes through its modules, from back/top to start/bottom, and updates its stored “knowledge” – matrices of continuous numbers called weights. Since information is passed from layer to layer, in a direction reverse to that followed in computing predictions, this technique is known as back-propagation.\nAnd all that is not triggered once, but iteratively, for a certain number of so-called “epochs”, and modulated by a set of so-called “hyper-parameters”. In practice, a lot of experimentation goes into deciding on the best-working configuration of these settings.\n\n\nMechanics of text generation\nWe already know that during model training, predictions are generated word-by-word; at every step, the model’s knowledge about what has been said so far is augmented by one token: the word that really was following at that point. If, making use of a trained model, a bot is asked to reply to a question, its response must by necessity be generated in the same way. However, the actual “correct word” is not known. The only way, then, is to feed back to the model its own most recent prediction. (By necessity, this lends to text generation a very special character, where every decision the bot makes co-determines its future behavior.)\nWhy, though, talk about decisions? Doesn’t the bot just act on behalf of the core model, the LLM – thus passing on the final output? Not quite. At each prediction step, the model yields a vector, with values as many as there are entries in the vocabulary. As per model design and training rationale, these vectors are “scores” – ratings, sort of, how good a fit a word would be in this situation. Like in life, higher is better. But that doesn’t mean you’d just pick the word with the highest value. In any case, these scores are converted to probabilities, and a suitable probability distribution is used to non-deterministically pick a likely (or likely-ish) word. The probability distribution commonly used is the multinomial distribution, appropriate for discrete choice among more than two alternatives. But what about the conversion to probabilities? Here, there is room for experimentation.\nTechnically, the algorithm employed is known as the softmax function. It is a simplified version of the Boltzmann distribution, famous in statistical mechanics, used to obtain the probability of a system’s state given that state’s energy and the temperature of the system. But for temperature10, both formulae are, in fact, identical. In physical systems, temperature modulates probabilities in the following way: The hotter the system, the closer the states’ probabilities are to each other; the colder it gets, the more distinct those probabilities. In the extreme, at very low temperatures there will be a few clear “winners” and a silent majority of “losers”.\nIn deep learning, a like effect is easy to achieve (by means of a scaling factor). That’s why you may have heard people talk about some weird thing called “temperature” that resulted in [insert adjective here] answers. If the application you use lets you vary that factor, you’ll see that a low temperature will result in deterministic-looking, repetitive, “boring” continuations, while a high one may make the machine appear as though it were on drugs.\nThat concludes our high-level overview of LLMs. Having seen the machine dissected in this way may already have left you with some sort of opinion of what these models are – not. This topic more than deserves a dedicated exposition – and papers are being written pointing to important aspects all the time – but in this text, I’d like to at least offer some input for thought."
  },
  {
    "objectID": "posts/llm-intro/index.html#large-language-models-what-they-are-not",
    "href": "posts/llm-intro/index.html#large-language-models-what-they-are-not",
    "title": "What are Large Language Models? What are they not?",
    "section": "Large Language Models: What they are not",
    "text": "Large Language Models: What they are not\nIn part one,describing LLMs technically, I’ve sometimes felt tempted to use terms like “understanding” or “knowledge” when applied to the machine. I may have ended up using them; in that case, I’ve tried to remember to always surround them with quotes. The latter, the adding quotes, stands in contrast to many texts, even ones published in an academic context (Bender and Koller 2020). The question is, though: Why did I even feel compelled to use these terms, given I do not think they apply, in their usual meaning? I can think of a simple – shockingly simple, maybe – answer: It’s because us, humans, we think, talk, share our thoughts in these terms. When I say understand, I surmise you will know what I mean.\nNow, why do I think that these machines do not understand human language, in the sense we usually imply when using that word?\n\nA few facts\nI’ll start out briefly mentioning empirical results, conclusive thought experiments, and theoretical considerations. All aspects touched upon (and many more) are more than worthy of in-depth discussion, but such discussion is clearly out of scope for this synoptic-in-character text.\nFirst, while it is hard to put a number on the quality of a chatbot’s answers, performance on standardized benchmarks is the “bread and butter” of machine learning – its reporting being an essential part of the prototypical deep-learning publication. (You could even call it the “cookie”, the driving incentive, since models usually are explicitly trained and fine-tuned for good results on these benchmarks.) And such benchmarks exist for most of the down-stream tasks the LLMs are used for: machine translation, generating summaries, text classification, and even rather ambitious-sounding setups associated with – quote/unquote – reasoning.\nHow do you assess such a capability? Here is an example from a benchmark named “Argument Reasoning Comprehension Task” (Habernal et al. 2018).\nClaim: Google is not a harmful monopoly\nReason: People can choose not to use Google\nWarrant: Other search engines don’t redirect to Google\nAlternative: All other search engines redirect to Google\nHere claim and reason together make up the argument. But what, exactly, is it that links them? At first look, this can even be confusing to a human. The missing link is what is called warrant here – add it in, and it all starts to make sense. The task, then, is to decide which of warrant or alternative supports the conclusion, and which one does not.\nIf you think about it, this is a surprisingly challenging task. Specifically, it seems to inescapingly require world knowledge. So if language models, as has been claimed, perform nearly as well as humans, it seems they must have such knowledge – no quotes added. However, in response to such claims, research has been performed to uncover the hidden mechanism that enables such seemingly-superior results. For that benchmark, it has been found (Niven and Kao 2019) that there were spurious statistical cues in the way the dataset was constructed – those removed, LLM performance was no better than random.\nWorld knowledge, in fact, is one of the main things an LLM lacks. Bender et al. (Bender and Koller 2020) convincingly demonstrate its essentiality by means of two thought experiments. One of them, situated on a lone island, imagines an octopus11 inserting itself into some cable-mediated human communication, learning the chit-chat, and finally – having gotten bored – impersonating one of the humans. This works fine, until one day, its communication partner finds themselves in an emergency, and needs to build some rescue tool out of things given in the environment. They urgently ask for advice – and the octopus has no idea what to respond. It has no ideas what these words actually refer to.\nThe other argument comes directly from machine learning, and strikingly simple though it may be, it makes its point very well. Imagine an LLM trained as usual, including on lots of text involving plants. It has also been trained on a dataset of unlabeled photos, the actual task being unsubstantial – say it had to fill out masked areas. Now, we pull out a picture and ask: How many of that blackberry’s blossoms have already opened? The model has no chance to answer the question.\nNow, please look back at the Joseph Weizenbaum quote I opened this article with. It is still true that language-generating machine have no knowledge of the world we live in.\nBefore moving on, I’d like to just quickly hint at a totally different type of consideration, brought up in a (2003!) paper by Spärck Jones (Spaerck 2004). Though written long before LLMs, and long before deep learning started its winning conquest, on an abstract level it is still very applicable to today’s situation. Today, LLMs are employed to “learn language”, i.e., for language acquisition. That skill is then built upon by specialized models, of task-dependent architecture. Popular real-world12 down-stream tasks are translation, document retrieval, or text summarization. When the paper was written, there was no such two-stage pipeline. The author was questioning the fit between how language modeling was conceptualized – namely, as a form of recovery – and the character of these down-stream tasks. Was recovery – inferring a missing, for whatever reasons – piece of text a good model, of, say, condensing a long, detailed piece of text into a short, concise, factual one? If not, could the reason it still seemed to work just fine be of a very different nature – a technical, operational, coincidental one?\n\n[…] the crucial characterisation of the relationship between the input and the output is in fact offloaded in the LM approach onto the choice of training data. We can use LM for summarising because we know that some set of training data consists of full texts paired with their summaries.13\n\nIt seems to me that today’s two-stage process notwithstanding, this is still an aspect worth giving some thought.\n\n\nIt’s us: Language learning, shared goals, and a shared world\nWe’ve already talked about world knowledge. What else are LLMs missing out on?\nIn our world, you’ll hardly find anything that does not involve other people. This goes a lot deeper than the easily observable facts: our constantly communicating, reading and typing messages, documenting our lives on social networks… We don’t experience, explore, explain a world of our own. Instead, all these activities are inter-subjectively constructed. Feelings are14. Cognition is; meaning is. And it goes deeper yet. Implicit assumptions guide us to constantly look for meaning, be it in overheard fragments, mysterious symbols, or life events.\nHow does this relate to LLMs? For one, they’re islands of their own. When you ask them for advice – to develop a research hypothesis and a matching operationalization, say, or whether a detainee should be released on parole – they have no stakes in the outcome, no motivation (be it intrinsic or extrinsic), no goals. If an innocent person is harmed, they don’t feel the remorse; if an experiment is successful but lacks explanatory power, they don’t sense the shallowness; if the world blows up, it won’t have been their world.\nSecondly, it’s us who are not islands. In Bender et al.’s octopus scenario, the human on one side of the cable plays an active role not just when they speak. In making sense of what the octopus says, they contribute an essential ingredient: namely, what they think the octopus wants, thinks, feels, expects… Anticipating, they reflect on what the octopus anticipates.\nAs Bender et al. put it:\n\nIt is not that O’s utterances make sense, but rather, that A can make sense of them.\n\nThat article (Bender and Koller 2020) also brings impressive evidence from human language acquisition: Our predisposition towards language learning notwithstanding, infants don’t learn from the availability of input alone. A situation of joint attention is needed for them to learn. Psychologizing, one could hypothesize they need to get the impression that these sounds, these words, and the fact they’re linked together, actually matters.\nLet me conclude, then, with my final “psychologization”.\n\n\nIt’s us, really: Anthropomorphism unleashed\nYes, it is amazing what these machines do. (And that makes them incredibly dangerous power instruments.) But this in no way affects the human-machine differences that have been existing throughout history, and continue to exist today. That we are inclined to think they understand, know, mean – that maybe even they’re conscious: that’s on us. We can experience deep emotions watching a movie; hope that if we just try enough, we can sense what a distant-in-evolutionary-genealogy creature is feeling; see a cloud encouragingly smiling at us; read a sign in an arrangement of pebbles.\nOur inclination to anthropomorphize is a gift; but it can sometimes be harmful. And nothing of this is special to the twenty-first century.\nLike I began with him, let me conclude with Weizenbaum.\n\nSome subjects have been very hard to convince that ELIZA (with its present script) is not human.\n\nPhoto by Marjan Blan on Unsplash"
  }
]