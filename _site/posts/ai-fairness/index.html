<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-99.9.9">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sigrid Keydana">
<meta name="dcterms.date" content="2021-07-15">
<meta name="description" content="The topic of AI fairness metrics is as important to society as it is confusing. Confusing it is due to a number of reasons: terminological proliferation, abundance of formulae, and last not least the impression that everyone else seems to know what they’re talking about. This text hopes to counteract some of that confusion by starting from a common-sense approach of contrasting two basic positions: On the one hand, the assumption that dataset features may be taken as reflecting the underlying concepts ML practitioners are interested in; on the other, that there inevitably is a gap between concept and measurement, a gap that may be bigger or smaller depending on what is being measured. In contrasting these fundamental views, we bring together concepts from ML, legal science, and political philosophy.">

<title>divergences - Starting to think about AI Fairness</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">divergences</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Starting to think about AI Fairness</h1>
                  <div>
        <div class="description">
          <p>The topic of AI fairness metrics is as important to society as it is confusing. Confusing it is due to a number of reasons: terminological proliferation, abundance of formulae, and last not least the impression that everyone else seems to know what they’re talking about. This text hopes to counteract some of that confusion by starting from a common-sense approach of contrasting two basic positions: On the one hand, the assumption that dataset features may be taken as reflecting the underlying concepts ML practitioners are interested in; on the other, that there inevitably is a gap between concept and measurement, a gap that may be bigger or smaller depending on what is being measured. In contrasting these fundamental views, we bring together concepts from ML, legal science, and political philosophy.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">AI Societal Impact</div>
                <div class="quarto-category">Deep Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Sigrid Keydana </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 15, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>If you use deep learning for unsupervised part-of-speech tagging of Sanskrit <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, or knowledge discovery in physics <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, you probably don’t need to worry about model fairness. If you’re a data scientist working at a place where decisions are made about <em>people</em>, however, or an academic researching models that will be used to such ends, chances are that you’ve already been thinking about this topic. — Or feeling that you should. And thinking about this is hard.</p>
<p>It is hard for several reasons. In this text, I will go into <em>just one</em>.</p>
<section id="the-forest-for-the-trees" class="level2">
<h2 class="anchored" data-anchor-id="the-forest-for-the-trees">The forest for the trees</h2>
<p>Nowadays, it is hard to find a modeling framework that does <em>not</em> include functionality to assess fairness. (Or is at least planning to.) And the terminology sounds so familiar, as well: “calibration”, “predictive parity”, “equal true [false] positive rate”… It almost seems as though we could just take the metrics we make use of anyway (recall or precision, say), test for equality across groups, and that’s it. Let’s assume, for a second, it really was that simple. Then the question still is: Which metrics, exactly, do we choose?</p>
<p>In reality things are <em>not</em> simple. And it gets worse. For very good reasons, there is a close connection in the ML fairness literature to concepts that are primarily treated in other disciplines, such as the legal sciences: <em>discrimination</em> and <em>disparate impact</em> (both not being far from yet another statistical concept, <em>statistical parity</em>). Statistical parity means that if we have a classifier, say to decide whom to hire, it should result in as many applicants from the disadvantaged group (e.g., Black people) being hired as from the advantaged one(s). But that is quite a different requirement from, say, equal true/false positive rates!</p>
<p>So despite all that abundance of software, guides, and decision trees, even: This is not a simple, technical decision. It is, in fact, a technical decision only to a small degree.</p>
</section>
<section id="common-sense-not-math" class="level2">
<h2 class="anchored" data-anchor-id="common-sense-not-math">Common sense, not math</h2>
<p>Let me start this section with a disclaimer: Most of the sources referenced in this text appear, or are implied on the <a href="http://aif360.mybluemix.net/resources#guidance">“Guidance” page</a> of IBM’s framework AI Fairness 360. If you read that page, and everything that’s said and not said there appears clear from the outset, then you may not need this more verbose exposition. If not, I invite you to read on.</p>
<p>Papers on fairness in machine learning, as is common in fields like computer science, abound with formulae. Even the papers referenced here, though selected not for their theorems and proofs but for the ideas they harbor, are no exception. But to start thinking about fairness as it might apply to an ML process at hand, common language – and common sense – will do just fine. If, after analyzing your use case, you judge that the more technical results <em>are</em> relevant to the process in question, you will find that their verbal characterizations will often suffice. It is only when you doubt their correctness that you will need to work through the proofs.</p>
<p>At this point, you may be wondering what it is I am contrasting those “more technical results” with. This is the topic of the next section, where I’ll try to give a birds-eye characterization of fairness criteria and what they imply.</p>
</section>
<section id="situating-fairness-criteria" class="level2">
<h2 class="anchored" data-anchor-id="situating-fairness-criteria">Situating fairness criteria</h2>
<p>Think back to the example of a hiring algorithm. What does it mean for this algorithm to be fair? We approach this question under two – incompatible, mostly – assumptions:</p>
<ol type="1">
<li><p>The algorithm is fair if it behaves the same way independent of which demographic group it is applied to. Here demographic group could be defined by ethnicity, gender, abledness, or in fact any categorization suggested by the context.</p></li>
<li><p>The algorithm is fair if it does not discriminate against any demographic group.</p></li>
</ol>
<p>I’ll call these the technical and societal views, respectively.</p>
<section id="fairness-viewed-the-technical-way" class="level3">
<h3 class="anchored" data-anchor-id="fairness-viewed-the-technical-way">Fairness, viewed the technical way</h3>
<p>What does it mean for an algorithm to “behave the same way” regardless of which group it is applied to?</p>
<p>In a classification setting, we can view the relationship between prediction (<span class="math inline">\(\hat{Y}\)</span>) and target (<span class="math inline">\(Y\)</span>) as a doubly directed path. In one direction: Given true target <span class="math inline">\(Y\)</span>, how accurate is prediction <span class="math inline">\(\hat{Y}\)</span>? In the other: Given <span class="math inline">\(\hat{Y}\)</span>, how well does it predict the true class <span class="math inline">\(Y\)</span>?</p>
<p>Based on the direction they operate in, metrics popular in machine learning overall can be split into two categories. In the first, starting from the true target, we have <em>recall</em>, together with “the <em>rate</em>s”: true positive, true negative, false positive, false negative. In the second, we have <em>precision</em>, together with positive (negative, resp.) <em>predictive value</em>.</p>
<p>If now we demand that these metrics be the same across groups, we arrive at corresponding fairness criteria: equal false positive rate, equal positive predictive value, etc. In the inter-group setting, the two types of metrics may be arranged under headings “equality of opportunity” and “predictive parity”. You’ll encounter these as actual headers in the summary table at the end of this text. (Said table organizes concepts from different areas into a three-category format. The overall narrative builds up towards that “map” in a bottom-up way – meaning, most entries will not make sense at this point.}</p>
<p>While overall, the terminology around metrics can be confusing (to me it is), these headings have some mnemonic value. <em>Equality of opportunity</em> suggests that people similar in real life (<span class="math inline">\(Y\)</span>) get classified similarly (<span class="math inline">\(\hat{Y}\)</span>). <em>Predictive parity</em> suggests that people classified similarly (<span class="math inline">\(\hat{Y}\)</span>) are, in fact, similar (<span class="math inline">\(Y\)</span>).</p>
<p>The two criteria can concisely be characterized using the language of statistical independence. Following <span class="citation" data-cites="barocas">Barocas, Hardt, and Narayanan (<a href="#ref-barocas" role="doc-biblioref">2019</a>)</span>, these are:</p>
<ul>
<li><p>Separation: Given true target <span class="math inline">\(Y\)</span>, prediction <span class="math inline">\(\hat{Y}\)</span> is independent of group membership (<span class="math inline">\(\hat{Y} \perp A | Y\)</span>).</p></li>
<li><p>Sufficiency: Given prediction <span class="math inline">\(\hat{Y}\)</span>, target <span class="math inline">\(Y\)</span> is independent of group membership (<span class="math inline">\(Y \perp A | \hat{Y}\)</span>).</p></li>
</ul>
<p>Given those two fairness criteria – and two sets of corresponding metrics – the natural question arises: Can we satisfy both? Above, I was mentioning precision and recall on purpose: to maybe “prime” you to think in the direction of “precision-recall trade-off”. And really, these two categories reflect different preferences; usually, it is impossible to optimize for both. The most famous, probably, result is due to <span class="citation" data-cites="2016arXiv161007524C">Chouldechova (<a href="#ref-2016arXiv161007524C" role="doc-biblioref">2016</a>)</span> : It says that predictive parity (testing for sufficiency) is incompatible with error rate balance (separation) when prevalence differs across groups. This is a theorem (yes, we’re in the realm of theorems and proofs here) that may not be surprising, in light of Bayes’ theorem, but is of great practical importance nonetheless: Unequal prevalence usually is the norm, not the exception.</p>
<p>This necessarily means we have to make a choice. And this is where the theorems and proofs <em>do</em> matter. For example, <span class="citation" data-cites="abs-1808-08619">Yeom and Tschantz (<a href="#ref-abs-1808-08619" role="doc-biblioref">2018</a>)</span> show that in this framework – the strictly technical approach to fairness – separation should be preferred over sufficiency, because the latter allows for arbitrary disparity amplification. Thus, <em>in this framework</em>, we may have to work through the theorems.</p>
<p>What is the alternative?</p>
</section>
<section id="fairness-viewed-as-a-social-construct" class="level3">
<h3 class="anchored" data-anchor-id="fairness-viewed-as-a-social-construct">Fairness, viewed as a social construct</h3>
<p>Starting with what I just wrote: No one will likely challenge fairness <em>being</em> a social construct. But what does that entail?</p>
<p>Let me start with a biographical reminiscence. In undergraduate psychology (a long time ago), probably the most hammered-in distinction relevant to experiment planning was that between a hypothesis and its operationalization. The hypothesis is what you want to substantiate, conceptually; the operationalization is what you measure. There necessarily can’t be a one-to-one correspondence; we’re just striving to implement the best operationalization possible.</p>
<p>In the world of datasets and algorithms, all we have are measurements. And often, these are treated <em>as though</em> they were the concepts. This will get more concrete with an example, and we’ll stay with the hiring software scenario.</p>
<p>Assume the dataset used for training, assembled from scoring previous employees, contains a set of predictors (among which, high-school grades) and a target variable, say an indicator whether an employee did “survive” probation. There is a concept-measurement mismatch on both sides.</p>
<p>For one, say the grades are intended to reflect ability to learn, and motivation to learn. But depending on the circumstances, there are influence factors of much higher impact: socioeconomic status, constantly having to struggle with prejudice, overt discrimination, and more.</p>
<p>And then, <em>the target variable</em>. If the thing it’s supposed to measure is “was hired for seemed like a good fit, and was retained since was a good fit”, then all is good. But normally, HR departments are aiming for more than just a strategy of “keep doing what we’ve always been doing”.</p>
<p>Unfortunately, that concept-measurement mismatch is even more fatal, and even less talked about, when it’s about the target and not the predictors. (Not accidentally, we also call the target the “ground truth”.) An infamous example is recidivism prediction, where what we really want to measure – whether someone did, in fact, commit a crime – is replaced, for measurability reasons, by whether they were convicted. These are not the same: Conviction depends on more then what someone has done – for instance, if they’ve been under intense scrutiny from the outset.</p>
<p>Fortunately, though, the mismatch is clearly pronounced in the AI fairness literature. <span class="citation" data-cites="FriedlerSV16">Friedler, Scheidegger, and Venkatasubramanian (<a href="#ref-FriedlerSV16" role="doc-biblioref">2016</a>)</span> distinguish between the <em>construct</em> and <em>observed</em> spaces; depending on whether a near-perfect mapping is assumed between these, they talk about two “worldviews”: “We’re all equal” (WAE) vs.&nbsp;“What you see is what you get” (WYSIWIG). If we’re all equal, membership in a societally disadvantaged group should not – in fact, may not – affect classification. In the hiring scenario, any algorithm employed thus has to result in the same proportion of applicants being hired, regardless of which demographic group they belong to. If “What you see is what you get”, we don’t question that the “ground truth” <em>is</em> the truth.</p>
<p>This talk of worldviews may seem unnecessary philosophical, but the authors go on and clarify: All that matters, in the end, is whether the data is seen as reflecting reality in a naïve, take-at-face-value way.</p>
<p>For example, we might be ready to concede that there could be small, albeit uninteresting effect-size-wise, statistical differences between men and women as to spatial vs.&nbsp;linguistic abilities, respectively. We know for sure, though, that there are much greater effects of socialization, starting in the core family and reinforced, progressively, as adolescents go through the education system. We therefore apply WAE, trying to (partly) compensate for historical injustice. This way, we’re effectively applying affirmative action, <a href="https://www.law.cornell.edu/wex/affirmative_action">defined as</a></p>
<blockquote class="blockquote">
<p>A set of procedures designed to eliminate unlawful discrimination among applicants, remedy the results of such prior discrimination, and prevent such discrimination in the future.</p>
</blockquote>
<p>In the already-mentioned summary table, you’ll find the WYSIWIG principle mapped to both equal opportunity and predictive parity metrics. WAE maps to the third category, one we haven’t dwelled upon yet: <em>demographic parity</em>, also known as <em>statistical parity</em>. In line with what was said before, the requirement here is for each group to be present in the positive-outcome class in proportion to its representation in the input sample. For example, if thirty percent of applicants are Black, then at least thirty percent of people selected should be Black, as well. A term commonly used for cases where this does <em>not</em> happen is <em>disparate impact</em>: The algorithm affects different groups in different ways.</p>
<p>Similar in spirit to demographic parity, but possibly leading to different outcomes in practice, is conditional demographic parity <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. Here we additionally take into account other predictors in the dataset; to be precise: <em>all</em> other predictors. The desiderate now is that for any choice of attributes, outcome proportions should be equal, given the protected attribute <strong>and</strong> the other attributes in question. I’ll come back to why this may sound better in theory than work in practice in the next section.</p>
<p>Summing up, we’ve seen commonly used fairness metrics organized into three groups, two of which share a common assumption: that the data used for training can be taken at face value. The other starts from the outside, contemplating what historical events, and what political and societal factors have made the given data look as they do.</p>
<p>Before we conclude, I’d like to try a quick glance at other disciplines, beyond machine learning and computer science, domains where fairness figures among the central topics. This section is necessarily limited in every respect; it should be seen as a flashlight, an invitation to read and reflect rather than an orderly exposition. The short section will end with a word of caution: Since drawing analogies can feel highly enlightening (and is intellectually satisfying, for sure), it is easy to abstract away practical realities. But I’m getting ahead of myself.</p>
</section>
</section>
<section id="a-quick-glance-at-neighboring-fields-law-and-political-philosophy" class="level2">
<h2 class="anchored" data-anchor-id="a-quick-glance-at-neighboring-fields-law-and-political-philosophy">A quick glance at neighboring fields: law and political philosophy</h2>
<p>In jurisprudence, fairness and discrimination constitute an important subject. A recent paper that caught my attention is <span class="citation" data-cites="nondisc">Wachter, Mittelstadt, and Russell (<a href="#ref-nondisc" role="doc-biblioref">2020a</a>)</span> . From a machine learning perspective, the interesting point is the classification of metrics into bias-preserving and bias-transforming. The terms speak for themselves: Metrics in the first group reflect biases in the dataset used for training; ones in the second do not. In that way, the distinction parallels <span class="citation" data-cites="FriedlerSV16">Friedler, Scheidegger, and Venkatasubramanian (<a href="#ref-FriedlerSV16" role="doc-biblioref">2016</a>)</span> ’s confrontation of two “worldviews”. But the exact words used also hint at how guidance by metrics feeds back into society: Seen as strategies, one preserves existing biases; the other, to consequences unknown a priori, <em>changes the world</em>.</p>
<p>To the ML practitioner, this framing is of great help in evaluating what criteria to apply in a project. Helpful, too, is the systematic mapping provided of metrics to the two groups; it is here that, as alluded to above, we encounter <em>conditional demographic parity</em> among the bias-transforming ones. I agree that in spirit, this metric can be seen as bias-transforming; if we take two sets of people who, per all available criteria, are equally qualified for a job, and then find the whites favored over the Blacks, fairness is clearly violated. But the problem here is “available”: per all <em>available</em> criteria. What if we have reason to assume that, in a dataset, all predictors are biased? Then it will be very hard to prove that discrimination has occurred.</p>
<p>A similar problem, I think, surfaces when we look at the field of political philosophy, and consult theories on <a href="https://plato.stanford.edu/entries/justice-distributive/">distributive justice</a> for guidance. <span class="citation" data-cites="abs-1809-03400">Heidari et al. (<a href="#ref-abs-1809-03400" role="doc-biblioref">2018</a>)</span> have written a paper comparing the three criteria – demographic parity, equality of opportunity, and predictive parity – to egalitarianism, equality of opportunity (EOP) in the Rawlsian sense, and EOP seen through the glass of luck egalitarianism, respectively. While the analogy is fascinating, it too assumes that we may take what is in the data at face value. In their likening predictive parity to luck egalitarianism, they have to go to especially great lengths, in assuming that the <em>predicted</em> class reflects <em>effort exerted</em>. In the below table, I therefore take the liberty to disagree, and map a libertarian view of distributive justice to both equality of opportunity and predictive parity metrics.</p>
<p>In summary, we end up with two highly controversial categories of fairness criteria, one bias-preserving, “what you see is what you get”-assuming, and libertarian, the other bias-transforming, “we’re all equal”-thinking, and egalitarian. Here, then, is that often-announced table.</p>
<table class="table">
<colgroup>
<col style="width: 17%">
<col style="width: 34%">
<col style="width: 21%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Demographic parity</th>
<th>Equality of opportunity</th>
<th>Predictive parity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>A.K.A. / subsumes / related concepts</strong></td>
<td>statistical parity, group fairness, disparate impact, conditional demographic parity <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></td>
<td>equalized odds, equal false positive / negative rates</td>
<td>equal positive / negative predictive values, calibration by group</td>
</tr>
<tr class="even">
<td><strong>Statistical independence criterion</strong> <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></td>
<td><p>independence</p>
<p><span class="math inline">\(\hat{Y} \perp A\)</span></p></td>
<td><p>separation</p>
<p><span class="math inline">\(\hat{Y} \perp A | Y\)</span></p></td>
<td><p>sufficiency</p>
<p><span class="math inline">\(Y \perp A | \hat{Y}\)</span></p></td>
</tr>
<tr class="odd">
<td><strong>Individual / group</strong></td>
<td>group</td>
<td>group (most) or individual (fairness through awareness)</td>
<td>group</td>
</tr>
<tr class="even">
<td><strong>Distributive Justice</strong></td>
<td>egalitarian</td>
<td>libertarian (contra Heidari et al., see above)</td>
<td>libertarian (contra Heidari et al., see above)</td>
</tr>
<tr class="odd">
<td><strong>Effect on bias</strong> <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></td>
<td>transforming</td>
<td>preserving</td>
<td>preserving</td>
</tr>
<tr class="even">
<td><strong>Policy / “worldview”</strong> <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></td>
<td>We’re all equal (WAE)</td>
<td>What you see is what you get (WYSIWIG)</td>
<td>What you see is what you get (WYSIWIG)</td>
</tr>
</tbody>
</table>
</section>
<section id="a-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="a-conclusion">(A) Conclusion</h2>
<p>In line with its original goal – to provide some help in starting to think about AI fairness metrics – this article does not end with recommendations. It does, however, end with an observation. As the last section has shown, amidst all theorems and theories, all proofs and memes, it makes sense to not lose sight of the concrete: the data trained on, and the ML process as a whole. Fairness is not something to be evaluated post hoc; the <em>feasibility of fairness</em> is to be reflected on right from the beginning.</p>
<p>In that regard, assessing impact on fairness is not that different from that essential, but often toilsome and non-beloved, stage of modeling that precedes the modeling itself: exploratory data analysis.</p>
<p>Thanks for reading!</p>
<p>Photo by <a href="https://unsplash.com/@andersjilden?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Anders Jildén</a> on <a href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-barocas" class="csl-entry" role="listitem">
Barocas, Solon, Moritz Hardt, and Arvind Narayanan. 2019. <em>Fairness and Machine Learning</em>. fairmlbook.org.
</div>
<div id="ref-2016arXiv161007524C" class="csl-entry" role="listitem">
Chouldechova, Alexandra. 2016. <span>“<span class="nocase">Fair prediction with disparate impact: A study of bias in recidivism prediction instruments</span>.”</span> <em>arXiv e-Prints</em>, October, arXiv:1610.07524. <a href="https://arxiv.org/abs/1610.07524">https://arxiv.org/abs/1610.07524</a>.
</div>
<div id="ref-Cranmer" class="csl-entry" role="listitem">
Cranmer, Miles D., Alvaro Sanchez-Gonzalez, Peter W. Battaglia, Rui Xu, Kyle Cranmer, David N. Spergel, and Shirley Ho. 2020. <span>“Discovering Symbolic Models from Deep Learning with Inductive Biases.”</span> <em>CoRR</em> abs/2006.11287. <a href="https://arxiv.org/abs/2006.11287">https://arxiv.org/abs/2006.11287</a>.
</div>
<div id="ref-FriedlerSV16" class="csl-entry" role="listitem">
Friedler, Sorelle A., Carlos Scheidegger, and Suresh Venkatasubramanian. 2016. <span>“On the (Im)possibility of Fairness.”</span> <em>CoRR</em> abs/1609.07236. <a href="http://arxiv.org/abs/1609.07236">http://arxiv.org/abs/1609.07236</a>.
</div>
<div id="ref-abs-1809-03400" class="csl-entry" role="listitem">
Heidari, Hoda, Michele Loi, Krishna P. Gummadi, and Andreas Krause. 2018. <span>“A Moral Framework for Understanding of Fair <span>ML</span> Through Economic Models of Equality of Opportunity.”</span> <em>CoRR</em> abs/1809.03400. <a href="http://arxiv.org/abs/1809.03400">http://arxiv.org/abs/1809.03400</a>.
</div>
<div id="ref-Srivastava" class="csl-entry" role="listitem">
Srivastava, Prakhar, Kushal Chauhan, Deepanshu Aggarwal, Anupam Shukla, Joydip Dhar, and Vrashabh Prasad Jain. 2018. <span>“Deep Learning Based Unsupervised POS Tagging for Sanskrit.”</span> In <em>Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence</em>. ACAI 2018. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3302425.3302487">https://doi.org/10.1145/3302425.3302487</a>.
</div>
<div id="ref-nondisc" class="csl-entry" role="listitem">
Wachter, Sandra, Brent D. Mittelstadt, and Chris Russell. 2020a. <span>“Bias Preservation in Machine Learning: The Legality of Fairness Metrics Under EU Non-Discrimination Law.”</span> <em>West Virginia Law Review, Forthcoming</em> abs/2005.05906. <a href="https://ssrn.com/abstract=3792772">https://ssrn.com/abstract=3792772</a>.
</div>
<div id="ref-abs-2005-05906" class="csl-entry" role="listitem">
———. 2020b. <span>“Why Fairness Cannot Be Automated: Bridging the Gap Between <span>EU</span> Non-Discrimination Law and <span>AI</span>.”</span> <em>CoRR</em> abs/2005.05906. <a href="https://arxiv.org/abs/2005.05906">https://arxiv.org/abs/2005.05906</a>.
</div>
<div id="ref-abs-1808-08619" class="csl-entry" role="listitem">
Yeom, Samuel, and Michael Carl Tschantz. 2018. <span>“Discriminative but Not Discriminatory: <span>A</span> Comparison of Fairness Definitions Under Different Worldviews.”</span> <em>CoRR</em> abs/1808.08619. <a href="http://arxiv.org/abs/1808.08619">http://arxiv.org/abs/1808.08619</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><span class="citation" data-cites="Srivastava">Srivastava et al. (<a href="#ref-Srivastava" role="doc-biblioref">2018</a>)</span><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><span class="citation" data-cites="Cranmer">Cranmer et al. (<a href="#ref-Cranmer" role="doc-biblioref">2020</a>)</span><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><span class="citation" data-cites="abs-2005-05906">Wachter, Mittelstadt, and Russell (<a href="#ref-abs-2005-05906" role="doc-biblioref">2020b</a>)</span><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><span class="citation" data-cites="abs-2005-05906">Wachter, Mittelstadt, and Russell (<a href="#ref-abs-2005-05906" role="doc-biblioref">2020b</a>)</span><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span class="citation" data-cites="barocas">Barocas, Hardt, and Narayanan (<a href="#ref-barocas" role="doc-biblioref">2019</a>)</span><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><span class="citation" data-cites="nondisc">Wachter, Mittelstadt, and Russell (<a href="#ref-nondisc" role="doc-biblioref">2020a</a>)</span><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><span class="citation" data-cites="FriedlerSV16">Friedler, Scheidegger, and Venkatasubramanian (<a href="#ref-FriedlerSV16" role="doc-biblioref">2016</a>)</span><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>