{
  "hash": "3702ddf826bf0ab58c9eab74540d26a8",
  "result": {
    "markdown": "---\ntitle: \"Forecasting El Niño-Southern Oscillation (ENSO)\"\nauthor: \"Sigrid Keydana\"\ndate: \"2022-02-02\"\ncategories: [Deep Learning]\nimage: pic.jpg\nbibliography: bibliography.bib\ndescription: >\n  El Niño-Southern Oscillation (ENSO) is an atmospheric phenomenon, located in the tropical Pacific, that greatly affects ecosystems as well as human well-being on a large portion of the globe. We use the convLSTM introduced in a prior post to predict the Niño 3.4 Index from spatially-ordered sequences of sea surface temperatures.\n---\n\n\n\n\nToday, we use the convLSTM introduced in a [previous](https://blogs.rstudio.com/ai/posts/2020-12-17-torch-convlstm/) post to predict [El Niño-Southern Oscillation (ENSO).](https://en.wikipedia.org/wiki/El_Ni%C3%B1o%E2%80%93Southern_Oscillation)\n\n# El Niño, la Niña\n\nENSO refers to a changing pattern of sea surface temperatures and sea-level pressures occurring in the equatorial Pacific. From its three overall states, probably the best-known is El Niño. El Niño occurs when surface water temperatures in the eastern Pacific are higher than normal, and the strong winds that normally blow from east to west are unusually weak. The opposite conditions are termed La Niña. Everything in-between is classified as normal.\n\nENSO has great impact on the weather worldwide, and routinely harms ecosystems and societies through storms, droughts and flooding, possibly resulting in famines and economic crises. The best societies can do is try to adapt and mitigate severe consequences. Such efforts are aided by accurate forecasts, the further ahead the better.\n\nHere, deep learning (DL) can potentially help: Variables like sea surface temperatures and pressures are given on a spatial grid -- that of the earth -- and as we know, DL is good at extracting spatial (e.g., image) features. For ENSO prediction, architectures like convolutional neural networks (@dlenso) or convolutional-recurrent hybrids[^1] are habitually used. One such hybrid is just our convLSTM; it operates on sequences of features given on a spatial grid. Today, thus, we'll be training a model for ENSO forecasting. This model will have a convLSTM for its central ingredient.\n\n[^1]: E.g., [Forecasting El Niño with Convolutional and Recurrent Neural Networks](https://www.climatechange.ai/papers/neurips2019/40/paper.pdf).\n\nBefore we start, a note. While our model fits well with architectures described in the relevant papers, the same cannot be said for amount of training data used. For reasons of practicality, we use actual observations only; consequently, we end up with a small (relative to the task) dataset. In contrast, research papers tend to make use of climate simulations[^2], resulting in significantly more data to work with.\n\n[^2]: E.g., [CMIP5](https://esgf-node.llnl.gov/projects/cmip5/), [CNRM-CM5](http://www.umr-cnrm.fr/spip.php?article126&lang=en), or [HADGEM2-ES](https://southernocean.arizona.edu/content/hadgem2-es).\n\nFrom the outset, then, we don't expect stellar performance. Nevertheless, this should make for an interesting case study, and a useful code template for our readers to apply to their own data.\n\n# Data\n\nWe will attempt to predict monthly average sea surface temperature in the Niño 3.4 region[^3], as represented by the Niño 3.4 Index, plus categorization as one of *El Niño*, *La Niña* or *neutral*[^4]. Predictions will be based on prior monthly sea surface temperatures spanning a large portion of the globe.\n\n[^3]: This region extends over latitudes from 5° south to 5° north and longitudes from 120° west to 170° west.\n\n[^4]: That classification is based on ONI (Oceanic Niño Index), a measure representing 3-month average anomalies in the Niño 3.4 Index.\n\nOn the input side, public and ready-to-use data may be downloaded from [Tokyo Climate Center](https://ds.data.jma.go.jp/tcc/tcc/index.html); as to prediction targets, we obtain index and classification [here](https://bmcnoldy.rsmas.miami.edu/tropics/oni/).\n\nInput and target data both are provided monthly. They intersect in the time period ranging from 1891-01-01 to 2020-08-01; so this is the range of dates we'll be zooming in on.\n\n## Input: Sea Surface Temperatures\n\nMonthly sea surface temperatures are provided in a latitude-longitude grid of resolution 1°. Details of how the data were processed are available [here](https://ds.data.jma.go.jp/tcc/tcc/library/MRCS_SV12/explanation/cobe_sst_e.htm).\n\nData files are available in [GRIB](https://en.wikipedia.org/wiki/GRIB) format; each file contains averages computed for a single month. We can either download individual files or [generate a text file of URLs](https://ds.data.jma.go.jp/tcc/tcc/products/elnino/cobesst/cobe-sst.html) for download. Once you've saved these URLs to a file, you can have R get the files for you like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npurrr::walk(\n   readLines(\"files\"),\n   function(f) download.file(url = f, destfile = basename(f))\n)\n```\n:::\n\n\nFrom R, we can read GRIB files using [stars](https://cran.r-project.org/web/packages/stars/). For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's just quickly load all libraries we require to start with\n\nlibrary(torch)\nlibrary(tidyverse)\nlibrary(stars)\nlibrary(viridis)\nlibrary(ggthemes)\n\ntorch_manual_seed(777)\n\nread_stars(file.path(grb_dir, \"sst189101.grb\"))\n```\n:::\n\n\n```         \nstars object with 2 dimensions and 1 attribute\nattribute(s):\n sst189101.grb   \n Min.   :-274.9  \n 1st Qu.:-272.8  \n Median :-259.1  \n Mean   :-260.0  \n 3rd Qu.:-248.4  \n Max.   :-242.8  \n NA's   :21001   \ndimension(s):\n  from  to offset delta                       refsys point values    \nx    1 360      0     1 Coordinate System importe...    NA   NULL [x]\ny    1 180     90    -1 Coordinate System importe...    NA   NULL [y]\n```\n\nSo in this GRIB file, we have one attribute - which we know to be sea surface temperature -- on a two-dimensional grid. As to the latter, we can complement what `stars` tells us with additional info found in the [documentation](https://ds.data.jma.go.jp/tcc/tcc/library/MRCS_SV12/explanation/cobe_sst_e.htm):\n\n> The east-west grid points run eastward from 0.5ºE to 0.5ºW, while the north-south grid points run northward from 89.5ºS to 89.5ºN.\n\nWe note a few things we'll want to do with this data. For one, the temperatures seem to be given in Kelvin, but with minus signs.[^5] We'll remove the minus signs and convert to degrees Celsius for convenience. We'll also have to think about what to do with the `NA`s that appear for all non-maritime coordinates.\n\n[^5]: This may also be an artifact produced by the software stack involved in reading the file.\n\nBefore we get there though, we need to combine data from all files into a single data frame. This adds an additional dimension, time, ranging from 1891/01/01 to 2020/01/12:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrb <- read_stars(\n  file.path(grb_dir, map(readLines(\"files\", warn = FALSE), basename)), along = \"time\") %>%\n  st_set_dimensions(3,\n                    values = seq(as.Date(\"1891-01-01\"), as.Date(\"2020-12-01\"), by = \"months\"),\n                    names = \"time\"\n                    )\n\ngrb\n```\n:::\n\n\n```         \nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n sst189101.grb   \n Min.   :-274.9  \n 1st Qu.:-273.3  \n Median :-258.8  \n Mean   :-260.0  \n 3rd Qu.:-247.8  \n Max.   :-242.8  \n NA's   :33724   \ndimension(s):\n     from   to offset delta                       refsys point                    values    \nx       1  360      0     1 Coordinate System importe...    NA                      NULL [x]\ny       1  180     90    -1 Coordinate System importe...    NA                      NULL [y]\ntime    1 1560     NA    NA                         Date    NA 1891-01-01,...,2020-12-01    \n```\n\nLet's visually inspect the spatial distribution of monthly temperatures for one year, 2020:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_stars(data = grb %>% filter(between(time, as.Date(\"2020-01-01\"), as.Date(\"2020-12-01\"))), alpha = 0.8) +\n  facet_wrap(\"time\") +\n  scale_fill_viridis() +\n  coord_equal() +\n  theme_map() +\n  theme(legend.position = \"none\") \n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](enso.png){fig-alt='Monthly sea surface temperatures, 2020/01/01 - 2020/01/12.' width=596}\n:::\n:::\n\n\n## Target: Niño 3.4 Index\n\nFor the Niño 3.4 Index, we download the monthly [data](https://bmcnoldy.rsmas.miami.edu/tropics/oni/ONI_NINO34_1854-2020.txt) and, among the provided features, zoom in on two: the index itself (column `NINO34_MEAN`) and `PHASE`, which can be `E` (El Niño), `L` (La Niño) or `N` (neutral).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnino <- read_table2(\"ONI_NINO34_1854-2020.txt\", skip = 9) %>%\n  mutate(month = as.Date(paste0(YEAR, \"-\", `MON/MMM`, \"-01\"))) %>%\n  select(month, NINO34_MEAN, PHASE) %>%\n  filter(between(month, as.Date(\"1891-01-01\"), as.Date(\"2020-08-01\"))) %>%\n  mutate(phase_code = as.numeric(as.factor(PHASE)))\n\nnrow(nino)\n```\n:::\n\n\n```         \n1556\n```\n\nNext, we look at how to get the data into a format convenient for training and prediction.\n\n# Preprocessing\n\n## Input\n\nFirst, we remove all input data for points in time where ground truth data are still missing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsst <- grb %>% filter(time <= as.Date(\"2020-08-01\"))\n```\n:::\n\n\nNext, as is done by e.g. @dlenso, we only use grid points between 55° south and 60° north. This has the additional advantage of reducing memory requirements.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsst <- grb %>% filter(between(y,-55, 60))\n\ndim(sst)\n```\n:::\n\n\n```         \n360, 115, 1560\n```\n\nAs already alluded to, with the little data we have we can't expect much in terms of generalization. Still, we set aside a small portion of the data for validation, since we'd like for this post to serve as a useful template to be used with bigger datasets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsst_train <- sst %>% filter(time < as.Date(\"1990-01-01\"))\nsst_valid <- sst %>% filter(time >= as.Date(\"1990-01-01\"))\n```\n:::\n\n\nFrom here on, we work with R arrays.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsst_train <- as.tbl_cube.stars(sst_train)$mets[[1]]\nsst_valid <- as.tbl_cube.stars(sst_valid)$mets[[1]]\n```\n:::\n\n\nConversion to degrees Celsius is not strictly necessary, as initial experiments showed a slight performance increase due to normalizing the input, and we're going to do that anyway. Still, it reads nicer to humans than Kelvin.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsst_train <- sst_train + 273.15\nquantile(sst_train, na.rm = TRUE)\n```\n:::\n\n\n```         \n     0%     25%     50%     75%    100% \n-1.8000 12.9975 21.8775 26.8200 34.3700 \n```\n\nNot at all surprisingly, global warming is evident from inspecting temperature distribution on the validation set (which was chosen to span the last thirty-one years).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsst_valid <- sst_valid + 273.15\nquantile(sst_valid, na.rm = TRUE)\n```\n:::\n\n\n```         \n    0%    25%    50%    75%   100% \n-1.800 13.425 22.335 27.240 34.870 \n```\n\nThe next-to-last step normalizes both sets according to training mean and variance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_mean <- mean(sst_train, na.rm = TRUE)\ntrain_sd <- sd(sst_train, na.rm = TRUE)\n\nsst_train <- (sst_train - train_mean) / train_sd\n\nsst_valid <- (sst_valid - train_mean) / train_sd\n```\n:::\n\n\nFinally, what should we do about the `NA` entries? We set them to zero, the (training set) mean. That may not be enough of an action though: It means we're feeding the network roughly 30% misleading data. This is something we're not done with yet.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsst_train[is.na(sst_train)] <- 0\nsst_valid[is.na(sst_valid)] <- 0\n```\n:::\n\n\n## Target\n\nThe target data are split analogously. Let's check though: Are phases (categorizations) distributedly similarly in both sets?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnino_train <- nino %>% filter(month < as.Date(\"1990-01-01\"))\nnino_valid <- nino %>% filter(month >= as.Date(\"1990-01-01\"))\n\nnino_train %>% group_by(phase_code, PHASE) %>% summarise(count = n(), avg = mean(NINO34_MEAN))\n```\n:::\n\n\n```         \n# A tibble: 3 x 4\n# Groups:   phase_code [3]\n  phase_code PHASE count   avg\n       <dbl> <chr> <int> <dbl>\n1          1 E       301  27.7\n2          2 L       333  25.6\n3          3 N       554  26.7\n```\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnino_valid %>% group_by(phase_code, PHASE) %>% summarise(count = n(), avg = mean(NINO34_MEAN))\n```\n:::\n\n\n```         \n# A tibble: 3 x 4\n# Groups:   phase_code [3]\n  phase_code PHASE count   avg\n       <dbl> <chr> <int> <dbl>\n1          1 E        93  28.1\n2          2 L        93  25.9\n3          3 N       182  27.2\n```\n\nThis doesn't look too bad. Of course, we again see the overall rise in temperature, irrespective of phase.\n\nLastly, we normalize the index, same as we did for the input data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_mean_nino <- mean(nino_train$NINO34_MEAN)\ntrain_sd_nino <- sd(nino_train$NINO34_MEAN)\n\nnino_train <- nino_train %>% mutate(NINO34_MEAN = scale(NINO34_MEAN, center = train_mean_nino, scale = train_sd_nino))\nnino_valid <- nino_valid %>% mutate(NINO34_MEAN = scale(NINO34_MEAN, center = train_mean_nino, scale = train_sd_nino))\n```\n:::\n\n\nOn to the `torch` dataset.\n\n# `Torch` dataset\n\nThe dataset is responsible for correctly matching up inputs and targets.\n\nOur goal is to take six months of global sea surface temperatures and predict the Niño 3.4 Index for the following month. Input-wise, the model will expect the following format semantics:\n\n`batch_size * timesteps * width * height * channels`, where\n\n-   `batch_size` is the number of observations worked on in one round of computations,\n\n-   `timesteps` chains consecutive observations from adjacent months,\n\n-   `width` and `height` together constitute the spatial grid, and\n\n-   `channels` corresponds to available visual channels in the \"image\".\n\nIn `.getitem()`, we select the consecutive observations, starting at a given index, and stack them in dimension one. (One, not two, as batches will only start to exist once the `dataloader` comes into play.)\n\nNow, what about the target? Our ultimate goal was -- is -- predicting the Niño 3.4 Index. However, as you see we define three targets: One is the index, as expected; an additional one holds the spatially-gridded sea surface temperatures for the prediction month. Why? Our main instrument, the most prominent constituent of the model, will be a convLSTM, an architecture designed for *spatial* prediction. Thus, to train it efficiently, we want to give it the opportunity to predict values on a spatial grid. So far so good; but there's one more target, the phase/category. This was added for experimentation purposes: Maybe predicting *both* index *and* phase helps in training?\n\nFinally, here is the code for the dataset. In our experiments, we based predictions on inputs from the preceding six months (`n_timesteps <- 6`). This is a parameter you might want to play with, though.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_timesteps <- 6\n\nenso_dataset <- dataset(\n  name = \"enso_dataset\",\n  \n  initialize = function(sst, nino, n_timesteps) {\n    self$sst <- sst\n    self$nino <- nino\n    self$n_timesteps <- n_timesteps\n  },\n  \n  .getitem = function(i) {\n    x <- torch_tensor(self$sst[, , i:(n_timesteps + i - 1)]) # (360, 115, n_timesteps)\n    x <- x$permute(c(3,1,2))$unsqueeze(2) # (n_timesteps, 1, 360, 115))\n    \n    y1 <- torch_tensor(self$sst[, , n_timesteps + i])$unsqueeze(1) # (1, 360, 115)\n    y2 <- torch_tensor(self$nino$NINO34_MEAN[n_timesteps + i])\n    y3 <- torch_tensor(self$nino$phase_code[n_timesteps + i])$squeeze()$to(torch_long())\n    list(x = x, y1 = y1, y2 = y2, y3 = y3)\n  },\n  \n  .length = function() {\n    nrow(self$nino) - n_timesteps\n  }\n  \n)\n\nvalid_ds <- enso_dataset(sst_valid, nino_valid, n_timesteps)\n```\n:::\n\n\n# Dataloaders\n\nAfter the custom dataset, we create the -- pretty typical -- `dataloader`s, making use of a batch size of 4.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatch_size <- 4\n\ntrain_dl <- train_ds %>% dataloader(batch_size = batch_size, shuffle = TRUE)\n\nvalid_dl <- valid_ds %>% dataloader(batch_size = batch_size)\n```\n:::\n\n\nNext, we proceed to model creation.\n\n# Model\n\nThe model's main ingredient is the convLSTM introduced in a [prior post](https://blogs.rstudio.com/ai/posts/2020-12-17-torch-convlstm/). For convenience, we reproduce the code in the appendix.\n\nBesides the convLSTM, the model makes use of three convolutional layers, a batchnorm layer and five linear layers. The logic is the following.\n\nFirst, the convLSTM job is to predict the next month's sea surface temperatures on the spatial grid. For that, we *almost* just return its final state, - almost: We use `self$conv1` to reduce the number channels to one.\n\nFor predicting index and phase, we then need to flatten the grid, as we require a single value each. This is where the additional conv layers come in. We *do* hope they'll aid in learning, but we also want to reduce the number of parameters a bit, downsizing the grid (`strides = 2` and `strides = 3`, resp.) a bit before the upcoming `torch_flatten()`.\n\nOnce we have a flat structure, learning is shared between the tasks of index and phase prediction (`self$linear`), until finally their paths split (`self$cont` and `self$cat`, resp.), and they return their separate outputs.\n\n(The batchnorm? I'll comment on that in the [Discussion].)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- nn_module(\n  \n  initialize = function(channels_in,\n                        convlstm_hidden,\n                        convlstm_kernel,\n                        convlstm_layers) {\n    \n    self$n_layers <- convlstm_layers\n    \n    self$convlstm <- convlstm(\n      input_dim = channels_in,\n      hidden_dims = convlstm_hidden,\n      kernel_sizes = convlstm_kernel,\n      n_layers = convlstm_layers\n    )\n    \n    self$conv1 <-\n      nn_conv2d(\n        in_channels = 32,\n        out_channels = 1,\n        kernel_size = 5,\n        padding = 2\n      )\n    \n    self$conv2 <-\n      nn_conv2d(\n        in_channels = 32,\n        out_channels = 32,\n        kernel_size = 5,\n        stride = 2\n      )\n    \n    self$conv3 <-\n      nn_conv2d(\n        in_channels = 32,\n        out_channels = 32,\n        kernel_size = 5,\n        stride = 3\n      )\n    \n    self$linear <- nn_linear(33408, 64)\n    \n    self$b1 <- nn_batch_norm1d(num_features = 64)\n        \n    self$cont <- nn_linear(64, 128)\n    self$cat <- nn_linear(64, 128)\n    \n    self$cont_output <- nn_linear(128, 1)\n    self$cat_output <- nn_linear(128, 3)\n    \n  },\n  \n  forward = function(x) {\n    \n    ret <- self$convlstm(x)\n    layer_last_states <- ret[[2]]\n    last_hidden <- layer_last_states[[self$n_layers]][[1]]\n    \n    next_sst <- last_hidden %>% self$conv1() \n    \n    c2 <- last_hidden %>% self$conv2() \n    c3 <- c2 %>% self$conv3() \n    \n    flat <- torch_flatten(c3, start_dim = 2)\n    common <- self$linear(flat) %>% self$b3() %>% nnf_relu()\n\n    next_temp <- common %>% self$cont() %>% nnf_relu() %>% self$cont_output()\n    next_nino <- common %>% self$cat() %>% nnf_relu() %>% self$cat_output()\n    \n    list(next_sst, next_temp, next_nino)\n    \n  }\n  \n)\n```\n:::\n\n\nNext, we instantiate a pretty small-ish model. You're more than welcome to experiment with larger models, but training time as well as GPU memory requirements will increase.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnet <- model(\n  channels_in = 1,\n  convlstm_hidden = c(16, 16, 32),\n  convlstm_kernel = c(3, 3, 5),\n  convlstm_layers = 3\n)\n\ndevice <- torch_device(if (cuda_is_available()) \"cuda\" else \"cpu\")\n\nnet <- net$to(device = device)\nnet\n```\n:::\n\n\n```         \nAn `nn_module` containing 2,389,605 parameters.\n\n── Modules ───────────────────────────────────────────────────────────────────────────────\n● convlstm: <nn_module> #182,080 parameters\n● conv1: <nn_conv2d> #801 parameters\n● conv2: <nn_conv2d> #25,632 parameters\n● conv3: <nn_conv2d> #25,632 parameters\n● linear: <nn_linear> #2,138,176 parameters\n● b1: <nn_batch_norm1d> #128 parameters\n● cont: <nn_linear> #8,320 parameters\n● cat: <nn_linear> #8,320 parameters\n● cont_output: <nn_linear> #129 parameters\n● cat_output: <nn_linear> #387 parameters\n```\n\n# Training\n\nWe have three model outputs. How should we combine the losses?\n\nGiven that the main goal is predicting the index, and the other two outputs are essentially means to an end, I found the following combination rather effective:\n\n```         \n# weight for sea surface temperature prediction\nlw_sst <- 0.2\n\n# weight for prediction of El Nino 3.4 Index\nlw_temp <- 0.4\n\n# weight for phase prediction\nlw_nino <- 0.4\n```\n\nThe training process follows the pattern seen in all `torch` posts so far: For each epoch, loop over the training set, backpropagate, check performance on validation set.\n\n*But*, when we did the pre-processing, we were aware of an imminent problem: the missing temperatures for continental areas, which we set to zero. As a sole measure, this approach is clearly insufficient. What if we had chosen to use latitude-dependent averages? Or interpolation? Both may be better than a global average, but both have their problems as well. Let's at least alleviate negative consequences by not using the respective pixels for spatial loss calculation. This is taken care of by the following line below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsst_loss <- nnf_mse_loss(sst_output[sst_target != 0], sst_target[sst_target != 0])\n```\n:::\n\n\nHere, then, is the complete training code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer <- optim_adam(net$parameters, lr = 0.001)\n\nnum_epochs <- 50\n\ntrain_batch <- function(b) {\n  \n  optimizer$zero_grad()\n  output <- net(b$x$to(device = device))\n  \n  sst_output <- output[[1]]\n  sst_target <- b$y1$to(device = device)\n  \n  sst_loss <- nnf_mse_loss(sst_output[sst_target != 0], sst_target[sst_target != 0])\n  temp_loss <- nnf_mse_loss(output[[2]], b$y2$to(device = device))\n  nino_loss <- nnf_cross_entropy(output[[3]], b$y3$to(device = device))\n  \n  loss <- lw_sst * sst_loss + lw_temp * temp_loss + lw_nino * nino_loss\n  loss$backward()\n  optimizer$step()\n\n  list(sst_loss$item(), temp_loss$item(), nino_loss$item(), loss$item())\n  \n}\n\nvalid_batch <- function(b) {\n  \n  output <- net(b$x$to(device = device))\n  \n  sst_output <- output[[1]]\n  sst_target <- b$y1$to(device = device)\n  \n  sst_loss <- nnf_mse_loss(sst_output[sst_target != 0], sst_target[sst_target != 0])\n  temp_loss <- nnf_mse_loss(output[[2]], b$y2$to(device = device))\n  nino_loss <- nnf_cross_entropy(output[[3]], b$y3$to(device = device))\n  \n  loss <-\n    lw_sst * sst_loss + lw_temp * temp_loss + lw_nino * nino_loss\n\n  list(sst_loss$item(),\n       temp_loss$item(),\n       nino_loss$item(),\n       loss$item())\n}\n\nfor (epoch in 1:num_epochs) {\n  \n  net$train()\n  \n  train_loss_sst <- c()\n  train_loss_temp <- c()\n  train_loss_nino <- c()\n  train_loss <- c()\n\n  coro::loop(for (b in train_dl) {\n    losses <- train_batch(b)\n    train_loss_sst <- c(train_loss_sst, losses[[1]])\n    train_loss_temp <- c(train_loss_temp, losses[[2]])\n    train_loss_nino <- c(train_loss_nino, losses[[3]])\n    train_loss <- c(train_loss, losses[[4]])\n  })\n  \n  cat(\n    sprintf(\n      \"\\nEpoch %d, training: loss: %3.3f sst: %3.3f temp: %3.3f nino: %3.3f \\n\",\n      epoch, mean(train_loss), mean(train_loss_sst), mean(train_loss_temp), mean(train_loss_nino)\n    )\n  )\n  \n  net$eval()\n  \n  valid_loss_sst <- c()\n  valid_loss_temp <- c()\n  valid_loss_nino <- c()\n  valid_loss <- c()\n\n  coro::loop(for (b in valid_dl) {\n    losses <- valid_batch(b)\n    valid_loss_sst <- c(valid_loss_sst, losses[[1]])\n    valid_loss_temp <- c(valid_loss_temp, losses[[2]])\n    valid_loss_nino <- c(valid_loss_nino, losses[[3]])\n    valid_loss <- c(valid_loss, losses[[4]])\n    \n  })\n  \n  cat(\n    sprintf(\n      \"\\nEpoch %d, validation: loss: %3.3f sst: %3.3f temp: %3.3f nino: %3.3f \\n\",\n      epoch, mean(valid_loss), mean(valid_loss_sst), mean(valid_loss_temp), mean(valid_loss_nino)\n    )\n  )\n  \n  torch_save(net, paste0(\n    \"model_\", epoch, \"_\", round(mean(train_loss), 3), \"_\", round(mean(valid_loss), 3), \".pt\"\n  ))\n  \n}\n```\n:::\n\n\nWhen I ran this, performance on the training set decreased in a not-too-fast, but continuous way, while validation set performance kept fluctuating. For reference, total (composite) losses looked like this:\n\n```         \nEpoch     Training    Validation\n   \n   10        0.336         0.633\n   20        0.233         0.295\n   30        0.135         0.461\n   40        0.099         0.903\n   50        0.061         0.727\n```\n\nThinking of the size of the validation set - thirty-one years, or equivalently, 372 data points -- those fluctuations may not be all too surprising.\n\n# Predictions\n\nNow losses tend to be abstract; let's see what actually gets predicted. We obtain predictions for index values and phases like so ...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnet$eval()\n\npred_index <- c()\npred_phase <- c()\n\ncoro::loop(for (b in valid_dl) {\n\n  output <- net(b$x$to(device = device))\n\n  pred_index <- c(pred_index, output[[2]]$to(device = \"cpu\"))\n  pred_phase <- rbind(pred_phase, as.matrix(output[[3]]$to(device = \"cpu\")))\n\n})\n```\n:::\n\n\n... and combine these with the ground truth, stripping off the first six rows (six was the number of timesteps used as predictors):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvalid_perf <- data.frame(\n  actual_temp = nino_valid$NINO34_MEAN[(batch_size + 1):nrow(nino_valid)] * train_sd_nino + train_mean_nino,\n  actual_nino = factor(nino_valid$phase_code[(batch_size + 1):nrow(nino_valid)]),\n  pred_temp = pred_index * train_sd_nino + train_mean_nino,\n  pred_nino = factor(pred_phase %>% apply(1, which.max))\n)\n```\n:::\n\n\nFor the phase, we can generate a confusion matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyardstick::conf_mat(valid_perf, actual_nino, pred_nino)\n```\n:::\n\n\n```         \n          Truth\nPrediction   1   2   3\n         1  70   0  43\n         2   0  47  10\n         3  23  46 123\n```\n\nThis looks better than expected (based on the losses). Phases 1 and 2 correspond to El Niño and La Niña, respectively, and these get sharply separated.\n\nWhat about the Niño 3.4 Index? Let's plot predictions versus ground truth:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvalid_perf <- valid_perf %>% \n  select(actual = actual_temp, predicted = pred_temp) %>% \n  add_column(month = seq(as.Date(\"1990-07-01\"), as.Date(\"2020-08-01\"), by = \"months\")) %>%\n  pivot_longer(-month, names_to = \"Index\", values_to = \"temperature\")\n\nggplot(valid_perf, aes(x = month, y = temperature, color = Index)) +\n  geom_line() +\n  scale_color_manual(values = c(\"#006D6F\", \"#B2FFFF\")) +\n  theme_classic()\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Nino 3.4 Index: Ground truth vs. predictions (validation set).](preds.png){width=600}\n:::\n:::\n\n\nThis does not look bad either. However, we need to keep in mind that we're predicting just a single time step ahead. We probably should not overestimate the results. Which leads directly to the discussion.\n\n# Discussion\n\nWhen working with small amounts of data, a lot can be learned by quick-ish experimentation. However, when *at the same time*, the task is complex, one should be cautious extrapolating.\n\nFor example, well-established regularizers such as batchnorm and dropout, while intended to improve generalization to the validation set, may turn out to severely impede training itself. This is the story behind the one batchnorm layer I kept (I did try having more), and it is also why there is no dropout.\n\nOne lesson to learn from this experience then is: Make sure the amount of data matches the complexity of the task. This is what we see in the ENSO prediction papers published on arxiv.\n\nIf we should treat the results with caution, why even publish the post?\n\nFor one, it shows an application of convLSTM to real-world data, employing a reasonably complex architecture and illustrating techniques like custom losses and loss masking. Similar architectures and strategies should be applicable to a wide range of real-world tasks -- basically, whenever predictors in a time-series problem are given on a spatial grid.\n\nSecondly, the application itself -- forecasting an atmospheric phenomenon that greatly affects ecosystems as well as human well-being -- seems like an excellent use of deep learning. Applications like these stand out as all the more worthwhile as the same cannot be said of everything deep learning is -- and will be, barring effective regulation -- used for.\n\nThanks for reading!\n\n# Appendix: `convlstm` code\n\nFor an in-depth explanation of `convlstm`, see [the blog post](https://blogs.rstudio.com/ai/posts/2020-12-17-torch-convlstm/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\nlibrary(torchvision)\n\nconvlstm_cell <- nn_module(\n  \n  initialize = function(input_dim, hidden_dim, kernel_size, bias) {\n    \n    self$hidden_dim <- hidden_dim\n    \n    padding <- kernel_size %/% 2\n    \n    self$conv <- nn_conv2d(\n      in_channels = input_dim + self$hidden_dim,\n      # for each of input, forget, output, and cell gates\n      out_channels = 4 * self$hidden_dim,\n      kernel_size = kernel_size,\n      padding = padding,\n      bias = bias\n    )\n  },\n  \n  forward = function(x, prev_states) {\n\n    h_prev <- prev_states[[1]]\n    c_prev <- prev_states[[2]]\n    \n    combined <- torch_cat(list(x, h_prev), dim = 2)  # concatenate along channel axis\n    combined_conv <- self$conv(combined)\n    gate_convs <- torch_split(combined_conv, self$hidden_dim, dim = 2)\n    cc_i <- gate_convs[[1]]\n    cc_f <- gate_convs[[2]]\n    cc_o <- gate_convs[[3]]\n    cc_g <- gate_convs[[4]]\n    \n    # input, forget, output, and cell gates (corresponding to torch's LSTM)\n    i <- torch_sigmoid(cc_i)\n    f <- torch_sigmoid(cc_f)\n    o <- torch_sigmoid(cc_o)\n    g <- torch_tanh(cc_g)\n    \n    # cell state\n    c_next <- f * c_prev + i * g\n    # hidden state\n    h_next <- o * torch_tanh(c_next)\n    \n    list(h_next, c_next)\n    \n  },\n  \n  init_hidden = function(batch_size, height, width) {\n    list(torch_zeros(batch_size, self$hidden_dim, height, width, device = self$conv$weight$device),\n         torch_zeros(batch_size, self$hidden_dim, height, width, device = self$conv$weight$device))\n  }\n)\n\nconvlstm <- nn_module(\n  \n  initialize = function(input_dim, hidden_dims, kernel_sizes, n_layers, bias = TRUE) {\n \n    self$n_layers <- n_layers\n    \n    self$cell_list <- nn_module_list()\n    \n    for (i in 1:n_layers) {\n      cur_input_dim <- if (i == 1) input_dim else hidden_dims[i - 1]\n      self$cell_list$append(convlstm_cell(cur_input_dim, hidden_dims[i], kernel_sizes[i], bias))\n    }\n  },\n  \n  # we always assume batch-first\n  forward = function(x) {\n    \n    batch_size <- x$size()[1]\n    seq_len <- x$size()[2]\n    height <- x$size()[4]\n    width <- x$size()[5]\n   \n    # initialize hidden states\n    init_hidden <- vector(mode = \"list\", length = self$n_layers)\n    for (i in 1:self$n_layers) {\n      init_hidden[[i]] <- self$cell_list[[i]]$init_hidden(batch_size, height, width)\n    }\n    \n    # list containing the outputs, of length seq_len, for each layer\n    # this is the same as h, at each step in the sequence\n    layer_output_list <- vector(mode = \"list\", length = self$n_layers)\n    \n    # list containing the last states (h, c) for each layer\n    layer_state_list <- vector(mode = \"list\", length = self$n_layers)\n\n    cur_layer_input <- x\n    hidden_states <- init_hidden\n    \n    # loop over layers\n    for (i in 1:self$n_layers) {\n      \n      # every layer's hidden state starts from 0 (non-stateful)\n      h_c <- hidden_states[[i]]\n      h <- h_c[[1]]\n      c <- h_c[[2]]\n      # outputs, of length seq_len, for this layer\n      # equivalently, list of h states for each time step\n      output_sequence <- vector(mode = \"list\", length = seq_len)\n      \n      # loop over timesteps\n      for (t in 1:seq_len) {\n        h_c <- self$cell_list[[i]](cur_layer_input[ , t, , , ], list(h, c))\n        h <- h_c[[1]]\n        c <- h_c[[2]]\n        # keep track of output (h) for every timestep\n        # h has dim (batch_size, hidden_size, height, width)\n        output_sequence[[t]] <- h\n      }\n\n      # stack hs for all timesteps over seq_len dimension\n      # stacked_outputs has dim (batch_size, seq_len, hidden_size, height, width)\n      # same as input to forward (x)\n      stacked_outputs <- torch_stack(output_sequence, dim = 2)\n      \n      # pass the list of outputs (hs) to next layer\n      cur_layer_input <- stacked_outputs\n      \n      # keep track of list of outputs or this layer\n      layer_output_list[[i]] <- stacked_outputs\n      # keep track of last state for this layer\n      layer_state_list[[i]] <- list(h, c)\n    }\n \n    list(layer_output_list, layer_state_list)\n      \n  }\n    \n)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}