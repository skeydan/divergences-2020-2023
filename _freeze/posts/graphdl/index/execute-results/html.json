{
  "hash": "3b593a47897201920bb43e5f0e9b153c",
  "result": {
    "markdown": "---\ntitle: \"Getting started with deep learning on graphs\"\ndescription: >\n  This post introduces deep learning on graphs by mapping its central concept - message passing - to minimal usage patterns of PyTorch Geometric's foundation-laying MessagePassing class. Exploring those patterns, we gain some basic, very concrete insights into how graph DL works.\nauthor: Sigrid Keydana\ndate: 2022-11-22\ncategories: [Technical, Deep Learning]\nimage: net.jpg\n---\n\n\n\n\nIf, in deep-learning world, the first half of the last decade has been the age of images, and the second, that of language, one could say that now, we're living in the age of graphs. At least, that's what commonly cited research metrics suggest. But as we're all aware, deep-learning research is anything but an ivory tower. To see real-world implications, it suffices to reflect on how many things can be modeled as graphs. Some things quite naturally \"are\" graphs, in the sense of having nodes and edges: neurons, underground stations, social networks. Other things can fruitfully be modeled as graphs: molecules, for example; or language, concepts, three-dimensional shapes ... If deep learning on graphs is desirable, what are the challenges, and what do we get for free?\n\n## What's so special about deep learning on graphs?\n\nGraphs are different from images, language, as well as tabular data in that node numbering does not matter. In other words, graphs are permutation-invariant. Already this means that architectures established in other domains cannot be transferred verbatim. (The *ideas* underlying them can be transferred though. Thus, in the graph neural network (henceforth: GNN) model zoo you'll see lots of allusions to \"convolutional\", \"attention\", and other established terms.) Put very simply, and in concordance with common sense, whatever algorithm is used, it will fundamentally be based on how nodes are connected: the edges, that is.\n\nWhen relationships are modeled as graphs, both nodes and edges can have features. This, too, adds complexity. But not everything is harder with graphs. Think of how cumbersome it can be to obtain labeled data for supervised learning. With graphs, often an astonishingly small amount of labeled data is needed. More surprisingly still, a graph can be constructed when not a single edge is present. Put differently, learning on *sets* can morph into learning on *graphs*.\n\nAt this point, let me switch gears and move on to the practical part: the raison d'Ãªtre of this post.\n\n## Matching concepts and code: PyTorch Geometric\n\nIn this (and future) posts, we'll make use of [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/#) (from hereon: PyG), the most popular, at this time, and fastest-growing in terms of functionality as well as user base, library dedicated to graph DL.\n\nDeep learning on graphs, in its most general form, is usually characterized by the term *message passing*. Messages are passed between nodes that are linked by an edge: If node $A$ has three neighbors, it will receive three messages. Those messages have to be summarized in some meaningful way. Finally -- GNNs consisting of consecutive layers -- the node will have to decide how to modify its previous-layer features (a.k.a. embeddings) based on that summary.\n\nTogether, these make up a three-step sequence: collect messages; aggegate; update. What about the \"learning\" in deep learning, though? There are two places where learning can happen: Firstly, in message collection: Incoming messages could be transformed by a MLP, for example. Secondly, as part of the update step. All in all, this yields mathematical formulae like this, given in the PyG documentation:\n\n$$\n\\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\right)\n$$\n\nScary though this looks, once we read it from the right, we see that it nicely fits the conceptual description. The $(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i})$ are the three types of incoming messages a node can receive: its own state at the previous layer, the states of its neighbors (the nodes $j \\in \\mathcal{N}(i)$) at the previous layer, and features/embeddings associated to the edge in question. (I'm leaving out edge features in this discussion completely, so as to not further enhance complexity.) These messages are (optionally) transformed by the neural network $\\phi$, and whatever comes out is summarized by the aggregator function $\\square$. Finally, a node will update itself based on that summary as well as its own previous-layer state, possibly by means of applying neural network $\\gamma$.\n\nNow that we have this conceptual/mathematical representation, how does it map to code we see, or would like to write? PyG has excellent, extensive documentation, including at the beginner level. But here, I'd like to spell things out in detail -- pedantically, if you like, but in a way that tells us a lot about how GNNs work.\n\nLet's start by the information given in one of the key documentation pages, [Creating message passing networks](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html):\n\n> PyG provides the `MessagePassing` base class, which helps in creating such kinds of message passing graph neural networks by automatically taking care of message propagation. The user only has to define the functions $\\phi$, i.e. `message()`, and $\\gamma$, i.e. `update()`, as well as the aggregation scheme to use, i.e. `aggr=\"add\"`, `aggr=\"mean\"` or `aggr=\"max\"`.\n\nScrolling down that page and looking at the two example implementations, however, we see that an implementation of `update()` does not have to be provided; and from inspecting the [source code](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/message_passing.html#MessagePassing), it is clear that, technically, the same holds for `message()`. (And unless we want a form of aggregation different from the default `add`, we do not even need to specify that, either.)\n\nThus, the question becomes: What happens if we code the *minimal PyG GNN*? To find out, we first need to create a minimal graph, one minimal enough for us to track what is going on.\n\n### A minimal graph\n\nNow, a basic `Data` object is created from three tensors. The first holds the node features: two features each for five nodes. (Both features are identical on purpose, for \"cognitive ease\" -- on our, not the algorithm's, part.)\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport torch\n\nx = torch.tensor([[1, 1], [2, 2], [3, 3], [11, 11], [12, 12]], dtype=torch.float)\n```\n:::\n\n\nThe second specifies existing connections. For undirected graphs (like ours), each edge appears twice. The tensor you see here is specified in one-edge-per-line form for convenience reasons; to the `Data()` constructor we'll pass its transpose instead.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nedge_index = torch.tensor([\n  [0, 1],\n  [1, 0],\n  [0, 2],\n  [2, 0],\n  [1, 2],\n  [2, 1],\n  [2, 3],\n  [3, 2],\n  [2, 4],\n  [4, 2],\n  [3, 4],\n  [4, 3]\n], dtype=torch.long)\n```\n:::\n\n\nThe third tensor holds the node labels. (The task will be one of node -- not edge, not graph -- classification.)\n\n\n::: {.cell}\n\n```{.python .cell-code}\ny = torch.tensor([[0], [0], [0], [1], [1]], dtype=torch.float)\n```\n:::\n\n\nConstructing and inspecting the resulting graph, we have:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom torch_geometric.data import Data\n\ndata = Data(x = x, edge_index = edge_index.t().contiguous(), y = y)\ndata.x\ndata.edge_index\ndata.y\n```\n:::\n\n\n    tensor([[ 1.,  1.],\n            [ 2.,  2.],\n            [ 3.,  3.],\n            [11., 11.],\n            [12., 12.]])\n            \n    tensor([[0, 1, 0, 2, 1, 2, 2, 3, 2, 4, 3, 4],\n            [1, 0, 2, 0, 2, 1, 3, 2, 4, 2, 4, 3]])\n            \n    tensor([[0.],\n            [0.],\n            [0.],\n            [1.],\n            [1.]])\n\nFor our upcoming experiments, it's more helpful, though, to visualize the graph:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom torch_geometric.utils import to_networkx\n\ndef visualize_graph(G, color, labels):\n    plt.figure(figsize=(7,7))\n    plt.axis('off')\n    nx.draw_networkx(\n      G,\n      pos = nx.spring_layout(G, seed = 777),\n      labels = labels,\n      node_color = color,\n      cmap = \"Set3\"\n      )\n    plt.show()\n\nG = to_networkx(data, to_undirected = True, node_attrs = [\"x\"])\nlabels = nx.get_node_attributes(G, \"x\")\nvisualize_graph(G, color = data.y, labels = labels)\n```\n:::\n\n\n![](graph.png)\n\nAlthough our experiments won't be about training performance (how could they be, with just five nodes), let me remark in passing that this graph is small, but not boring: The middle node is equally connected to both \"sides\", yet feature-wise, it would pretty clearly appear to belong on just one of them. (Which is true, given the provided class labels). Such a constellation is interesting because, in the majority of networks, edges indicate similarity.\n\n### A minimal GNN\n\nNow, we code and run the minimal GNN. We're not interested in class labels (yet); we just want to see each node's embeddings after a single pass.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom torch_geometric.nn import MessagePassing\n\nclass IAmLazy(MessagePassing):\n    def forward(self, x, edge_index):\n        out = self.propagate(edge_index, x = x)\n        return out\n      \nmodule = IAmLazy()\nout = module(data.x, data.edge_index)\nout\n```\n:::\n\n\n    tensor([[ 5.,  5.],\n            [ 4.,  4.],\n            [26., 26.],\n            [15., 15.],\n            [14., 14.]])\n\nEvidently, we just had to start the process -- but what process, exactly? From what we know about the three stages of message passing, an essential question is what nodes do with the information that flows over the edges. Our first experiment, then, is to inspect the incoming messages.\n\n### Poking into `message()`\n\nIn `message()`, we have access to a structure named `x_j`. This tensor holds, for each node $i$, the embeddings of all nodes $j$ connected to it via incoming edges. We'll print them, and then, just return them, unchanged.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass IAmMyOthers(MessagePassing):\n    def forward(self, x, edge_index):\n        out = self.propagate(edge_index, x = x)\n        return out\n    def message(self, x_j):\n        print(\"in message, x_j is\")\n        print(x_j)\n        return x_j\n      \nmodule = IAmMyOthers()\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n```\n:::\n\n\n    in message, x_j is\n    tensor([[ 1.,  1.],\n            [ 2.,  2.],\n            [ 1.,  1.],\n            [ 3.,  3.],\n            [ 2.,  2.],\n            [ 3.,  3.],\n            [ 3.,  3.],\n            [11., 11.],\n            [ 3.,  3.],\n            [12., 12.],\n            [11., 11.],\n            [12., 12.]])\n            \n    result is:\n    tensor([[ 5.,  5.],\n            [ 4.,  4.],\n            [26., 26.],\n            [15., 15.],\n            [14., 14.]])\n\nLet me spell this out. In `data.edge_index`, repeated here for convenience:\n\n    tensor([[0, 1, 0, 2, 1, 2, 2, 3, 2, 4, 3, 4],\n            [1, 0, 2, 0, 2, 1, 3, 2, 4, 2, 4, 3]])\n\nthe first pair denotes the edge from node `0` (that had features `(1, 1)`) to node `1`. This information is found in `x_j`'s first row. Then the second row holds the information flowing in the opposite direction, namely, the features associated with node `1`. And so on.\n\nInterestingly, since we're passing through this module just once, we can see the messages that will be sent without even running it.\n\nNamely, since `data.edge_index[0]` designates the *source* nodes for each edge:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndata.edge_index[0]\n```\n:::\n\n\nwe can index `into data.x` to pick up what will be the *incoming* features for each connection.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndata.x[data.edge_index[0]]\n```\n:::\n\n\n    tensor([[ 1.,  1.],\n            [ 2.,  2.],\n            [ 1.,  1.],\n            [ 3.,  3.],\n            [ 2.,  2.],\n            [ 3.,  3.],\n            [ 3.,  3.],\n            [11., 11.],\n            [ 3.,  3.],\n            [12., 12.],\n            [11., 11.],\n            [12., 12.]])\n\nNow, what does this tell us? Node `0`, for example, received messages from nodes `1` and `2`: `(2, 2)` and `(3, 3)`, respectively. We know that the default aggregation mode is `add`; and so, would expect an outcome of `(5, 5)`. Indeed, this is the new embedding for node `0`.\n\nIn a nutshell, thus, the minimal GNN updates every node's embedding so as to prototypically reflect the node's neighborhood. Take care though: Nodes represent their neighborhoods, but themselves, they count for nothing. We will change that now.\n\n### Adding self loops\n\nAll we need to do is modify the adjacency matrix to include edges going from each node back to itself.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom torch_geometric.utils import add_self_loops\n\nclass IAmMyOthersAndMyselfAsWell(MessagePassing):\n    def forward(self, x, edge_index):\n        edge_index, _ = add_self_loops(edge_index, num_nodes = x.size(0))\n        print(\"in forward, augmented edge index now has shape\")\n        print(edge_index.shape)\n        out = self.propagate(edge_index, x = x)\n        return out\n    def message(self, x_j):\n        return x_j\n\nmodule = IAmMyOthersAndMyselfAsWell()\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n```\n:::\n\n\n    in forward, augmented edge index now has shape:\n    torch.Size([2, 17])\n\n    result is:\n    tensor([[ 6.,  6.],\n            [ 6.,  6.],\n            [29., 29.],\n            [26., 26.],\n            [26., 26.]])\n\nAs expected, the neighborhood summary at each node now includes a contribution from each node itself.\n\nNow we know how to access the messages, we'd like to aggregate them in a non-standard way.\n\n### Customizing `aggregate()`\n\nInstead of `message()`, we now override `aggregate()`. If we wanted to use another of the \"standard\" aggregation modes (`mean`, `mul`, `min`, or `max`), we could just override `__init__(),` like so:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef __init__(self):\n        super().__init__(aggr = \"mean\")\n```\n:::\n\n\nTo implement custom summaries, however, we make use of `torch_scatter` (one of PyG's installation prerequisites) for optimal performance. Let me show this by means of a simple example.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom torch_scatter import scatter\n\nclass IAmJustTheOppositeReally(MessagePassing):\n    def forward(self, x, edge_index):\n        out = self.propagate(edge_index, x = x)\n        return out\n    def aggregate(self, inputs, index):\n        print(\"in aggregate, inputs is\")\n        # same as x_j (incoming node features)\n        print(inputs)\n        print(\"in aggregate, index is\")\n        # this is data.edge_index[1]\n        print(index)\n        # see https://pytorch-scatter.readthedocs.io/en/1.3.0/index.html\n        # for other aggregation modes\n        # default dim is -1\n        return - scatter(inputs, index, dim = 0, reduce = \"add\") \n      \nmodule = IAmJustTheOppositeReally()\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n```\n:::\n\n\n    in aggregate, inputs is\n    tensor([[ 1.,  1.],\n            [ 2.,  2.],\n            [ 1.,  1.],\n            [ 3.,  3.],\n            [ 2.,  2.],\n            [ 3.,  3.],\n            [ 3.,  3.],\n            [11., 11.],\n            [ 3.,  3.],\n            [12., 12.],\n            [11., 11.],\n            [12., 12.]])\n            \n    in aggregate, index is\n    tensor([1, 0, 2, 0, 2, 1, 3, 2, 4, 2, 4, 3])\n\n    result is:\n    tensor([[ -5.,  -5.],\n            [ -4.,  -4.],\n            [-26., -26.],\n            [-15., -15.],\n            [-14., -14.]])\n\nIn `aggregate()`, we have two types of tensors to work with. One, `inputs`, holds what was returned from `message()`. In our case, this is identical to `x_j`, since we didn't make any modifications to the default behavior. The second, `index`, holds the recipe for where in the aggregation those features should go. Here, the very first tuple, `(1, 1)`, will contribute to the summary for node `1`; the second, `(2, 2)`, to that for node `0` -- and so on. By the way, just like `x_j` (in a single-layer, single-pass setup) is \"just\" `data.x[data.edge_index[0]]`, that `index` is \"just\" `data.edge_index[1]`. Meaning, this is the list of target nodes connected to the edges in question.\n\nAt this point, all kinds of manipulations could be done on either `inputs` or `index`; however, we content ourselves with just passing them through to `torch_scatter.scatter()`, and returning the negated sums. We've successfully built a network of contrarians.\n\nBy now, we've played with `message()` as well as `aggregate()`. What about `update()`?\n\n### Adding memory to `update()`\n\nThere's one thing really strange in what we're doing. It doesn't jump to the eye, since we're not simulating a real training phase; we've been calling the layer just once. If we hadn't, we'd have noticed that at every call, the nodes happily forget who they were before, dutifully assuming the new identities assigned. In reality, we probably want them to evolve in a more consistent way.\n\nFor example:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass IDoEvolveOverTime(MessagePassing):\n    def forward(self, x, edge_index):\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n        out = self.propagate(edge_index, x = x)\n        return out\n    def update(self, inputs, x):\n        print(\"in update, inputs is\")\n        print(inputs)\n        print(\"in update, x is\")\n        print(x)\n        return (inputs + x)/2\n\nmodule = IDoEvolveOverTime()\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n```\n:::\n\n\n    in update, inputs is\n    tensor([[ 6.,  6.],\n            [ 6.,  6.],\n            [29., 29.],\n            [26., 26.],\n            [26., 26.]])\n    in update, x is\n    tensor([[ 1.,  1.],\n            [ 2.,  2.],\n            [ 3.,  3.],\n            [11., 11.],\n            [12., 12.]])\n    result is:\n    tensor([[ 3.5000,  3.5000],\n            [ 4.0000,  4.0000],\n            [16.0000, 16.0000],\n            [18.5000, 18.5000],\n            [19.0000, 19.0000]])\n\nIn `update()`, we have access to both the final message aggregate (`inputs`) and the nodes' prior states (`x`). Here, I'm just averaging those two.\n\nAt this point, we've successfully acquainted ourselves with the three stages of message passing: acting on individual messages, aggregating them, and self-updating based on past state and new information. But none of our models so far could be called a neural network, since there was no learning involved.\n\n### Adding parameters\n\nIf we look back at the generic message passing formulation:\n\n$$\n\\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\right)\n$$ we see two places where neural network modules can act on the computation: before message aggregation, and as part of the node update process. First, we illustrate the former option. For example, we can apply a MLP in `forward()`, before the call to `aggregate()`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom torch.nn import Sequential as Seq, Linear, ReLU\n\nclass ILearnAndEvolve(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(aggr = \"sum\")\n        self.mlp = Seq(Linear(in_channels, out_channels),\n                       ReLU(),\n                       Linear(out_channels, out_channels))\n    def forward(self, x, edge_index):\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n        x = self.mlp(x)\n        out = self.propagate(edge_index = edge_index, x = x)\n        return out\n    def update(self, inputs, x):\n        return (inputs + x)/2\n\nmodule = ILearnAndEvolve(2, 2)\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n```\n:::\n\n\n    result is:\n    tensor([[-0.8724, -0.4407],\n            [-0.9056, -0.4623],\n            [-2.0229, -1.1240],\n            [-1.8691, -1.0867],\n            [-1.9024, -1.1082]], grad_fn=<DivBackward0>)\n\nFinally, we can apply network modules in both places, as exemplified next.\n\n### General message passing\n\nWe keep the MLP from the previous class, and add a second in `update()`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass ILearnAndEvolveDoubly(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(aggr = \"sum\")\n        self.mlp_msg = Seq(Linear(in_channels, out_channels),\n                       ReLU(),\n                       Linear(out_channels, out_channels))\n        self.mlp_upd = Seq(Linear(out_channels, out_channels),\n                       ReLU(),\n                       Linear(out_channels, out_channels))\n    def forward(self, x, edge_index):\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n        x = self.mlp_msg(x)\n        out = self.propagate(edge_index = edge_index, x = x)\n        return out\n    def update(self, inputs, x):\n        return self.mlp_upd((inputs + x)/2)\n\nmodule = ILearnAndEvolveDoubly(2, 2)\nout = module(data.x, data.edge_index)\nprint(\"result is:\")\nout\n```\n:::\n\n\n    result is:\n    tensor([[ 0.0573, -0.6988],\n            [ 0.0358, -0.6894],\n            [-0.1730, -0.6450],\n            [-0.5855, -0.4171],\n            [-0.5890, -0.4141]], grad_fn=<AddmmBackward0>)\n\nAt this point, I hope you'll feel comfortable to play around, subclassing the `MessagePassing` base class. Also, if now you consult the above-mentioned documentation page ([Creating message passing networks](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html)), you'll be able to map the example implementations (dedicated to popular GNN layer types) to where they \"hook into\" the message passing process.\n\nExperimentation with `MessagePassing` was the point of this post. However, you may be wondering: How do I actually use this for node classification? Didn't the graph have a class defined for each node? (It did: `data.y`.)\n\nSo let me conclude with a (minimal) end-to-end example that uses one of the above modules.\n\n### A minimal workflow\n\nTo that purpose, we compose that module with a linear one that performs node classification:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nclass Network(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, num_classes):\n        super().__init__()\n        self.conv = ILearnAndEvolveDoubly(in_channels, out_channels)\n        self.classifier = Linear(out_channels, num_classes)\n    def forward(self, x, edge_index):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv(x, edge_index)\n        return self.classifier(x)\n\nmodel = Network(2, 2, 1) \n```\n:::\n\n\nWe can then train the model like any other:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport torch.nn.functional as F\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\nmodel.train()\n\nfor epoch in range(5):\n    optimizer.zero_grad()\n    out = model(data.x, data.edge_index)\n    loss = F.binary_cross_entropy_with_logits(out, data.y)\n    loss.backward()\n    optimizer.step()\n\npreds = torch.sigmoid(out)\npreds\n```\n:::\n\n\n    tensor([[0.6502],\n            [0.6532],\n            [0.7027],\n            [0.7145],\n            [0.7165]], grad_fn=<SigmoidBackward0>)\n\nAnd that's it for this time. Stay tuned for examples of how graph models are applied in the sciences, as well as illustrations of bleeding-edge developments in [Geometric Deep Learning](https://blogs.rstudio.com/ai/posts/2021-08-26-geometric-deep-learning/), the principles-based, heuristics-transcending approach to neural networks.\n\nThanks for reading!\n\nPhoto by <a href=\"https://unsplash.com/@alinnnaaaa?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Alina Grubnyak</a> on <a href=\"https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}